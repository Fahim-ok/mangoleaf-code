{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T18:55:43.194756Z","iopub.status.busy":"2023-12-12T18:55:43.194234Z","iopub.status.idle":"2023-12-12T18:56:01.035686Z","shell.execute_reply":"2023-12-12T18:56:01.034762Z","shell.execute_reply.started":"2023-12-12T18:55:43.194730Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Device mapping:\n","/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n","/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","from tensorflow import keras\n","import seaborn as sns\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","from tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D\n","from tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Dropout\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.optimizers import Adam, RMSprop\n","from sklearn.utils import compute_class_weight\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","np.random.seed(72)\n","tf.random.set_seed(72)\n","sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T18:57:18.809484Z","iopub.status.busy":"2023-12-12T18:57:18.808831Z","iopub.status.idle":"2023-12-12T18:57:18.816159Z","shell.execute_reply":"2023-12-12T18:57:18.815247Z","shell.execute_reply.started":"2023-12-12T18:57:18.809453Z"},"trusted":true},"outputs":[],"source":["def evaluate_(model, generator_test):\n","    model.evaluate(generator_test)\n","    \n","    y_pred = model.predict(generator_test)\n","    y_pred_classes = np.argmax(y_pred, axis=1)\n","    y_true = generator_test.classes\n","    class_labels = list(generator_test.class_indices.keys())\n","\n","    print(classification_report(y_true, y_pred_classes))\n","    cm = confusion_matrix(y_true, y_pred_classes)\n","    \n","    # Plotting the confusion matrix\n","    plt.figure(figsize=(8, 8))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n","    plt.show()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T18:57:23.312245Z","iopub.status.busy":"2023-12-12T18:57:23.311550Z","iopub.status.idle":"2023-12-12T18:57:23.323309Z","shell.execute_reply":"2023-12-12T18:57:23.322343Z","shell.execute_reply.started":"2023-12-12T18:57:23.312212Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","def evaluate_(model, generator_test):\n","    model.evaluate(generator_test)\n","    \n","    y_pred = model.predict(generator_test)\n","    y_pred_classes = np.argmax(y_pred, axis=1)\n","    y_true = generator_test.classes\n","    class_labels = list(generator_test.class_indices.keys())\n","\n","    print(classification_report(y_true, y_pred_classes))\n","    cm = confusion_matrix(y_true, y_pred_classes)\n","    \n","    # Plotting the confusion matrix\n","    plt.figure(figsize=(12, 4))\n","    \n","    plt.subplot(1, 2, 1)\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n","    plt.title('Confusion Matrix')\n","    \n","    # ROC curve\n","    plt.subplot(1, 2, 2)\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","\n","    for i in range(len(class_labels)):\n","        fpr[i], tpr[i], _ = roc_curve(y_true == i, y_pred[:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","    for i in range(len(class_labels)):\n","        plt.plot(fpr[i], tpr[i], label=f'{class_labels[i]} (AUC = {roc_auc[i]:.2f})')\n","\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('ROC Curve')\n","    plt.legend(loc=\"lower right\")\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Call the function with your model and test generator\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T18:57:26.756785Z","iopub.status.busy":"2023-12-12T18:57:26.755825Z","iopub.status.idle":"2023-12-12T18:57:28.085798Z","shell.execute_reply":"2023-12-12T18:57:28.084952Z","shell.execute_reply.started":"2023-12-12T18:57:26.756750Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 13307 images belonging to 26 classes.\n","Found 1684 images belonging to 26 classes.\n","{0: 1.0640492563569486, 1: 1.0662660256410257, 2: 1.0662660256410257, 3: 1.0402595372107568, 4: 1.2273565762774397, 5: 1.0381494772975504, 6: 1.053102247546692, 7: 1.2795192307692307, 8: 1.0487862547288778, 9: 1.053102247546692, 10: 0.6486789509603198, 11: 1.0662660256410257, 12: 1.4623076923076923, 13: 0.8809082483781279, 14: 0.7571119708693673, 15: 1.0466414975617429, 16: 1.7527660695468914, 17: 0.5415954415954416, 18: 1.0662660256410257, 19: 0.9042538733351454, 20: 0.789826685660019, 21: 1.5462468045549616, 22: 1.0256667180514876, 23: 1.8085077466702908, 24: 1.0445054945054946, 25: 0.7259683578832515}\n"]}],"source":["train_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\n","test_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\n","datagen_train = ImageDataGenerator(rescale=1./255,\n","                                  width_shift_range=0.1,\n","                                  height_shift_range=0.1,\n","                                  horizontal_flip=True,\n","                                  vertical_flip=False)\n","\n","\n","datagen_test = ImageDataGenerator(rescale=1./255)\n","\n","\n","batch_size = 16\n","generator_train = datagen_train.flow_from_directory(directory=train_dir,\n","                                                    target_size=(112, 112),\n","                                                    batch_size=batch_size,\n","                                                    shuffle=True)\n","\n","generator_test = datagen_test.flow_from_directory(directory=test_dir,\n","                                                  target_size=(112, 112),\n","                                                  batch_size=batch_size,\n","                                                  shuffle=False)\n","# Calculate class weights\n","labels = generator_train.classes\n","class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n","class_weights = dict(zip(np.unique(labels), class_weights))\n","print(class_weights)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T18:57:31.881368Z","iopub.status.busy":"2023-12-12T18:57:31.881026Z","iopub.status.idle":"2023-12-12T18:57:46.093669Z","shell.execute_reply":"2023-12-12T18:57:46.092648Z","shell.execute_reply.started":"2023-12-12T18:57:31.881344Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting keras_cv_attention_models\n","  Obtaining dependency information for keras_cv_attention_models from https://files.pythonhosted.org/packages/67/b4/a581ae34f6a37b021e32d1d874b8b7df4664cbb8a71659886193caf341e0/keras_cv_attention_models-1.3.22-py3-none-any.whl.metadata\n","  Downloading keras_cv_attention_models-1.3.22-py3-none-any.whl.metadata (183 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.8/183.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (10.1.0)\n","Collecting ftfy (from keras_cv_attention_models)\n","  Obtaining dependency information for ftfy from https://files.pythonhosted.org/packages/91/f8/dfa32d06cfcbdb76bc46e0f5d69c537de33f4cedb1a15cd4746ab45a6a26/ftfy-6.1.3-py3-none-any.whl.metadata\n","  Downloading ftfy-6.1.3-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (2023.8.8)\n","Requirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (4.9.2)\n","Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (2.13.0)\n","Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy->keras_cv_attention_models)\n","  Obtaining dependency information for wcwidth<0.3.0,>=0.2.12 from https://files.pythonhosted.org/packages/31/b1/a59de0ad3aabb17523a39804f4c6df3ae87ead053a4e25362ae03d73d03a/wcwidth-0.2.12-py2.py3-none-any.whl.metadata\n","  Downloading wcwidth-0.2.12-py2.py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.1.21 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.51.1)\n","Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (3.9.0)\n","Requirement already satisfied: keras<2.14,>=2.13.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (2.13.1)\n","Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (16.0.6)\n","Requirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.24.3)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (3.3.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (21.3)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (3.20.3)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (68.1.2)\n","Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.16.0)\n","Requirement already satisfied: tensorboard<2.14,>=2.13 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (2.13.0)\n","Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (2.13.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (2.3.0)\n","Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.15.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (0.34.0)\n","Requirement already satisfied: array-record in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (0.4.1)\n","Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (8.1.7)\n","Requirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (0.1.8)\n","Requirement already satisfied: etils[enp,epath]>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (1.4.1)\n","Requirement already satisfied: promise in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (2.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (5.9.3)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (2.31.0)\n","Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (0.14.0)\n","Requirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (0.10.2)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (4.66.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow->keras_cv_attention_models) (0.41.2)\n","Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->keras_cv_attention_models) (5.13.0)\n","Requirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->keras_cv_attention_models) (3.16.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (2023.11.17)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (2.22.0)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (3.4.4)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (3.0.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow->keras_cv_attention_models) (3.0.9)\n","Requirement already satisfied: googleapis-common-protos in /opt/conda/lib/python3.10/site-packages (from tensorflow-metadata->tensorflow-datasets->keras_cv_attention_models) (1.60.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (0.2.7)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (2.1.3)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (3.2.2)\n","Downloading keras_cv_attention_models-1.3.22-py3-none-any.whl (773 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.2/773.2 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\n","Installing collected packages: wcwidth, ftfy, keras_cv_attention_models\n","  Attempting uninstall: wcwidth\n","    Found existing installation: wcwidth 0.2.6\n","    Uninstalling wcwidth-0.2.6:\n","      Successfully uninstalled wcwidth-0.2.6\n","Successfully installed ftfy-6.1.3 keras_cv_attention_models-1.3.22 wcwidth-0.2.12\n"]}],"source":["!pip install keras_cv_attention_models "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-16T19:09:23.491565Z","iopub.status.busy":"2023-11-16T19:09:23.491279Z","iopub.status.idle":"2023-11-16T19:09:23.505707Z","shell.execute_reply":"2023-11-16T19:09:23.504783Z","shell.execute_reply.started":"2023-11-16T19:09:23.491539Z"},"trusted":true},"outputs":[],"source":["np.random.seed(72)\n","tf.random.set_seed(72)\n","sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T18:58:19.980233Z","iopub.status.busy":"2023-12-12T18:58:19.979560Z","iopub.status.idle":"2023-12-12T18:58:19.984155Z","shell.execute_reply":"2023-12-12T18:58:19.983218Z","shell.execute_reply.started":"2023-12-12T18:58:19.980199Z"},"trusted":true},"outputs":[],"source":["from keras_cv_attention_models import wave_mlp"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T10:49:58.765357Z","iopub.status.busy":"2023-12-10T10:49:58.764769Z","iopub.status.idle":"2023-12-10T10:50:12.966644Z","shell.execute_reply":"2023-12-10T10:50:12.965518Z","shell.execute_reply.started":"2023-12-10T10:49:58.765324Z"},"trusted":true},"outputs":[],"source":["pip install keras-self-attention\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T07:50:34.077348Z","iopub.status.busy":"2023-12-10T07:50:34.076284Z","iopub.status.idle":"2023-12-10T07:50:37.949949Z","shell.execute_reply":"2023-12-10T07:50:37.948644Z","shell.execute_reply.started":"2023-12-10T07:50:34.077311Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Flatten, Dense, Dropout, Layer, Add, Input, Conv2D, BatchNormalization, Activation, GlobalAveragePooling2D, Reshape, Concatenate, Lambda, Permute\n","from tensorflow.keras import Model\n","\n","class SelfAttention(Layer):\n","    def __init__(self, **kwargs):\n","        super(SelfAttention, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.W_q = self.add_weight(name='W_q', shape=(input_shape[-1], input_shape[-1]),\n","                                   initializer='uniform', trainable=True)\n","        self.W_k = self.add_weight(name='W_k', shape=(input_shape[-1], input_shape[-1]),\n","                                   initializer='uniform', trainable=True)\n","        self.W_v = self.add_weight(name='W_v', shape=(input_shape[-1], input_shape[-1]),\n","                                   initializer='uniform', trainable=True)\n","        super(SelfAttention, self).build(input_shape)\n","\n","    def call(self, x):\n","        q = x @ self.W_q\n","        k = x @ self.W_k\n","        v = x @ self.W_v\n","\n","        attn_score = tf.matmul(q, k, transpose_b=True)\n","        attn_score = tf.nn.softmax(attn_score, axis=-1)\n","\n","        output = attn_score @ v\n","        return output\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","class PositionalEncoding(Layer):\n","    def __init__(self, input_shape, **kwargs):\n","        super(PositionalEncoding, self).__init__(**kwargs)\n","        self.input_shape_ = input_shape\n","\n","    def build(self, input_shape):\n","        self.positional_encoding = self.add_weight(name='positional_encoding',\n","                                                   shape=(1, *self.input_shape_[1:], 1),\n","                                                   initializer='uniform',\n","                                                   trainable=True)\n","        super(PositionalEncoding, self).build(input_shape)\n","\n","    def call(self, x):\n","        return x + self.positional_encoding\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","def shufflenet_block(x, groups):\n","    _, _, _, c = x.get_shape().as_list()\n","\n","    # Channel split\n","    x = Reshape((-1, groups))(x)\n","    x = Permute((2, 1))(x)\n","    x = Reshape((-1, c))(x)\n","\n","    # Channel shuffle\n","    x = Lambda(lambda z: tf.keras.backend.batch_flatten(tf.keras.backend.permute_dimensions(z, (0, 2, 1))))(x)\n","\n","    # Pointwise group convolution\n","    x = Reshape((1, 1, c))(x)\n","    x = Conv2D(c, kernel_size=(1, 1), groups=groups, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # Global Average Pooling\n","    x = GlobalAveragePooling2D()(x)\n","\n","    # Reshape for concatenation\n","    x = Reshape((1, 1, c))(x)\n","\n","    # Concatenate with the original input\n","    x = Concatenate(axis=-1)([x, x])\n","\n","    return x\n","\n","def custom_head_with_shufflenet(input_tensor, num_classes):\n","    x = Flatten()(input_tensor)\n","    x = Dense(256, activation='relu')(x)\n","    x = Dropout(0.5)(x)\n","\n","    # Add ShuffleNet block\n","    shufflenet_output = shufflenet_block(tf.keras.layers.Reshape((1, 1, 256))(x), groups=4)\n","    x = Add()([input_tensor, shufflenet_output])\n","\n","    # Add self-attention mechanism\n","    self_attention = SelfAttention()(x)\n","    x = Add()([x, self_attention])\n","\n","    # Add 2D positional encoding\n","    position_encoding = PositionalEncoding(input_shape=(1, 1, 256))(x)\n","    x = Add()([x, position_encoding])\n","\n","    x = Dense(num_classes, activation='softmax')(x)\n","    return x\n","\n","def modify_wave_mlp(input_shape, num_classes):\n","    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n","    head_output = custom_head_with_shufflenet(mm_headless.output, num_classes)\n","    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n","    return custom_model\n","\n","# Example usage:\n","input_shape = (112, 112, 3)\n","num_classes = 26  # Adjust based on your task\n","\n","custom_model = modify_wave_mlp(input_shape, num_classes)\n","custom_model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T07:48:25.113968Z","iopub.status.busy":"2023-12-10T07:48:25.112994Z","iopub.status.idle":"2023-12-10T07:48:28.485141Z","shell.execute_reply":"2023-12-10T07:48:28.483792Z","shell.execute_reply.started":"2023-12-10T07:48:25.113926Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Flatten, Dense, Dropout, Layer, Add\n","from tensorflow.keras import Input, Model\n","# Import your wave_mlp module\n","\n","class SelfAttention(Layer):\n","    def __init__(self, **kwargs):\n","        super(SelfAttention, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.W_q = self.add_weight(name='W_q', shape=(input_shape[-1], input_shape[-1]),\n","                                   initializer='uniform', trainable=True)\n","        self.W_k = self.add_weight(name='W_k', shape=(input_shape[-1], input_shape[-1]),\n","                                   initializer='uniform', trainable=True)\n","        self.W_v = self.add_weight(name='W_v', shape=(input_shape[-1], input_shape[-1]),\n","                                   initializer='uniform', trainable=True)\n","        super(SelfAttention, self).build(input_shape)\n","\n","    def call(self, x):\n","        q = x @ self.W_q\n","        k = x @ self.W_k\n","        v = x @ self.W_v\n","\n","        attn_score = tf.matmul(q, k, transpose_b=True)\n","        attn_score = tf.nn.softmax(attn_score, axis=-1)\n","\n","        output = attn_score @ v\n","        return output\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","class PositionalEncoding(Layer):\n","    def __init__(self, input_shape, **kwargs):\n","        super(PositionalEncoding, self).__init__(**kwargs)\n","        self.input_shape_ = input_shape\n","\n","    def build(self, input_shape):\n","        self.positional_encoding = self.add_weight(name='positional_encoding',\n","                                                   shape=(1, *self.input_shape_[1:], 1),\n","                                                   initializer='uniform',\n","                                                   trainable=True)\n","        super(PositionalEncoding, self).build(input_shape)\n","\n","    def call(self, x):\n","        return x + self.positional_encoding\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","class ResidualBlock(Layer):\n","    def __init__(self, **kwargs):\n","        super(ResidualBlock, self).__init__(**kwargs)\n","        self.conv1 = tf.keras.layers.Conv2D(filters=None, kernel_size=(3, 3), padding='same', activation='relu')\n","        self.conv2 = tf.keras.layers.Conv2D(filters=None, kernel_size=(3, 3), padding='same', activation='relu')\n","        self.add = Add()\n","\n","    def build(self, input_shape):\n","        # Infer the number of filters dynamically based on the input shape\n","        num_filters = input_shape[-1]\n","        self.conv1.filters = num_filters\n","        self.conv2.filters = num_filters\n","        super(ResidualBlock, self).build(input_shape)\n","\n","    def call(self, inputs):\n","        x = self.conv1(inputs)\n","        x = self.conv2(x)\n","        return self.add([inputs, x])\n","\n","\n","\n","def custom_head_with_stages(input_tensor, num_classes):\n","    # Stage 1: Global Self-Attention\n","    x = SelfAttention()(input_tensor)\n","\n","    # Stage 2: Global Self-Attention\n","    x = SelfAttention()(x)\n","\n","    # Stage 3: Global Self-Attention\n","    x = SelfAttention()(x)\n","\n","    # Stage 4: Residual Blocks\n","    x = ResidualBlock(256, (3, 3))(x)\n","    x = ResidualBlock(256, (3, 3))(x)\n","\n","    # Flatten and add your custom dense layers\n","    x = Flatten()(x)\n","    x = Dense(256, activation='relu')(x)\n","    x = Dropout(0.5)(x)\n","\n","    # Add self-attention mechanism\n","    self_attention = SelfAttention()(x)\n","    x = Add()([x, self_attention])\n","\n","    # Add 2D positional encoding\n","    position_encoding = PositionalEncoding(input_shape=(1, 1, 256))(x)\n","    x = Add()([x, position_encoding])\n","\n","    # Final Dense layer\n","    x = Dense(num_classes, activation='softmax')(x)\n","\n","    return x\n","\n","def modify_wave_mlp_with_stages(input_shape, num_classes):\n","    # Load the WaveMLP model without the top layers (head)\n","    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n","\n","    # Add your custom head with stages\n","    head_output = custom_head_with_stages(mm_headless.output, num_classes)\n","\n","    # Create the custom model by combining the base model and the custom head\n","    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n","\n","    return custom_model\n","\n","# Example usage:\n","input_shape = (112, 112, 3)\n","num_classes = 26  # Adjust based on your task\n","\n","custom_model_with_stages = modify_wave_mlp_with_stages(input_shape, num_classes)\n","custom_model_with_stages.summary()\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T19:04:29.078601Z","iopub.status.busy":"2023-12-12T19:04:29.078231Z","iopub.status.idle":"2023-12-12T19:04:33.748433Z","shell.execute_reply":"2023-12-12T19:04:33.747491Z","shell.execute_reply.started":"2023-12-12T19:04:29.078573Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[">>>> Load pretrained from: /root/.keras/models/wavemlp_t_imagenet.h5\n","Model: \"model_2\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_3 (InputLayer)        [(None, 112, 112, 3)]        0         []                            \n","                                                                                                  \n"," stem_pad (ZeroPadding2D)    (None, 116, 116, 3)          0         ['input_3[0][0]']             \n","                                                                                                  \n"," stem_conv (Conv2D)          (None, 28, 28, 64)           9472      ['stem_pad[0][0]']            \n","                                                                                                  \n"," stem_bn (BatchNormalizatio  (None, 28, 28, 64)           256       ['stem_conv[0][0]']           \n"," n)                                                                                               \n","                                                                                                  \n"," stack1_block1_attn_bn (Bat  (None, 28, 28, 64)           256       ['stem_bn[0][0]']             \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," stack1_block1_attn_theta_h  (None, 28, 28, 64)           4160      ['stack1_block1_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack1_block1_attn_theta_w  (None, 28, 28, 64)           4160      ['stack1_block1_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack1_block1_attn_theta_h  (None, 28, 28, 64)           256       ['stack1_block1_attn_theta_h_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack1_block1_attn_theta_w  (None, 28, 28, 64)           256       ['stack1_block1_attn_theta_w_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack1_block1_attn_theta_h  (None, 28, 28, 64)           0         ['stack1_block1_attn_theta_h_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack1_block1_attn_theta_w  (None, 28, 28, 64)           0         ['stack1_block1_attn_theta_w_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack1_block1_attn_height_  (None, 28, 28, 64)           4096      ['stack1_block1_attn_bn[0][0]'\n"," conv (Conv2D)                                                      ]                             \n","                                                                                                  \n"," tf.math.cos_40 (TFOpLambda  (None, 28, 28, 64)           0         ['stack1_block1_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_40 (TFOpLambda  (None, 28, 28, 64)           0         ['stack1_block1_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," stack1_block1_attn_width_c  (None, 28, 28, 64)           4096      ['stack1_block1_attn_bn[0][0]'\n"," onv (Conv2D)                                                       ]                             \n","                                                                                                  \n"," tf.math.cos_41 (TFOpLambda  (None, 28, 28, 64)           0         ['stack1_block1_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_41 (TFOpLambda  (None, 28, 28, 64)           0         ['stack1_block1_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," multiply_140 (Multiply)     (None, 28, 28, 64)           0         ['stack1_block1_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.cos_40[0][0]']      \n","                                                                                                  \n"," multiply_141 (Multiply)     (None, 28, 28, 64)           0         ['stack1_block1_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.sin_40[0][0]']      \n","                                                                                                  \n"," multiply_142 (Multiply)     (None, 28, 28, 64)           0         ['stack1_block1_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.cos_41[0][0]']      \n","                                                                                                  \n"," multiply_143 (Multiply)     (None, 28, 28, 64)           0         ['stack1_block1_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.sin_41[0][0]']      \n","                                                                                                  \n"," concatenate_40 (Concatenat  (None, 28, 28, 128)          0         ['multiply_140[0][0]',        \n"," e)                                                                  'multiply_141[0][0]']        \n","                                                                                                  \n"," concatenate_41 (Concatenat  (None, 28, 28, 128)          0         ['multiply_142[0][0]',        \n"," e)                                                                  'multiply_143[0][0]']        \n","                                                                                                  \n"," stack1_block1_attn_height_  (None, 28, 34, 128)          0         ['concatenate_40[0][0]']      \n"," down_pad (ZeroPadding2D)                                                                         \n","                                                                                                  \n"," stack1_block1_attn_width_d  (None, 34, 28, 128)          0         ['concatenate_41[0][0]']      \n"," own_pad (ZeroPadding2D)                                                                          \n","                                                                                                  \n"," stack1_block1_attn_height_  (None, 28, 28, 64)           896       ['stack1_block1_attn_height_do\n"," down_conv (Conv2D)                                                 wn_pad[0][0]']                \n","                                                                                                  \n"," stack1_block1_attn_width_d  (None, 28, 28, 64)           896       ['stack1_block1_attn_width_dow\n"," own_conv (Conv2D)                                                  n_pad[0][0]']                 \n","                                                                                                  \n"," stack1_block1_attn_channel  (None, 28, 28, 64)           4096      ['stack1_block1_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack1_block1_attn_combine  (None, 28, 28, 64)           0         ['stack1_block1_attn_height_do\n","  (Add)                                                             wn_conv[0][0]',               \n","                                                                     'stack1_block1_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'stack1_block1_attn_channel_c\n","                                                                    onv[0][0]']                   \n","                                                                                                  \n"," global_average_pooling2d_2  (None, 1, 1, 64)             0         ['stack1_block1_attn_combine[0\n"," 0 (GlobalAveragePooling2D)                                         ][0]']                        \n","                                                                                                  \n"," stack1_block1_attn_reweigh  (None, 1, 1, 16)             1040      ['global_average_pooling2d_20[\n"," t_Conv_0 (Conv2D)                                                  0][0]']                       \n","                                                                                                  \n"," stack1_block1_attn_reweigh  (None, 1, 1, 16)             0         ['stack1_block1_attn_reweight_\n"," t_gelugelu (Activation)                                            Conv_0[0][0]']                \n","                                                                                                  \n"," stack1_block1_attn_reweigh  (None, 1, 1, 192)            3264      ['stack1_block1_attn_reweight_\n"," t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n","                                                                                                  \n"," reshape_20 (Reshape)        (None, 1, 1, 64, 3)          0         ['stack1_block1_attn_reweight_\n","                                                                    Conv_1[0][0]']                \n","                                                                                                  \n"," stack1_block1_attn_attenti  (None, 1, 1, 64, 3)          0         ['reshape_20[0][0]']          \n"," on_scores (Softmax)                                                                              \n","                                                                                                  \n"," tf.unstack_20 (TFOpLambda)  [(None, 1, 1, 64),           0         ['stack1_block1_attn_attention\n","                              (None, 1, 1, 64),                     _scores[0][0]']               \n","                              (None, 1, 1, 64)]                                                   \n","                                                                                                  \n"," multiply_144 (Multiply)     (None, 28, 28, 64)           0         ['stack1_block1_attn_height_do\n","                                                                    wn_conv[0][0]',               \n","                                                                     'tf.unstack_20[0][0]']       \n","                                                                                                  \n"," multiply_145 (Multiply)     (None, 28, 28, 64)           0         ['stack1_block1_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'tf.unstack_20[0][1]']       \n","                                                                                                  \n"," multiply_146 (Multiply)     (None, 28, 28, 64)           0         ['stack1_block1_attn_channel_c\n","                                                                    onv[0][0]',                   \n","                                                                     'tf.unstack_20[0][2]']       \n","                                                                                                  \n"," add_42 (Add)                (None, 28, 28, 64)           0         ['multiply_144[0][0]',        \n","                                                                     'multiply_145[0][0]',        \n","                                                                     'multiply_146[0][0]']        \n","                                                                                                  \n"," stack1_block1_attn_out_con  (None, 28, 28, 64)           4160      ['add_42[0][0]']              \n"," v (Conv2D)                                                                                       \n","                                                                                                  \n"," stack1_block1_attn_out (Ad  (None, 28, 28, 64)           0         ['stem_bn[0][0]',             \n"," d)                                                                  'stack1_block1_attn_out_conv[\n","                                                                    0][0]']                       \n","                                                                                                  \n"," stack1_block1_mlp_bn (Batc  (None, 28, 28, 64)           256       ['stack1_block1_attn_out[0][0]\n"," hNormalization)                                                    ']                            \n","                                                                                                  \n"," stack1_block1_mlp_Conv_0 (  (None, 28, 28, 256)          16640     ['stack1_block1_mlp_bn[0][0]']\n"," Conv2D)                                                                                          \n","                                                                                                  \n"," stack1_block1_mlp_gelugelu  (None, 28, 28, 256)          0         ['stack1_block1_mlp_Conv_0[0][\n","  (Activation)                                                      0]']                          \n","                                                                                                  \n"," stack1_block1_mlp_Conv_1 (  (None, 28, 28, 64)           16448     ['stack1_block1_mlp_gelugelu[0\n"," Conv2D)                                                            ][0]']                        \n","                                                                                                  \n"," stack1_block1_mlp_out (Add  (None, 28, 28, 64)           0         ['stack1_block1_attn_out[0][0]\n"," )                                                                  ',                            \n","                                                                     'stack1_block1_mlp_Conv_1[0][\n","                                                                    0]']                          \n","                                                                                                  \n"," stack1_block2_attn_bn (Bat  (None, 28, 28, 64)           256       ['stack1_block1_mlp_out[0][0]'\n"," chNormalization)                                                   ]                             \n","                                                                                                  \n"," stack1_block2_attn_theta_h  (None, 28, 28, 64)           4160      ['stack1_block2_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack1_block2_attn_theta_w  (None, 28, 28, 64)           4160      ['stack1_block2_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack1_block2_attn_theta_h  (None, 28, 28, 64)           256       ['stack1_block2_attn_theta_h_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack1_block2_attn_theta_w  (None, 28, 28, 64)           256       ['stack1_block2_attn_theta_w_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack1_block2_attn_theta_h  (None, 28, 28, 64)           0         ['stack1_block2_attn_theta_h_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack1_block2_attn_theta_w  (None, 28, 28, 64)           0         ['stack1_block2_attn_theta_w_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack1_block2_attn_height_  (None, 28, 28, 64)           4096      ['stack1_block2_attn_bn[0][0]'\n"," conv (Conv2D)                                                      ]                             \n","                                                                                                  \n"," tf.math.cos_42 (TFOpLambda  (None, 28, 28, 64)           0         ['stack1_block2_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_42 (TFOpLambda  (None, 28, 28, 64)           0         ['stack1_block2_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," stack1_block2_attn_width_c  (None, 28, 28, 64)           4096      ['stack1_block2_attn_bn[0][0]'\n"," onv (Conv2D)                                                       ]                             \n","                                                                                                  \n"," tf.math.cos_43 (TFOpLambda  (None, 28, 28, 64)           0         ['stack1_block2_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_43 (TFOpLambda  (None, 28, 28, 64)           0         ['stack1_block2_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," multiply_147 (Multiply)     (None, 28, 28, 64)           0         ['stack1_block2_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.cos_42[0][0]']      \n","                                                                                                  \n"," multiply_148 (Multiply)     (None, 28, 28, 64)           0         ['stack1_block2_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.sin_42[0][0]']      \n","                                                                                                  \n"," multiply_149 (Multiply)     (None, 28, 28, 64)           0         ['stack1_block2_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.cos_43[0][0]']      \n","                                                                                                  \n"," multiply_150 (Multiply)     (None, 28, 28, 64)           0         ['stack1_block2_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.sin_43[0][0]']      \n","                                                                                                  \n"," concatenate_42 (Concatenat  (None, 28, 28, 128)          0         ['multiply_147[0][0]',        \n"," e)                                                                  'multiply_148[0][0]']        \n","                                                                                                  \n"," concatenate_43 (Concatenat  (None, 28, 28, 128)          0         ['multiply_149[0][0]',        \n"," e)                                                                  'multiply_150[0][0]']        \n","                                                                                                  \n"," stack1_block2_attn_height_  (None, 28, 34, 128)          0         ['concatenate_42[0][0]']      \n"," down_pad (ZeroPadding2D)                                                                         \n","                                                                                                  \n"," stack1_block2_attn_width_d  (None, 34, 28, 128)          0         ['concatenate_43[0][0]']      \n"," own_pad (ZeroPadding2D)                                                                          \n","                                                                                                  \n"," stack1_block2_attn_height_  (None, 28, 28, 64)           896       ['stack1_block2_attn_height_do\n"," down_conv (Conv2D)                                                 wn_pad[0][0]']                \n","                                                                                                  \n"," stack1_block2_attn_width_d  (None, 28, 28, 64)           896       ['stack1_block2_attn_width_dow\n"," own_conv (Conv2D)                                                  n_pad[0][0]']                 \n","                                                                                                  \n"," stack1_block2_attn_channel  (None, 28, 28, 64)           4096      ['stack1_block2_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack1_block2_attn_combine  (None, 28, 28, 64)           0         ['stack1_block2_attn_height_do\n","  (Add)                                                             wn_conv[0][0]',               \n","                                                                     'stack1_block2_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'stack1_block2_attn_channel_c\n","                                                                    onv[0][0]']                   \n","                                                                                                  \n"," global_average_pooling2d_2  (None, 1, 1, 64)             0         ['stack1_block2_attn_combine[0\n"," 1 (GlobalAveragePooling2D)                                         ][0]']                        \n","                                                                                                  \n"," stack1_block2_attn_reweigh  (None, 1, 1, 16)             1040      ['global_average_pooling2d_21[\n"," t_Conv_0 (Conv2D)                                                  0][0]']                       \n","                                                                                                  \n"," stack1_block2_attn_reweigh  (None, 1, 1, 16)             0         ['stack1_block2_attn_reweight_\n"," t_gelugelu (Activation)                                            Conv_0[0][0]']                \n","                                                                                                  \n"," stack1_block2_attn_reweigh  (None, 1, 1, 192)            3264      ['stack1_block2_attn_reweight_\n"," t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n","                                                                                                  \n"," reshape_21 (Reshape)        (None, 1, 1, 64, 3)          0         ['stack1_block2_attn_reweight_\n","                                                                    Conv_1[0][0]']                \n","                                                                                                  \n"," stack1_block2_attn_attenti  (None, 1, 1, 64, 3)          0         ['reshape_21[0][0]']          \n"," on_scores (Softmax)                                                                              \n","                                                                                                  \n"," tf.unstack_21 (TFOpLambda)  [(None, 1, 1, 64),           0         ['stack1_block2_attn_attention\n","                              (None, 1, 1, 64),                     _scores[0][0]']               \n","                              (None, 1, 1, 64)]                                                   \n","                                                                                                  \n"," multiply_151 (Multiply)     (None, 28, 28, 64)           0         ['stack1_block2_attn_height_do\n","                                                                    wn_conv[0][0]',               \n","                                                                     'tf.unstack_21[0][0]']       \n","                                                                                                  \n"," multiply_152 (Multiply)     (None, 28, 28, 64)           0         ['stack1_block2_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'tf.unstack_21[0][1]']       \n","                                                                                                  \n"," multiply_153 (Multiply)     (None, 28, 28, 64)           0         ['stack1_block2_attn_channel_c\n","                                                                    onv[0][0]',                   \n","                                                                     'tf.unstack_21[0][2]']       \n","                                                                                                  \n"," add_43 (Add)                (None, 28, 28, 64)           0         ['multiply_151[0][0]',        \n","                                                                     'multiply_152[0][0]',        \n","                                                                     'multiply_153[0][0]']        \n","                                                                                                  \n"," stack1_block2_attn_out_con  (None, 28, 28, 64)           4160      ['add_43[0][0]']              \n"," v (Conv2D)                                                                                       \n","                                                                                                  \n"," stack1_block2_attn_out (Ad  (None, 28, 28, 64)           0         ['stack1_block1_mlp_out[0][0]'\n"," d)                                                                 , 'stack1_block2_attn_out_conv\n","                                                                    [0][0]']                      \n","                                                                                                  \n"," stack1_block2_mlp_bn (Batc  (None, 28, 28, 64)           256       ['stack1_block2_attn_out[0][0]\n"," hNormalization)                                                    ']                            \n","                                                                                                  \n"," stack1_block2_mlp_Conv_0 (  (None, 28, 28, 256)          16640     ['stack1_block2_mlp_bn[0][0]']\n"," Conv2D)                                                                                          \n","                                                                                                  \n"," stack1_block2_mlp_gelugelu  (None, 28, 28, 256)          0         ['stack1_block2_mlp_Conv_0[0][\n","  (Activation)                                                      0]']                          \n","                                                                                                  \n"," stack1_block2_mlp_Conv_1 (  (None, 28, 28, 64)           16448     ['stack1_block2_mlp_gelugelu[0\n"," Conv2D)                                                            ][0]']                        \n","                                                                                                  \n"," stack1_block2_mlp_out (Add  (None, 28, 28, 64)           0         ['stack1_block2_attn_out[0][0]\n"," )                                                                  ',                            \n","                                                                     'stack1_block2_mlp_Conv_1[0][\n","                                                                    0]']                          \n","                                                                                                  \n"," stack2_down_sample_pad (Ze  (None, 30, 30, 64)           0         ['stack1_block2_mlp_out[0][0]'\n"," roPadding2D)                                                       ]                             \n","                                                                                                  \n"," stack2_down_sample_conv (C  (None, 14, 14, 128)          73856     ['stack2_down_sample_pad[0][0]\n"," onv2D)                                                             ']                            \n","                                                                                                  \n"," stack2_down_sample_bn (Bat  (None, 14, 14, 128)          512       ['stack2_down_sample_conv[0][0\n"," chNormalization)                                                   ]']                           \n","                                                                                                  \n"," stack2_block1_attn_bn (Bat  (None, 14, 14, 128)          512       ['stack2_down_sample_bn[0][0]'\n"," chNormalization)                                                   ]                             \n","                                                                                                  \n"," stack2_block1_attn_theta_h  (None, 14, 14, 128)          16512     ['stack2_block1_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack2_block1_attn_theta_w  (None, 14, 14, 128)          16512     ['stack2_block1_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack2_block1_attn_theta_h  (None, 14, 14, 128)          512       ['stack2_block1_attn_theta_h_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack2_block1_attn_theta_w  (None, 14, 14, 128)          512       ['stack2_block1_attn_theta_w_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack2_block1_attn_theta_h  (None, 14, 14, 128)          0         ['stack2_block1_attn_theta_h_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack2_block1_attn_theta_w  (None, 14, 14, 128)          0         ['stack2_block1_attn_theta_w_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack2_block1_attn_height_  (None, 14, 14, 128)          16384     ['stack2_block1_attn_bn[0][0]'\n"," conv (Conv2D)                                                      ]                             \n","                                                                                                  \n"," tf.math.cos_44 (TFOpLambda  (None, 14, 14, 128)          0         ['stack2_block1_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_44 (TFOpLambda  (None, 14, 14, 128)          0         ['stack2_block1_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," stack2_block1_attn_width_c  (None, 14, 14, 128)          16384     ['stack2_block1_attn_bn[0][0]'\n"," onv (Conv2D)                                                       ]                             \n","                                                                                                  \n"," tf.math.cos_45 (TFOpLambda  (None, 14, 14, 128)          0         ['stack2_block1_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_45 (TFOpLambda  (None, 14, 14, 128)          0         ['stack2_block1_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," multiply_154 (Multiply)     (None, 14, 14, 128)          0         ['stack2_block1_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.cos_44[0][0]']      \n","                                                                                                  \n"," multiply_155 (Multiply)     (None, 14, 14, 128)          0         ['stack2_block1_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.sin_44[0][0]']      \n","                                                                                                  \n"," multiply_156 (Multiply)     (None, 14, 14, 128)          0         ['stack2_block1_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.cos_45[0][0]']      \n","                                                                                                  \n"," multiply_157 (Multiply)     (None, 14, 14, 128)          0         ['stack2_block1_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.sin_45[0][0]']      \n","                                                                                                  \n"," concatenate_44 (Concatenat  (None, 14, 14, 256)          0         ['multiply_154[0][0]',        \n"," e)                                                                  'multiply_155[0][0]']        \n","                                                                                                  \n"," concatenate_45 (Concatenat  (None, 14, 14, 256)          0         ['multiply_156[0][0]',        \n"," e)                                                                  'multiply_157[0][0]']        \n","                                                                                                  \n"," stack2_block1_attn_height_  (None, 14, 20, 256)          0         ['concatenate_44[0][0]']      \n"," down_pad (ZeroPadding2D)                                                                         \n","                                                                                                  \n"," stack2_block1_attn_width_d  (None, 20, 14, 256)          0         ['concatenate_45[0][0]']      \n"," own_pad (ZeroPadding2D)                                                                          \n","                                                                                                  \n"," stack2_block1_attn_height_  (None, 14, 14, 128)          1792      ['stack2_block1_attn_height_do\n"," down_conv (Conv2D)                                                 wn_pad[0][0]']                \n","                                                                                                  \n"," stack2_block1_attn_width_d  (None, 14, 14, 128)          1792      ['stack2_block1_attn_width_dow\n"," own_conv (Conv2D)                                                  n_pad[0][0]']                 \n","                                                                                                  \n"," stack2_block1_attn_channel  (None, 14, 14, 128)          16384     ['stack2_block1_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack2_block1_attn_combine  (None, 14, 14, 128)          0         ['stack2_block1_attn_height_do\n","  (Add)                                                             wn_conv[0][0]',               \n","                                                                     'stack2_block1_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'stack2_block1_attn_channel_c\n","                                                                    onv[0][0]']                   \n","                                                                                                  \n"," global_average_pooling2d_2  (None, 1, 1, 128)            0         ['stack2_block1_attn_combine[0\n"," 2 (GlobalAveragePooling2D)                                         ][0]']                        \n","                                                                                                  \n"," stack2_block1_attn_reweigh  (None, 1, 1, 32)             4128      ['global_average_pooling2d_22[\n"," t_Conv_0 (Conv2D)                                                  0][0]']                       \n","                                                                                                  \n"," stack2_block1_attn_reweigh  (None, 1, 1, 32)             0         ['stack2_block1_attn_reweight_\n"," t_gelugelu (Activation)                                            Conv_0[0][0]']                \n","                                                                                                  \n"," stack2_block1_attn_reweigh  (None, 1, 1, 384)            12672     ['stack2_block1_attn_reweight_\n"," t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n","                                                                                                  \n"," reshape_22 (Reshape)        (None, 1, 1, 128, 3)         0         ['stack2_block1_attn_reweight_\n","                                                                    Conv_1[0][0]']                \n","                                                                                                  \n"," stack2_block1_attn_attenti  (None, 1, 1, 128, 3)         0         ['reshape_22[0][0]']          \n"," on_scores (Softmax)                                                                              \n","                                                                                                  \n"," tf.unstack_22 (TFOpLambda)  [(None, 1, 1, 128),          0         ['stack2_block1_attn_attention\n","                              (None, 1, 1, 128),                    _scores[0][0]']               \n","                              (None, 1, 1, 128)]                                                  \n","                                                                                                  \n"," multiply_158 (Multiply)     (None, 14, 14, 128)          0         ['stack2_block1_attn_height_do\n","                                                                    wn_conv[0][0]',               \n","                                                                     'tf.unstack_22[0][0]']       \n","                                                                                                  \n"," multiply_159 (Multiply)     (None, 14, 14, 128)          0         ['stack2_block1_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'tf.unstack_22[0][1]']       \n","                                                                                                  \n"," multiply_160 (Multiply)     (None, 14, 14, 128)          0         ['stack2_block1_attn_channel_c\n","                                                                    onv[0][0]',                   \n","                                                                     'tf.unstack_22[0][2]']       \n","                                                                                                  \n"," add_44 (Add)                (None, 14, 14, 128)          0         ['multiply_158[0][0]',        \n","                                                                     'multiply_159[0][0]',        \n","                                                                     'multiply_160[0][0]']        \n","                                                                                                  \n"," stack2_block1_attn_out_con  (None, 14, 14, 128)          16512     ['add_44[0][0]']              \n"," v (Conv2D)                                                                                       \n","                                                                                                  \n"," stack2_block1_attn_out (Ad  (None, 14, 14, 128)          0         ['stack2_down_sample_bn[0][0]'\n"," d)                                                                 , 'stack2_block1_attn_out_conv\n","                                                                    [0][0]']                      \n","                                                                                                  \n"," stack2_block1_mlp_bn (Batc  (None, 14, 14, 128)          512       ['stack2_block1_attn_out[0][0]\n"," hNormalization)                                                    ']                            \n","                                                                                                  \n"," stack2_block1_mlp_Conv_0 (  (None, 14, 14, 512)          66048     ['stack2_block1_mlp_bn[0][0]']\n"," Conv2D)                                                                                          \n","                                                                                                  \n"," stack2_block1_mlp_gelugelu  (None, 14, 14, 512)          0         ['stack2_block1_mlp_Conv_0[0][\n","  (Activation)                                                      0]']                          \n","                                                                                                  \n"," stack2_block1_mlp_Conv_1 (  (None, 14, 14, 128)          65664     ['stack2_block1_mlp_gelugelu[0\n"," Conv2D)                                                            ][0]']                        \n","                                                                                                  \n"," stack2_block1_mlp_out (Add  (None, 14, 14, 128)          0         ['stack2_block1_attn_out[0][0]\n"," )                                                                  ',                            \n","                                                                     'stack2_block1_mlp_Conv_1[0][\n","                                                                    0]']                          \n","                                                                                                  \n"," stack2_block2_attn_bn (Bat  (None, 14, 14, 128)          512       ['stack2_block1_mlp_out[0][0]'\n"," chNormalization)                                                   ]                             \n","                                                                                                  \n"," stack2_block2_attn_theta_h  (None, 14, 14, 128)          16512     ['stack2_block2_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack2_block2_attn_theta_w  (None, 14, 14, 128)          16512     ['stack2_block2_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack2_block2_attn_theta_h  (None, 14, 14, 128)          512       ['stack2_block2_attn_theta_h_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack2_block2_attn_theta_w  (None, 14, 14, 128)          512       ['stack2_block2_attn_theta_w_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack2_block2_attn_theta_h  (None, 14, 14, 128)          0         ['stack2_block2_attn_theta_h_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack2_block2_attn_theta_w  (None, 14, 14, 128)          0         ['stack2_block2_attn_theta_w_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack2_block2_attn_height_  (None, 14, 14, 128)          16384     ['stack2_block2_attn_bn[0][0]'\n"," conv (Conv2D)                                                      ]                             \n","                                                                                                  \n"," tf.math.cos_46 (TFOpLambda  (None, 14, 14, 128)          0         ['stack2_block2_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_46 (TFOpLambda  (None, 14, 14, 128)          0         ['stack2_block2_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," stack2_block2_attn_width_c  (None, 14, 14, 128)          16384     ['stack2_block2_attn_bn[0][0]'\n"," onv (Conv2D)                                                       ]                             \n","                                                                                                  \n"," tf.math.cos_47 (TFOpLambda  (None, 14, 14, 128)          0         ['stack2_block2_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_47 (TFOpLambda  (None, 14, 14, 128)          0         ['stack2_block2_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," multiply_161 (Multiply)     (None, 14, 14, 128)          0         ['stack2_block2_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.cos_46[0][0]']      \n","                                                                                                  \n"," multiply_162 (Multiply)     (None, 14, 14, 128)          0         ['stack2_block2_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.sin_46[0][0]']      \n","                                                                                                  \n"," multiply_163 (Multiply)     (None, 14, 14, 128)          0         ['stack2_block2_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.cos_47[0][0]']      \n","                                                                                                  \n"," multiply_164 (Multiply)     (None, 14, 14, 128)          0         ['stack2_block2_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.sin_47[0][0]']      \n","                                                                                                  \n"," concatenate_46 (Concatenat  (None, 14, 14, 256)          0         ['multiply_161[0][0]',        \n"," e)                                                                  'multiply_162[0][0]']        \n","                                                                                                  \n"," concatenate_47 (Concatenat  (None, 14, 14, 256)          0         ['multiply_163[0][0]',        \n"," e)                                                                  'multiply_164[0][0]']        \n","                                                                                                  \n"," stack2_block2_attn_height_  (None, 14, 20, 256)          0         ['concatenate_46[0][0]']      \n"," down_pad (ZeroPadding2D)                                                                         \n","                                                                                                  \n"," stack2_block2_attn_width_d  (None, 20, 14, 256)          0         ['concatenate_47[0][0]']      \n"," own_pad (ZeroPadding2D)                                                                          \n","                                                                                                  \n"," stack2_block2_attn_height_  (None, 14, 14, 128)          1792      ['stack2_block2_attn_height_do\n"," down_conv (Conv2D)                                                 wn_pad[0][0]']                \n","                                                                                                  \n"," stack2_block2_attn_width_d  (None, 14, 14, 128)          1792      ['stack2_block2_attn_width_dow\n"," own_conv (Conv2D)                                                  n_pad[0][0]']                 \n","                                                                                                  \n"," stack2_block2_attn_channel  (None, 14, 14, 128)          16384     ['stack2_block2_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack2_block2_attn_combine  (None, 14, 14, 128)          0         ['stack2_block2_attn_height_do\n","  (Add)                                                             wn_conv[0][0]',               \n","                                                                     'stack2_block2_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'stack2_block2_attn_channel_c\n","                                                                    onv[0][0]']                   \n","                                                                                                  \n"," global_average_pooling2d_2  (None, 1, 1, 128)            0         ['stack2_block2_attn_combine[0\n"," 3 (GlobalAveragePooling2D)                                         ][0]']                        \n","                                                                                                  \n"," stack2_block2_attn_reweigh  (None, 1, 1, 32)             4128      ['global_average_pooling2d_23[\n"," t_Conv_0 (Conv2D)                                                  0][0]']                       \n","                                                                                                  \n"," stack2_block2_attn_reweigh  (None, 1, 1, 32)             0         ['stack2_block2_attn_reweight_\n"," t_gelugelu (Activation)                                            Conv_0[0][0]']                \n","                                                                                                  \n"," stack2_block2_attn_reweigh  (None, 1, 1, 384)            12672     ['stack2_block2_attn_reweight_\n"," t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n","                                                                                                  \n"," reshape_23 (Reshape)        (None, 1, 1, 128, 3)         0         ['stack2_block2_attn_reweight_\n","                                                                    Conv_1[0][0]']                \n","                                                                                                  \n"," stack2_block2_attn_attenti  (None, 1, 1, 128, 3)         0         ['reshape_23[0][0]']          \n"," on_scores (Softmax)                                                                              \n","                                                                                                  \n"," tf.unstack_23 (TFOpLambda)  [(None, 1, 1, 128),          0         ['stack2_block2_attn_attention\n","                              (None, 1, 1, 128),                    _scores[0][0]']               \n","                              (None, 1, 1, 128)]                                                  \n","                                                                                                  \n"," multiply_165 (Multiply)     (None, 14, 14, 128)          0         ['stack2_block2_attn_height_do\n","                                                                    wn_conv[0][0]',               \n","                                                                     'tf.unstack_23[0][0]']       \n","                                                                                                  \n"," multiply_166 (Multiply)     (None, 14, 14, 128)          0         ['stack2_block2_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'tf.unstack_23[0][1]']       \n","                                                                                                  \n"," multiply_167 (Multiply)     (None, 14, 14, 128)          0         ['stack2_block2_attn_channel_c\n","                                                                    onv[0][0]',                   \n","                                                                     'tf.unstack_23[0][2]']       \n","                                                                                                  \n"," add_45 (Add)                (None, 14, 14, 128)          0         ['multiply_165[0][0]',        \n","                                                                     'multiply_166[0][0]',        \n","                                                                     'multiply_167[0][0]']        \n","                                                                                                  \n"," stack2_block2_attn_out_con  (None, 14, 14, 128)          16512     ['add_45[0][0]']              \n"," v (Conv2D)                                                                                       \n","                                                                                                  \n"," stack2_block2_attn_out (Ad  (None, 14, 14, 128)          0         ['stack2_block1_mlp_out[0][0]'\n"," d)                                                                 , 'stack2_block2_attn_out_conv\n","                                                                    [0][0]']                      \n","                                                                                                  \n"," stack2_block2_mlp_bn (Batc  (None, 14, 14, 128)          512       ['stack2_block2_attn_out[0][0]\n"," hNormalization)                                                    ']                            \n","                                                                                                  \n"," stack2_block2_mlp_Conv_0 (  (None, 14, 14, 512)          66048     ['stack2_block2_mlp_bn[0][0]']\n"," Conv2D)                                                                                          \n","                                                                                                  \n"," stack2_block2_mlp_gelugelu  (None, 14, 14, 512)          0         ['stack2_block2_mlp_Conv_0[0][\n","  (Activation)                                                      0]']                          \n","                                                                                                  \n"," stack2_block2_mlp_Conv_1 (  (None, 14, 14, 128)          65664     ['stack2_block2_mlp_gelugelu[0\n"," Conv2D)                                                            ][0]']                        \n","                                                                                                  \n"," stack2_block2_mlp_out (Add  (None, 14, 14, 128)          0         ['stack2_block2_attn_out[0][0]\n"," )                                                                  ',                            \n","                                                                     'stack2_block2_mlp_Conv_1[0][\n","                                                                    0]']                          \n","                                                                                                  \n"," stack3_down_sample_pad (Ze  (None, 16, 16, 128)          0         ['stack2_block2_mlp_out[0][0]'\n"," roPadding2D)                                                       ]                             \n","                                                                                                  \n"," stack3_down_sample_conv (C  (None, 7, 7, 320)            368960    ['stack3_down_sample_pad[0][0]\n"," onv2D)                                                             ']                            \n","                                                                                                  \n"," stack3_down_sample_bn (Bat  (None, 7, 7, 320)            1280      ['stack3_down_sample_conv[0][0\n"," chNormalization)                                                   ]']                           \n","                                                                                                  \n"," stack3_block1_attn_bn (Bat  (None, 7, 7, 320)            1280      ['stack3_down_sample_bn[0][0]'\n"," chNormalization)                                                   ]                             \n","                                                                                                  \n"," stack3_block1_attn_theta_h  (None, 7, 7, 320)            102720    ['stack3_block1_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack3_block1_attn_theta_w  (None, 7, 7, 320)            102720    ['stack3_block1_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack3_block1_attn_theta_h  (None, 7, 7, 320)            1280      ['stack3_block1_attn_theta_h_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack3_block1_attn_theta_w  (None, 7, 7, 320)            1280      ['stack3_block1_attn_theta_w_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack3_block1_attn_theta_h  (None, 7, 7, 320)            0         ['stack3_block1_attn_theta_h_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack3_block1_attn_theta_w  (None, 7, 7, 320)            0         ['stack3_block1_attn_theta_w_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack3_block1_attn_height_  (None, 7, 7, 320)            102400    ['stack3_block1_attn_bn[0][0]'\n"," conv (Conv2D)                                                      ]                             \n","                                                                                                  \n"," tf.math.cos_48 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block1_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_48 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block1_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," stack3_block1_attn_width_c  (None, 7, 7, 320)            102400    ['stack3_block1_attn_bn[0][0]'\n"," onv (Conv2D)                                                       ]                             \n","                                                                                                  \n"," tf.math.cos_49 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block1_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_49 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block1_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," multiply_168 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block1_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.cos_48[0][0]']      \n","                                                                                                  \n"," multiply_169 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block1_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.sin_48[0][0]']      \n","                                                                                                  \n"," multiply_170 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block1_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.cos_49[0][0]']      \n","                                                                                                  \n"," multiply_171 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block1_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.sin_49[0][0]']      \n","                                                                                                  \n"," concatenate_48 (Concatenat  (None, 7, 7, 640)            0         ['multiply_168[0][0]',        \n"," e)                                                                  'multiply_169[0][0]']        \n","                                                                                                  \n"," concatenate_49 (Concatenat  (None, 7, 7, 640)            0         ['multiply_170[0][0]',        \n"," e)                                                                  'multiply_171[0][0]']        \n","                                                                                                  \n"," stack3_block1_attn_height_  (None, 7, 13, 640)           0         ['concatenate_48[0][0]']      \n"," down_pad (ZeroPadding2D)                                                                         \n","                                                                                                  \n"," stack3_block1_attn_width_d  (None, 13, 7, 640)           0         ['concatenate_49[0][0]']      \n"," own_pad (ZeroPadding2D)                                                                          \n","                                                                                                  \n"," stack3_block1_attn_height_  (None, 7, 7, 320)            4480      ['stack3_block1_attn_height_do\n"," down_conv (Conv2D)                                                 wn_pad[0][0]']                \n","                                                                                                  \n"," stack3_block1_attn_width_d  (None, 7, 7, 320)            4480      ['stack3_block1_attn_width_dow\n"," own_conv (Conv2D)                                                  n_pad[0][0]']                 \n","                                                                                                  \n"," stack3_block1_attn_channel  (None, 7, 7, 320)            102400    ['stack3_block1_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack3_block1_attn_combine  (None, 7, 7, 320)            0         ['stack3_block1_attn_height_do\n","  (Add)                                                             wn_conv[0][0]',               \n","                                                                     'stack3_block1_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'stack3_block1_attn_channel_c\n","                                                                    onv[0][0]']                   \n","                                                                                                  \n"," global_average_pooling2d_2  (None, 1, 1, 320)            0         ['stack3_block1_attn_combine[0\n"," 4 (GlobalAveragePooling2D)                                         ][0]']                        \n","                                                                                                  \n"," stack3_block1_attn_reweigh  (None, 1, 1, 80)             25680     ['global_average_pooling2d_24[\n"," t_Conv_0 (Conv2D)                                                  0][0]']                       \n","                                                                                                  \n"," stack3_block1_attn_reweigh  (None, 1, 1, 80)             0         ['stack3_block1_attn_reweight_\n"," t_gelugelu (Activation)                                            Conv_0[0][0]']                \n","                                                                                                  \n"," stack3_block1_attn_reweigh  (None, 1, 1, 960)            77760     ['stack3_block1_attn_reweight_\n"," t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n","                                                                                                  \n"," reshape_24 (Reshape)        (None, 1, 1, 320, 3)         0         ['stack3_block1_attn_reweight_\n","                                                                    Conv_1[0][0]']                \n","                                                                                                  \n"," stack3_block1_attn_attenti  (None, 1, 1, 320, 3)         0         ['reshape_24[0][0]']          \n"," on_scores (Softmax)                                                                              \n","                                                                                                  \n"," tf.unstack_24 (TFOpLambda)  [(None, 1, 1, 320),          0         ['stack3_block1_attn_attention\n","                              (None, 1, 1, 320),                    _scores[0][0]']               \n","                              (None, 1, 1, 320)]                                                  \n","                                                                                                  \n"," multiply_172 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block1_attn_height_do\n","                                                                    wn_conv[0][0]',               \n","                                                                     'tf.unstack_24[0][0]']       \n","                                                                                                  \n"," multiply_173 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block1_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'tf.unstack_24[0][1]']       \n","                                                                                                  \n"," multiply_174 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block1_attn_channel_c\n","                                                                    onv[0][0]',                   \n","                                                                     'tf.unstack_24[0][2]']       \n","                                                                                                  \n"," add_46 (Add)                (None, 7, 7, 320)            0         ['multiply_172[0][0]',        \n","                                                                     'multiply_173[0][0]',        \n","                                                                     'multiply_174[0][0]']        \n","                                                                                                  \n"," stack3_block1_attn_out_con  (None, 7, 7, 320)            102720    ['add_46[0][0]']              \n"," v (Conv2D)                                                                                       \n","                                                                                                  \n"," stack3_block1_attn_out (Ad  (None, 7, 7, 320)            0         ['stack3_down_sample_bn[0][0]'\n"," d)                                                                 , 'stack3_block1_attn_out_conv\n","                                                                    [0][0]']                      \n","                                                                                                  \n"," stack3_block1_mlp_bn (Batc  (None, 7, 7, 320)            1280      ['stack3_block1_attn_out[0][0]\n"," hNormalization)                                                    ']                            \n","                                                                                                  \n"," stack3_block1_mlp_Conv_0 (  (None, 7, 7, 1280)           410880    ['stack3_block1_mlp_bn[0][0]']\n"," Conv2D)                                                                                          \n","                                                                                                  \n"," stack3_block1_mlp_gelugelu  (None, 7, 7, 1280)           0         ['stack3_block1_mlp_Conv_0[0][\n","  (Activation)                                                      0]']                          \n","                                                                                                  \n"," stack3_block1_mlp_Conv_1 (  (None, 7, 7, 320)            409920    ['stack3_block1_mlp_gelugelu[0\n"," Conv2D)                                                            ][0]']                        \n","                                                                                                  \n"," stack3_block1_mlp_out (Add  (None, 7, 7, 320)            0         ['stack3_block1_attn_out[0][0]\n"," )                                                                  ',                            \n","                                                                     'stack3_block1_mlp_Conv_1[0][\n","                                                                    0]']                          \n","                                                                                                  \n"," stack3_block2_attn_bn (Bat  (None, 7, 7, 320)            1280      ['stack3_block1_mlp_out[0][0]'\n"," chNormalization)                                                   ]                             \n","                                                                                                  \n"," stack3_block2_attn_theta_h  (None, 7, 7, 320)            102720    ['stack3_block2_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack3_block2_attn_theta_w  (None, 7, 7, 320)            102720    ['stack3_block2_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack3_block2_attn_theta_h  (None, 7, 7, 320)            1280      ['stack3_block2_attn_theta_h_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack3_block2_attn_theta_w  (None, 7, 7, 320)            1280      ['stack3_block2_attn_theta_w_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack3_block2_attn_theta_h  (None, 7, 7, 320)            0         ['stack3_block2_attn_theta_h_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack3_block2_attn_theta_w  (None, 7, 7, 320)            0         ['stack3_block2_attn_theta_w_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack3_block2_attn_height_  (None, 7, 7, 320)            102400    ['stack3_block2_attn_bn[0][0]'\n"," conv (Conv2D)                                                      ]                             \n","                                                                                                  \n"," tf.math.cos_50 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block2_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_50 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block2_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," stack3_block2_attn_width_c  (None, 7, 7, 320)            102400    ['stack3_block2_attn_bn[0][0]'\n"," onv (Conv2D)                                                       ]                             \n","                                                                                                  \n"," tf.math.cos_51 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block2_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_51 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block2_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," multiply_175 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block2_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.cos_50[0][0]']      \n","                                                                                                  \n"," multiply_176 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block2_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.sin_50[0][0]']      \n","                                                                                                  \n"," multiply_177 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block2_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.cos_51[0][0]']      \n","                                                                                                  \n"," multiply_178 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block2_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.sin_51[0][0]']      \n","                                                                                                  \n"," concatenate_50 (Concatenat  (None, 7, 7, 640)            0         ['multiply_175[0][0]',        \n"," e)                                                                  'multiply_176[0][0]']        \n","                                                                                                  \n"," concatenate_51 (Concatenat  (None, 7, 7, 640)            0         ['multiply_177[0][0]',        \n"," e)                                                                  'multiply_178[0][0]']        \n","                                                                                                  \n"," stack3_block2_attn_height_  (None, 7, 13, 640)           0         ['concatenate_50[0][0]']      \n"," down_pad (ZeroPadding2D)                                                                         \n","                                                                                                  \n"," stack3_block2_attn_width_d  (None, 13, 7, 640)           0         ['concatenate_51[0][0]']      \n"," own_pad (ZeroPadding2D)                                                                          \n","                                                                                                  \n"," stack3_block2_attn_height_  (None, 7, 7, 320)            4480      ['stack3_block2_attn_height_do\n"," down_conv (Conv2D)                                                 wn_pad[0][0]']                \n","                                                                                                  \n"," stack3_block2_attn_width_d  (None, 7, 7, 320)            4480      ['stack3_block2_attn_width_dow\n"," own_conv (Conv2D)                                                  n_pad[0][0]']                 \n","                                                                                                  \n"," stack3_block2_attn_channel  (None, 7, 7, 320)            102400    ['stack3_block2_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack3_block2_attn_combine  (None, 7, 7, 320)            0         ['stack3_block2_attn_height_do\n","  (Add)                                                             wn_conv[0][0]',               \n","                                                                     'stack3_block2_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'stack3_block2_attn_channel_c\n","                                                                    onv[0][0]']                   \n","                                                                                                  \n"," global_average_pooling2d_2  (None, 1, 1, 320)            0         ['stack3_block2_attn_combine[0\n"," 5 (GlobalAveragePooling2D)                                         ][0]']                        \n","                                                                                                  \n"," stack3_block2_attn_reweigh  (None, 1, 1, 80)             25680     ['global_average_pooling2d_25[\n"," t_Conv_0 (Conv2D)                                                  0][0]']                       \n","                                                                                                  \n"," stack3_block2_attn_reweigh  (None, 1, 1, 80)             0         ['stack3_block2_attn_reweight_\n"," t_gelugelu (Activation)                                            Conv_0[0][0]']                \n","                                                                                                  \n"," stack3_block2_attn_reweigh  (None, 1, 1, 960)            77760     ['stack3_block2_attn_reweight_\n"," t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n","                                                                                                  \n"," reshape_25 (Reshape)        (None, 1, 1, 320, 3)         0         ['stack3_block2_attn_reweight_\n","                                                                    Conv_1[0][0]']                \n","                                                                                                  \n"," stack3_block2_attn_attenti  (None, 1, 1, 320, 3)         0         ['reshape_25[0][0]']          \n"," on_scores (Softmax)                                                                              \n","                                                                                                  \n"," tf.unstack_25 (TFOpLambda)  [(None, 1, 1, 320),          0         ['stack3_block2_attn_attention\n","                              (None, 1, 1, 320),                    _scores[0][0]']               \n","                              (None, 1, 1, 320)]                                                  \n","                                                                                                  \n"," multiply_179 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block2_attn_height_do\n","                                                                    wn_conv[0][0]',               \n","                                                                     'tf.unstack_25[0][0]']       \n","                                                                                                  \n"," multiply_180 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block2_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'tf.unstack_25[0][1]']       \n","                                                                                                  \n"," multiply_181 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block2_attn_channel_c\n","                                                                    onv[0][0]',                   \n","                                                                     'tf.unstack_25[0][2]']       \n","                                                                                                  \n"," add_47 (Add)                (None, 7, 7, 320)            0         ['multiply_179[0][0]',        \n","                                                                     'multiply_180[0][0]',        \n","                                                                     'multiply_181[0][0]']        \n","                                                                                                  \n"," stack3_block2_attn_out_con  (None, 7, 7, 320)            102720    ['add_47[0][0]']              \n"," v (Conv2D)                                                                                       \n","                                                                                                  \n"," stack3_block2_attn_out (Ad  (None, 7, 7, 320)            0         ['stack3_block1_mlp_out[0][0]'\n"," d)                                                                 , 'stack3_block2_attn_out_conv\n","                                                                    [0][0]']                      \n","                                                                                                  \n"," stack3_block2_mlp_bn (Batc  (None, 7, 7, 320)            1280      ['stack3_block2_attn_out[0][0]\n"," hNormalization)                                                    ']                            \n","                                                                                                  \n"," stack3_block2_mlp_Conv_0 (  (None, 7, 7, 1280)           410880    ['stack3_block2_mlp_bn[0][0]']\n"," Conv2D)                                                                                          \n","                                                                                                  \n"," stack3_block2_mlp_gelugelu  (None, 7, 7, 1280)           0         ['stack3_block2_mlp_Conv_0[0][\n","  (Activation)                                                      0]']                          \n","                                                                                                  \n"," stack3_block2_mlp_Conv_1 (  (None, 7, 7, 320)            409920    ['stack3_block2_mlp_gelugelu[0\n"," Conv2D)                                                            ][0]']                        \n","                                                                                                  \n"," stack3_block2_mlp_out (Add  (None, 7, 7, 320)            0         ['stack3_block2_attn_out[0][0]\n"," )                                                                  ',                            \n","                                                                     'stack3_block2_mlp_Conv_1[0][\n","                                                                    0]']                          \n","                                                                                                  \n"," stack3_block3_attn_bn (Bat  (None, 7, 7, 320)            1280      ['stack3_block2_mlp_out[0][0]'\n"," chNormalization)                                                   ]                             \n","                                                                                                  \n"," stack3_block3_attn_theta_h  (None, 7, 7, 320)            102720    ['stack3_block3_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack3_block3_attn_theta_w  (None, 7, 7, 320)            102720    ['stack3_block3_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack3_block3_attn_theta_h  (None, 7, 7, 320)            1280      ['stack3_block3_attn_theta_h_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack3_block3_attn_theta_w  (None, 7, 7, 320)            1280      ['stack3_block3_attn_theta_w_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack3_block3_attn_theta_h  (None, 7, 7, 320)            0         ['stack3_block3_attn_theta_h_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack3_block3_attn_theta_w  (None, 7, 7, 320)            0         ['stack3_block3_attn_theta_w_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack3_block3_attn_height_  (None, 7, 7, 320)            102400    ['stack3_block3_attn_bn[0][0]'\n"," conv (Conv2D)                                                      ]                             \n","                                                                                                  \n"," tf.math.cos_52 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block3_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_52 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block3_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," stack3_block3_attn_width_c  (None, 7, 7, 320)            102400    ['stack3_block3_attn_bn[0][0]'\n"," onv (Conv2D)                                                       ]                             \n","                                                                                                  \n"," tf.math.cos_53 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block3_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_53 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block3_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," multiply_182 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block3_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.cos_52[0][0]']      \n","                                                                                                  \n"," multiply_183 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block3_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.sin_52[0][0]']      \n","                                                                                                  \n"," multiply_184 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block3_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.cos_53[0][0]']      \n","                                                                                                  \n"," multiply_185 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block3_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.sin_53[0][0]']      \n","                                                                                                  \n"," concatenate_52 (Concatenat  (None, 7, 7, 640)            0         ['multiply_182[0][0]',        \n"," e)                                                                  'multiply_183[0][0]']        \n","                                                                                                  \n"," concatenate_53 (Concatenat  (None, 7, 7, 640)            0         ['multiply_184[0][0]',        \n"," e)                                                                  'multiply_185[0][0]']        \n","                                                                                                  \n"," stack3_block3_attn_height_  (None, 7, 13, 640)           0         ['concatenate_52[0][0]']      \n"," down_pad (ZeroPadding2D)                                                                         \n","                                                                                                  \n"," stack3_block3_attn_width_d  (None, 13, 7, 640)           0         ['concatenate_53[0][0]']      \n"," own_pad (ZeroPadding2D)                                                                          \n","                                                                                                  \n"," stack3_block3_attn_height_  (None, 7, 7, 320)            4480      ['stack3_block3_attn_height_do\n"," down_conv (Conv2D)                                                 wn_pad[0][0]']                \n","                                                                                                  \n"," stack3_block3_attn_width_d  (None, 7, 7, 320)            4480      ['stack3_block3_attn_width_dow\n"," own_conv (Conv2D)                                                  n_pad[0][0]']                 \n","                                                                                                  \n"," stack3_block3_attn_channel  (None, 7, 7, 320)            102400    ['stack3_block3_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack3_block3_attn_combine  (None, 7, 7, 320)            0         ['stack3_block3_attn_height_do\n","  (Add)                                                             wn_conv[0][0]',               \n","                                                                     'stack3_block3_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'stack3_block3_attn_channel_c\n","                                                                    onv[0][0]']                   \n","                                                                                                  \n"," global_average_pooling2d_2  (None, 1, 1, 320)            0         ['stack3_block3_attn_combine[0\n"," 6 (GlobalAveragePooling2D)                                         ][0]']                        \n","                                                                                                  \n"," stack3_block3_attn_reweigh  (None, 1, 1, 80)             25680     ['global_average_pooling2d_26[\n"," t_Conv_0 (Conv2D)                                                  0][0]']                       \n","                                                                                                  \n"," stack3_block3_attn_reweigh  (None, 1, 1, 80)             0         ['stack3_block3_attn_reweight_\n"," t_gelugelu (Activation)                                            Conv_0[0][0]']                \n","                                                                                                  \n"," stack3_block3_attn_reweigh  (None, 1, 1, 960)            77760     ['stack3_block3_attn_reweight_\n"," t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n","                                                                                                  \n"," reshape_26 (Reshape)        (None, 1, 1, 320, 3)         0         ['stack3_block3_attn_reweight_\n","                                                                    Conv_1[0][0]']                \n","                                                                                                  \n"," stack3_block3_attn_attenti  (None, 1, 1, 320, 3)         0         ['reshape_26[0][0]']          \n"," on_scores (Softmax)                                                                              \n","                                                                                                  \n"," tf.unstack_26 (TFOpLambda)  [(None, 1, 1, 320),          0         ['stack3_block3_attn_attention\n","                              (None, 1, 1, 320),                    _scores[0][0]']               \n","                              (None, 1, 1, 320)]                                                  \n","                                                                                                  \n"," multiply_186 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block3_attn_height_do\n","                                                                    wn_conv[0][0]',               \n","                                                                     'tf.unstack_26[0][0]']       \n","                                                                                                  \n"," multiply_187 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block3_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'tf.unstack_26[0][1]']       \n","                                                                                                  \n"," multiply_188 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block3_attn_channel_c\n","                                                                    onv[0][0]',                   \n","                                                                     'tf.unstack_26[0][2]']       \n","                                                                                                  \n"," add_48 (Add)                (None, 7, 7, 320)            0         ['multiply_186[0][0]',        \n","                                                                     'multiply_187[0][0]',        \n","                                                                     'multiply_188[0][0]']        \n","                                                                                                  \n"," stack3_block3_attn_out_con  (None, 7, 7, 320)            102720    ['add_48[0][0]']              \n"," v (Conv2D)                                                                                       \n","                                                                                                  \n"," stack3_block3_attn_out (Ad  (None, 7, 7, 320)            0         ['stack3_block2_mlp_out[0][0]'\n"," d)                                                                 , 'stack3_block3_attn_out_conv\n","                                                                    [0][0]']                      \n","                                                                                                  \n"," stack3_block3_mlp_bn (Batc  (None, 7, 7, 320)            1280      ['stack3_block3_attn_out[0][0]\n"," hNormalization)                                                    ']                            \n","                                                                                                  \n"," stack3_block3_mlp_Conv_0 (  (None, 7, 7, 1280)           410880    ['stack3_block3_mlp_bn[0][0]']\n"," Conv2D)                                                                                          \n","                                                                                                  \n"," stack3_block3_mlp_gelugelu  (None, 7, 7, 1280)           0         ['stack3_block3_mlp_Conv_0[0][\n","  (Activation)                                                      0]']                          \n","                                                                                                  \n"," stack3_block3_mlp_Conv_1 (  (None, 7, 7, 320)            409920    ['stack3_block3_mlp_gelugelu[0\n"," Conv2D)                                                            ][0]']                        \n","                                                                                                  \n"," stack3_block3_mlp_out (Add  (None, 7, 7, 320)            0         ['stack3_block3_attn_out[0][0]\n"," )                                                                  ',                            \n","                                                                     'stack3_block3_mlp_Conv_1[0][\n","                                                                    0]']                          \n","                                                                                                  \n"," stack3_block4_attn_bn (Bat  (None, 7, 7, 320)            1280      ['stack3_block3_mlp_out[0][0]'\n"," chNormalization)                                                   ]                             \n","                                                                                                  \n"," stack3_block4_attn_theta_h  (None, 7, 7, 320)            102720    ['stack3_block4_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack3_block4_attn_theta_w  (None, 7, 7, 320)            102720    ['stack3_block4_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack3_block4_attn_theta_h  (None, 7, 7, 320)            1280      ['stack3_block4_attn_theta_h_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack3_block4_attn_theta_w  (None, 7, 7, 320)            1280      ['stack3_block4_attn_theta_w_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack3_block4_attn_theta_h  (None, 7, 7, 320)            0         ['stack3_block4_attn_theta_h_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack3_block4_attn_theta_w  (None, 7, 7, 320)            0         ['stack3_block4_attn_theta_w_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack3_block4_attn_height_  (None, 7, 7, 320)            102400    ['stack3_block4_attn_bn[0][0]'\n"," conv (Conv2D)                                                      ]                             \n","                                                                                                  \n"," tf.math.cos_54 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block4_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_54 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block4_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," stack3_block4_attn_width_c  (None, 7, 7, 320)            102400    ['stack3_block4_attn_bn[0][0]'\n"," onv (Conv2D)                                                       ]                             \n","                                                                                                  \n"," tf.math.cos_55 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block4_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_55 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block4_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," multiply_189 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block4_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.cos_54[0][0]']      \n","                                                                                                  \n"," multiply_190 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block4_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.sin_54[0][0]']      \n","                                                                                                  \n"," multiply_191 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block4_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.cos_55[0][0]']      \n","                                                                                                  \n"," multiply_192 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block4_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.sin_55[0][0]']      \n","                                                                                                  \n"," concatenate_54 (Concatenat  (None, 7, 7, 640)            0         ['multiply_189[0][0]',        \n"," e)                                                                  'multiply_190[0][0]']        \n","                                                                                                  \n"," concatenate_55 (Concatenat  (None, 7, 7, 640)            0         ['multiply_191[0][0]',        \n"," e)                                                                  'multiply_192[0][0]']        \n","                                                                                                  \n"," stack3_block4_attn_height_  (None, 7, 13, 640)           0         ['concatenate_54[0][0]']      \n"," down_pad (ZeroPadding2D)                                                                         \n","                                                                                                  \n"," stack3_block4_attn_width_d  (None, 13, 7, 640)           0         ['concatenate_55[0][0]']      \n"," own_pad (ZeroPadding2D)                                                                          \n","                                                                                                  \n"," stack3_block4_attn_height_  (None, 7, 7, 320)            4480      ['stack3_block4_attn_height_do\n"," down_conv (Conv2D)                                                 wn_pad[0][0]']                \n","                                                                                                  \n"," stack3_block4_attn_width_d  (None, 7, 7, 320)            4480      ['stack3_block4_attn_width_dow\n"," own_conv (Conv2D)                                                  n_pad[0][0]']                 \n","                                                                                                  \n"," stack3_block4_attn_channel  (None, 7, 7, 320)            102400    ['stack3_block4_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack3_block4_attn_combine  (None, 7, 7, 320)            0         ['stack3_block4_attn_height_do\n","  (Add)                                                             wn_conv[0][0]',               \n","                                                                     'stack3_block4_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'stack3_block4_attn_channel_c\n","                                                                    onv[0][0]']                   \n","                                                                                                  \n"," global_average_pooling2d_2  (None, 1, 1, 320)            0         ['stack3_block4_attn_combine[0\n"," 7 (GlobalAveragePooling2D)                                         ][0]']                        \n","                                                                                                  \n"," stack3_block4_attn_reweigh  (None, 1, 1, 80)             25680     ['global_average_pooling2d_27[\n"," t_Conv_0 (Conv2D)                                                  0][0]']                       \n","                                                                                                  \n"," stack3_block4_attn_reweigh  (None, 1, 1, 80)             0         ['stack3_block4_attn_reweight_\n"," t_gelugelu (Activation)                                            Conv_0[0][0]']                \n","                                                                                                  \n"," stack3_block4_attn_reweigh  (None, 1, 1, 960)            77760     ['stack3_block4_attn_reweight_\n"," t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n","                                                                                                  \n"," reshape_27 (Reshape)        (None, 1, 1, 320, 3)         0         ['stack3_block4_attn_reweight_\n","                                                                    Conv_1[0][0]']                \n","                                                                                                  \n"," stack3_block4_attn_attenti  (None, 1, 1, 320, 3)         0         ['reshape_27[0][0]']          \n"," on_scores (Softmax)                                                                              \n","                                                                                                  \n"," tf.unstack_27 (TFOpLambda)  [(None, 1, 1, 320),          0         ['stack3_block4_attn_attention\n","                              (None, 1, 1, 320),                    _scores[0][0]']               \n","                              (None, 1, 1, 320)]                                                  \n","                                                                                                  \n"," multiply_193 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block4_attn_height_do\n","                                                                    wn_conv[0][0]',               \n","                                                                     'tf.unstack_27[0][0]']       \n","                                                                                                  \n"," multiply_194 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block4_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'tf.unstack_27[0][1]']       \n","                                                                                                  \n"," multiply_195 (Multiply)     (None, 7, 7, 320)            0         ['stack3_block4_attn_channel_c\n","                                                                    onv[0][0]',                   \n","                                                                     'tf.unstack_27[0][2]']       \n","                                                                                                  \n"," add_49 (Add)                (None, 7, 7, 320)            0         ['multiply_193[0][0]',        \n","                                                                     'multiply_194[0][0]',        \n","                                                                     'multiply_195[0][0]']        \n","                                                                                                  \n"," stack3_block4_attn_out_con  (None, 7, 7, 320)            102720    ['add_49[0][0]']              \n"," v (Conv2D)                                                                                       \n","                                                                                                  \n"," stack3_block4_attn_out (Ad  (None, 7, 7, 320)            0         ['stack3_block3_mlp_out[0][0]'\n"," d)                                                                 , 'stack3_block4_attn_out_conv\n","                                                                    [0][0]']                      \n","                                                                                                  \n"," stack3_block4_mlp_bn (Batc  (None, 7, 7, 320)            1280      ['stack3_block4_attn_out[0][0]\n"," hNormalization)                                                    ']                            \n","                                                                                                  \n"," stack3_block4_mlp_Conv_0 (  (None, 7, 7, 1280)           410880    ['stack3_block4_mlp_bn[0][0]']\n"," Conv2D)                                                                                          \n","                                                                                                  \n"," stack3_block4_mlp_gelugelu  (None, 7, 7, 1280)           0         ['stack3_block4_mlp_Conv_0[0][\n","  (Activation)                                                      0]']                          \n","                                                                                                  \n"," stack3_block4_mlp_Conv_1 (  (None, 7, 7, 320)            409920    ['stack3_block4_mlp_gelugelu[0\n"," Conv2D)                                                            ][0]']                        \n","                                                                                                  \n"," stack3_block4_mlp_out (Add  (None, 7, 7, 320)            0         ['stack3_block4_attn_out[0][0]\n"," )                                                                  ',                            \n","                                                                     'stack3_block4_mlp_Conv_1[0][\n","                                                                    0]']                          \n","                                                                                                  \n"," stack4_down_sample_pad (Ze  (None, 9, 9, 320)            0         ['stack3_block4_mlp_out[0][0]'\n"," roPadding2D)                                                       ]                             \n","                                                                                                  \n"," stack4_down_sample_conv (C  (None, 4, 4, 512)            1475072   ['stack4_down_sample_pad[0][0]\n"," onv2D)                                                             ']                            \n","                                                                                                  \n"," stack4_down_sample_bn (Bat  (None, 4, 4, 512)            2048      ['stack4_down_sample_conv[0][0\n"," chNormalization)                                                   ]']                           \n","                                                                                                  \n"," stack4_block1_attn_bn (Bat  (None, 4, 4, 512)            2048      ['stack4_down_sample_bn[0][0]'\n"," chNormalization)                                                   ]                             \n","                                                                                                  \n"," stack4_block1_attn_theta_h  (None, 4, 4, 512)            262656    ['stack4_block1_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack4_block1_attn_theta_w  (None, 4, 4, 512)            262656    ['stack4_block1_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack4_block1_attn_theta_h  (None, 4, 4, 512)            2048      ['stack4_block1_attn_theta_h_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack4_block1_attn_theta_w  (None, 4, 4, 512)            2048      ['stack4_block1_attn_theta_w_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack4_block1_attn_theta_h  (None, 4, 4, 512)            0         ['stack4_block1_attn_theta_h_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack4_block1_attn_theta_w  (None, 4, 4, 512)            0         ['stack4_block1_attn_theta_w_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack4_block1_attn_height_  (None, 4, 4, 512)            262144    ['stack4_block1_attn_bn[0][0]'\n"," conv (Conv2D)                                                      ]                             \n","                                                                                                  \n"," tf.math.cos_56 (TFOpLambda  (None, 4, 4, 512)            0         ['stack4_block1_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_56 (TFOpLambda  (None, 4, 4, 512)            0         ['stack4_block1_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," stack4_block1_attn_width_c  (None, 4, 4, 512)            262144    ['stack4_block1_attn_bn[0][0]'\n"," onv (Conv2D)                                                       ]                             \n","                                                                                                  \n"," tf.math.cos_57 (TFOpLambda  (None, 4, 4, 512)            0         ['stack4_block1_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_57 (TFOpLambda  (None, 4, 4, 512)            0         ['stack4_block1_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," multiply_196 (Multiply)     (None, 4, 4, 512)            0         ['stack4_block1_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.cos_56[0][0]']      \n","                                                                                                  \n"," multiply_197 (Multiply)     (None, 4, 4, 512)            0         ['stack4_block1_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.sin_56[0][0]']      \n","                                                                                                  \n"," multiply_198 (Multiply)     (None, 4, 4, 512)            0         ['stack4_block1_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.cos_57[0][0]']      \n","                                                                                                  \n"," multiply_199 (Multiply)     (None, 4, 4, 512)            0         ['stack4_block1_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.sin_57[0][0]']      \n","                                                                                                  \n"," concatenate_56 (Concatenat  (None, 4, 4, 1024)           0         ['multiply_196[0][0]',        \n"," e)                                                                  'multiply_197[0][0]']        \n","                                                                                                  \n"," concatenate_57 (Concatenat  (None, 4, 4, 1024)           0         ['multiply_198[0][0]',        \n"," e)                                                                  'multiply_199[0][0]']        \n","                                                                                                  \n"," stack4_block1_attn_height_  (None, 4, 10, 1024)          0         ['concatenate_56[0][0]']      \n"," down_pad (ZeroPadding2D)                                                                         \n","                                                                                                  \n"," stack4_block1_attn_width_d  (None, 10, 4, 1024)          0         ['concatenate_57[0][0]']      \n"," own_pad (ZeroPadding2D)                                                                          \n","                                                                                                  \n"," stack4_block1_attn_height_  (None, 4, 4, 512)            7168      ['stack4_block1_attn_height_do\n"," down_conv (Conv2D)                                                 wn_pad[0][0]']                \n","                                                                                                  \n"," stack4_block1_attn_width_d  (None, 4, 4, 512)            7168      ['stack4_block1_attn_width_dow\n"," own_conv (Conv2D)                                                  n_pad[0][0]']                 \n","                                                                                                  \n"," stack4_block1_attn_channel  (None, 4, 4, 512)            262144    ['stack4_block1_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack4_block1_attn_combine  (None, 4, 4, 512)            0         ['stack4_block1_attn_height_do\n","  (Add)                                                             wn_conv[0][0]',               \n","                                                                     'stack4_block1_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'stack4_block1_attn_channel_c\n","                                                                    onv[0][0]']                   \n","                                                                                                  \n"," global_average_pooling2d_2  (None, 1, 1, 512)            0         ['stack4_block1_attn_combine[0\n"," 8 (GlobalAveragePooling2D)                                         ][0]']                        \n","                                                                                                  \n"," stack4_block1_attn_reweigh  (None, 1, 1, 128)            65664     ['global_average_pooling2d_28[\n"," t_Conv_0 (Conv2D)                                                  0][0]']                       \n","                                                                                                  \n"," stack4_block1_attn_reweigh  (None, 1, 1, 128)            0         ['stack4_block1_attn_reweight_\n"," t_gelugelu (Activation)                                            Conv_0[0][0]']                \n","                                                                                                  \n"," stack4_block1_attn_reweigh  (None, 1, 1, 1536)           198144    ['stack4_block1_attn_reweight_\n"," t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n","                                                                                                  \n"," reshape_28 (Reshape)        (None, 1, 1, 512, 3)         0         ['stack4_block1_attn_reweight_\n","                                                                    Conv_1[0][0]']                \n","                                                                                                  \n"," stack4_block1_attn_attenti  (None, 1, 1, 512, 3)         0         ['reshape_28[0][0]']          \n"," on_scores (Softmax)                                                                              \n","                                                                                                  \n"," tf.unstack_28 (TFOpLambda)  [(None, 1, 1, 512),          0         ['stack4_block1_attn_attention\n","                              (None, 1, 1, 512),                    _scores[0][0]']               \n","                              (None, 1, 1, 512)]                                                  \n","                                                                                                  \n"," multiply_200 (Multiply)     (None, 4, 4, 512)            0         ['stack4_block1_attn_height_do\n","                                                                    wn_conv[0][0]',               \n","                                                                     'tf.unstack_28[0][0]']       \n","                                                                                                  \n"," multiply_201 (Multiply)     (None, 4, 4, 512)            0         ['stack4_block1_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'tf.unstack_28[0][1]']       \n","                                                                                                  \n"," multiply_202 (Multiply)     (None, 4, 4, 512)            0         ['stack4_block1_attn_channel_c\n","                                                                    onv[0][0]',                   \n","                                                                     'tf.unstack_28[0][2]']       \n","                                                                                                  \n"," add_50 (Add)                (None, 4, 4, 512)            0         ['multiply_200[0][0]',        \n","                                                                     'multiply_201[0][0]',        \n","                                                                     'multiply_202[0][0]']        \n","                                                                                                  \n"," stack4_block1_attn_out_con  (None, 4, 4, 512)            262656    ['add_50[0][0]']              \n"," v (Conv2D)                                                                                       \n","                                                                                                  \n"," stack4_block1_attn_out (Ad  (None, 4, 4, 512)            0         ['stack4_down_sample_bn[0][0]'\n"," d)                                                                 , 'stack4_block1_attn_out_conv\n","                                                                    [0][0]']                      \n","                                                                                                  \n"," stack4_block1_mlp_bn (Batc  (None, 4, 4, 512)            2048      ['stack4_block1_attn_out[0][0]\n"," hNormalization)                                                    ']                            \n","                                                                                                  \n"," stack4_block1_mlp_Conv_0 (  (None, 4, 4, 2048)           1050624   ['stack4_block1_mlp_bn[0][0]']\n"," Conv2D)                                                                                          \n","                                                                                                  \n"," stack4_block1_mlp_gelugelu  (None, 4, 4, 2048)           0         ['stack4_block1_mlp_Conv_0[0][\n","  (Activation)                                                      0]']                          \n","                                                                                                  \n"," stack4_block1_mlp_Conv_1 (  (None, 4, 4, 512)            1049088   ['stack4_block1_mlp_gelugelu[0\n"," Conv2D)                                                            ][0]']                        \n","                                                                                                  \n"," stack4_block1_mlp_out (Add  (None, 4, 4, 512)            0         ['stack4_block1_attn_out[0][0]\n"," )                                                                  ',                            \n","                                                                     'stack4_block1_mlp_Conv_1[0][\n","                                                                    0]']                          \n","                                                                                                  \n"," stack4_block2_attn_bn (Bat  (None, 4, 4, 512)            2048      ['stack4_block1_mlp_out[0][0]'\n"," chNormalization)                                                   ]                             \n","                                                                                                  \n"," stack4_block2_attn_theta_h  (None, 4, 4, 512)            262656    ['stack4_block2_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack4_block2_attn_theta_w  (None, 4, 4, 512)            262656    ['stack4_block2_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack4_block2_attn_theta_h  (None, 4, 4, 512)            2048      ['stack4_block2_attn_theta_h_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack4_block2_attn_theta_w  (None, 4, 4, 512)            2048      ['stack4_block2_attn_theta_w_c\n"," _bn (BatchNormalization)                                           onv[0][0]']                   \n","                                                                                                  \n"," stack4_block2_attn_theta_h  (None, 4, 4, 512)            0         ['stack4_block2_attn_theta_h_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack4_block2_attn_theta_w  (None, 4, 4, 512)            0         ['stack4_block2_attn_theta_w_b\n"," _relu (Activation)                                                 n[0][0]']                     \n","                                                                                                  \n"," stack4_block2_attn_height_  (None, 4, 4, 512)            262144    ['stack4_block2_attn_bn[0][0]'\n"," conv (Conv2D)                                                      ]                             \n","                                                                                                  \n"," tf.math.cos_58 (TFOpLambda  (None, 4, 4, 512)            0         ['stack4_block2_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_58 (TFOpLambda  (None, 4, 4, 512)            0         ['stack4_block2_attn_theta_h_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," stack4_block2_attn_width_c  (None, 4, 4, 512)            262144    ['stack4_block2_attn_bn[0][0]'\n"," onv (Conv2D)                                                       ]                             \n","                                                                                                  \n"," tf.math.cos_59 (TFOpLambda  (None, 4, 4, 512)            0         ['stack4_block2_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," tf.math.sin_59 (TFOpLambda  (None, 4, 4, 512)            0         ['stack4_block2_attn_theta_w_r\n"," )                                                                  elu[0][0]']                   \n","                                                                                                  \n"," multiply_203 (Multiply)     (None, 4, 4, 512)            0         ['stack4_block2_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.cos_58[0][0]']      \n","                                                                                                  \n"," multiply_204 (Multiply)     (None, 4, 4, 512)            0         ['stack4_block2_attn_height_co\n","                                                                    nv[0][0]',                    \n","                                                                     'tf.math.sin_58[0][0]']      \n","                                                                                                  \n"," multiply_205 (Multiply)     (None, 4, 4, 512)            0         ['stack4_block2_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.cos_59[0][0]']      \n","                                                                                                  \n"," multiply_206 (Multiply)     (None, 4, 4, 512)            0         ['stack4_block2_attn_width_con\n","                                                                    v[0][0]',                     \n","                                                                     'tf.math.sin_59[0][0]']      \n","                                                                                                  \n"," concatenate_58 (Concatenat  (None, 4, 4, 1024)           0         ['multiply_203[0][0]',        \n"," e)                                                                  'multiply_204[0][0]']        \n","                                                                                                  \n"," concatenate_59 (Concatenat  (None, 4, 4, 1024)           0         ['multiply_205[0][0]',        \n"," e)                                                                  'multiply_206[0][0]']        \n","                                                                                                  \n"," stack4_block2_attn_height_  (None, 4, 10, 1024)          0         ['concatenate_58[0][0]']      \n"," down_pad (ZeroPadding2D)                                                                         \n","                                                                                                  \n"," stack4_block2_attn_width_d  (None, 10, 4, 1024)          0         ['concatenate_59[0][0]']      \n"," own_pad (ZeroPadding2D)                                                                          \n","                                                                                                  \n"," stack4_block2_attn_height_  (None, 4, 4, 512)            7168      ['stack4_block2_attn_height_do\n"," down_conv (Conv2D)                                                 wn_pad[0][0]']                \n","                                                                                                  \n"," stack4_block2_attn_width_d  (None, 4, 4, 512)            7168      ['stack4_block2_attn_width_dow\n"," own_conv (Conv2D)                                                  n_pad[0][0]']                 \n","                                                                                                  \n"," stack4_block2_attn_channel  (None, 4, 4, 512)            262144    ['stack4_block2_attn_bn[0][0]'\n"," _conv (Conv2D)                                                     ]                             \n","                                                                                                  \n"," stack4_block2_attn_combine  (None, 4, 4, 512)            0         ['stack4_block2_attn_height_do\n","  (Add)                                                             wn_conv[0][0]',               \n","                                                                     'stack4_block2_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'stack4_block2_attn_channel_c\n","                                                                    onv[0][0]']                   \n","                                                                                                  \n"," global_average_pooling2d_2  (None, 1, 1, 512)            0         ['stack4_block2_attn_combine[0\n"," 9 (GlobalAveragePooling2D)                                         ][0]']                        \n","                                                                                                  \n"," stack4_block2_attn_reweigh  (None, 1, 1, 128)            65664     ['global_average_pooling2d_29[\n"," t_Conv_0 (Conv2D)                                                  0][0]']                       \n","                                                                                                  \n"," stack4_block2_attn_reweigh  (None, 1, 1, 128)            0         ['stack4_block2_attn_reweight_\n"," t_gelugelu (Activation)                                            Conv_0[0][0]']                \n","                                                                                                  \n"," stack4_block2_attn_reweigh  (None, 1, 1, 1536)           198144    ['stack4_block2_attn_reweight_\n"," t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n","                                                                                                  \n"," reshape_29 (Reshape)        (None, 1, 1, 512, 3)         0         ['stack4_block2_attn_reweight_\n","                                                                    Conv_1[0][0]']                \n","                                                                                                  \n"," stack4_block2_attn_attenti  (None, 1, 1, 512, 3)         0         ['reshape_29[0][0]']          \n"," on_scores (Softmax)                                                                              \n","                                                                                                  \n"," tf.unstack_29 (TFOpLambda)  [(None, 1, 1, 512),          0         ['stack4_block2_attn_attention\n","                              (None, 1, 1, 512),                    _scores[0][0]']               \n","                              (None, 1, 1, 512)]                                                  \n","                                                                                                  \n"," multiply_207 (Multiply)     (None, 4, 4, 512)            0         ['stack4_block2_attn_height_do\n","                                                                    wn_conv[0][0]',               \n","                                                                     'tf.unstack_29[0][0]']       \n","                                                                                                  \n"," multiply_208 (Multiply)     (None, 4, 4, 512)            0         ['stack4_block2_attn_width_dow\n","                                                                    n_conv[0][0]',                \n","                                                                     'tf.unstack_29[0][1]']       \n","                                                                                                  \n"," multiply_209 (Multiply)     (None, 4, 4, 512)            0         ['stack4_block2_attn_channel_c\n","                                                                    onv[0][0]',                   \n","                                                                     'tf.unstack_29[0][2]']       \n","                                                                                                  \n"," add_51 (Add)                (None, 4, 4, 512)            0         ['multiply_207[0][0]',        \n","                                                                     'multiply_208[0][0]',        \n","                                                                     'multiply_209[0][0]']        \n","                                                                                                  \n"," stack4_block2_attn_out_con  (None, 4, 4, 512)            262656    ['add_51[0][0]']              \n"," v (Conv2D)                                                                                       \n","                                                                                                  \n"," stack4_block2_attn_out (Ad  (None, 4, 4, 512)            0         ['stack4_block1_mlp_out[0][0]'\n"," d)                                                                 , 'stack4_block2_attn_out_conv\n","                                                                    [0][0]']                      \n","                                                                                                  \n"," stack4_block2_mlp_bn (Batc  (None, 4, 4, 512)            2048      ['stack4_block2_attn_out[0][0]\n"," hNormalization)                                                    ']                            \n","                                                                                                  \n"," stack4_block2_mlp_Conv_0 (  (None, 4, 4, 2048)           1050624   ['stack4_block2_mlp_bn[0][0]']\n"," Conv2D)                                                                                          \n","                                                                                                  \n"," stack4_block2_mlp_gelugelu  (None, 4, 4, 2048)           0         ['stack4_block2_mlp_Conv_0[0][\n","  (Activation)                                                      0]']                          \n","                                                                                                  \n"," stack4_block2_mlp_Conv_1 (  (None, 4, 4, 512)            1049088   ['stack4_block2_mlp_gelugelu[0\n"," Conv2D)                                                            ][0]']                        \n","                                                                                                  \n"," stack4_block2_mlp_out (Add  (None, 4, 4, 512)            0         ['stack4_block2_attn_out[0][0]\n"," )                                                                  ',                            \n","                                                                     'stack4_block2_mlp_Conv_1[0][\n","                                                                    0]']                          \n","                                                                                                  \n"," output_bn (BatchNormalizat  (None, 4, 4, 512)            2048      ['stack4_block2_mlp_out[0][0]'\n"," ion)                                                               ]                             \n","                                                                                                  \n"," avg_pool (GlobalAveragePoo  (None, 512)                  0         ['output_bn[0][0]']           \n"," ling2D)                                                                                          \n","                                                                                                  \n"," predictions (Dense)         (None, 1000)                 513000    ['avg_pool[0][0]']            \n","                                                                                                  \n"," tf.reshape_2 (TFOpLambda)   (None, 31, 31, 1)            0         ['predictions[0][0]']         \n","                                                                                                  \n"," self_attention_14 (SelfAtt  (None, 31, 31, 1)            3         ['tf.reshape_2[0][0]']        \n"," ention)                                                                                          \n","                                                                                                  \n"," conv2d_48 (Conv2D)          (None, 31, 31, 64)           640       ['self_attention_14[0][0]']   \n","                                                                                                  \n"," batch_normalization_30 (Ba  (None, 31, 31, 64)           256       ['conv2d_48[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_30 (Activation)  (None, 31, 31, 64)           0         ['batch_normalization_30[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_49 (Conv2D)          (None, 31, 31, 64)           36928     ['activation_30[0][0]']       \n","                                                                                                  \n"," batch_normalization_31 (Ba  (None, 31, 31, 64)           256       ['conv2d_49[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_31 (Activation)  (None, 31, 31, 64)           0         ['batch_normalization_31[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_50 (Conv2D)          (None, 31, 31, 64)           128       ['tf.reshape_2[0][0]']        \n","                                                                                                  \n"," add_52 (Add)                (None, 31, 31, 64)           0         ['activation_31[0][0]',       \n","                                                                     'conv2d_50[0][0]']           \n","                                                                                                  \n"," self_attention_15 (SelfAtt  (None, 31, 31, 64)           12288     ['add_52[0][0]']              \n"," ention)                                                                                          \n","                                                                                                  \n"," conv2d_51 (Conv2D)          (None, 31, 31, 64)           36928     ['self_attention_15[0][0]']   \n","                                                                                                  \n"," batch_normalization_32 (Ba  (None, 31, 31, 64)           256       ['conv2d_51[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_32 (Activation)  (None, 31, 31, 64)           0         ['batch_normalization_32[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_52 (Conv2D)          (None, 31, 31, 64)           36928     ['activation_32[0][0]']       \n","                                                                                                  \n"," batch_normalization_33 (Ba  (None, 31, 31, 64)           256       ['conv2d_52[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_33 (Activation)  (None, 31, 31, 64)           0         ['batch_normalization_33[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_53 (Conv2D)          (None, 31, 31, 64)           4160      ['add_52[0][0]']              \n","                                                                                                  \n"," add_53 (Add)                (None, 31, 31, 64)           0         ['activation_33[0][0]',       \n","                                                                     'conv2d_53[0][0]']           \n","                                                                                                  \n"," self_attention_16 (SelfAtt  (None, 31, 31, 64)           12288     ['add_53[0][0]']              \n"," ention)                                                                                          \n","                                                                                                  \n"," conv2d_54 (Conv2D)          (None, 31, 31, 64)           36928     ['self_attention_16[0][0]']   \n","                                                                                                  \n"," batch_normalization_34 (Ba  (None, 31, 31, 64)           256       ['conv2d_54[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_34 (Activation)  (None, 31, 31, 64)           0         ['batch_normalization_34[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_55 (Conv2D)          (None, 31, 31, 64)           36928     ['activation_34[0][0]']       \n","                                                                                                  \n"," batch_normalization_35 (Ba  (None, 31, 31, 64)           256       ['conv2d_55[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_35 (Activation)  (None, 31, 31, 64)           0         ['batch_normalization_35[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_56 (Conv2D)          (None, 31, 31, 64)           4160      ['add_53[0][0]']              \n","                                                                                                  \n"," add_54 (Add)                (None, 31, 31, 64)           0         ['activation_35[0][0]',       \n","                                                                     'conv2d_56[0][0]']           \n","                                                                                                  \n"," max_pooling2d_6 (MaxPoolin  (None, 15, 15, 64)           0         ['add_54[0][0]']              \n"," g2D)                                                                                             \n","                                                                                                  \n"," flatten_2 (Flatten)         (None, 14400)                0         ['max_pooling2d_6[0][0]']     \n","                                                                                                  \n"," dense_4 (Dense)             (None, 256)                  3686656   ['flatten_2[0][0]']           \n","                                                                                                  \n"," dropout_2 (Dropout)         (None, 256)                  0         ['dense_4[0][0]']             \n","                                                                                                  \n"," self_attention_17 (SelfAtt  (None, 256)                  196608    ['dropout_2[0][0]']           \n"," ention)                                                                                          \n","                                                                                                  \n"," add_55 (Add)                (None, 256)                  0         ['dropout_2[0][0]',           \n","                                                                     'self_attention_17[0][0]']   \n","                                                                                                  \n"," positional_encoding_2 (Pos  (1, 1, 256, 256)             256       ['add_55[0][0]']              \n"," itionalEncoding)                                                                                 \n","                                                                                                  \n"," add_56 (Add)                (None, 1, 256, 256)          0         ['add_55[0][0]',              \n","                                                                     'positional_encoding_2[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," dense_5 (Dense)             (None, 1, 256, 26)           6682      ['add_56[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 21327781 (81.36 MB)\n","Trainable params: 21302437 (81.26 MB)\n","Non-trainable params: 25344 (99.00 KB)\n","__________________________________________________________________________________________________\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Flatten, Dense, Dropout, Layer, Add, Conv2D, BatchNormalization, Activation, MaxPooling2D\n","from tensorflow.keras import Input, Model\n","# Import your wave_mlp module\n","\n","class SelfAttention(Layer):\n","    def __init__(self, **kwargs):\n","        super(SelfAttention, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.W_q = self.add_weight(name='W_q', shape=(input_shape[-1], input_shape[-1]),\n","                                   initializer='uniform', trainable=True)\n","        self.W_k = self.add_weight(name='W_k', shape=(input_shape[-1], input_shape[-1]),\n","                                   initializer='uniform', trainable=True)\n","        self.W_v = self.add_weight(name='W_v', shape=(input_shape[-1], input_shape[-1]),\n","                                   initializer='uniform', trainable=True)\n","        super(SelfAttention, self).build(input_shape)\n","\n","    def call(self, x):\n","        q = x @ self.W_q\n","        k = x @ self.W_k\n","        v = x @ self.W_v\n","\n","        attn_score = tf.matmul(q, k, transpose_b=True)\n","        attn_score = tf.nn.softmax(attn_score, axis=-1)\n","\n","        output = attn_score @ v\n","        return output\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","class PositionalEncoding(Layer):\n","    def __init__(self, input_shape, **kwargs):\n","        super(PositionalEncoding, self).__init__(**kwargs)\n","        self.input_shape_ = input_shape\n","\n","    def build(self, input_shape):\n","        self.positional_encoding = self.add_weight(name='positional_encoding',\n","                                                   shape=(1, *self.input_shape_[1:], 1),\n","                                                   initializer='uniform',\n","                                                   trainable=True)\n","        super(PositionalEncoding, self).build(input_shape)\n","\n","    def call(self, x):\n","        return x + self.positional_encoding\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","def custom_head(input_tensor, num_classes):\n","    x = Flatten()(input_tensor)\n","    x = Dense(256, activation='relu')(x)  # Add your own dense layers\n","    x = Dropout(0.5)(x)  # Add dropout for regularization\n","\n","    # Add self-attention mechanism\n","    self_attention = SelfAttention()(x)\n","    x = Add()([x, self_attention])\n","\n","    # Add 2D positional encoding\n","    position_encoding = PositionalEncoding(input_shape=(1, 1, 256))(x)  # Adjust the input_shape\n","    x = Add()([x, position_encoding])\n","\n","    x = Dense(num_classes, activation='softmax')(x)\n","    return x\n","\n","def stage_block(x, filters, num_blocks, attention=True, hdc=False):\n","    # Check if the input is flattened\n","    if len(x.shape) == 2:\n","        # Reshape to 4D tensor\n","        x = tf.reshape(x, [-1, int(x.shape[1] ** 0.5), int(x.shape[1] ** 0.5), 1])\n","\n","    for _ in range(num_blocks):\n","        # Residual block\n","        residual = x\n","\n","        # Global self-attention\n","        if attention:\n","            x = SelfAttention()(x)\n","\n","        # Convolutional layer with Hybrid Dilated Convolution (HDC)\n","        if hdc:\n","            x = Conv2D(filters, kernel_size=(3, 3), dilation_rate=(2, 2), padding='same')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)\n","            x = Conv2D(filters, kernel_size=(3, 3), dilation_rate=(3, 3), padding='same')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)\n","        else:\n","            x = Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)\n","\n","        # Residual connection (adjusting the residual size)\n","        residual = Conv2D(filters, kernel_size=(1, 1), padding='same')(residual)\n","        x = Add()([x, residual])\n","\n","    # MaxPooling for downsampling\n","    x = MaxPooling2D(pool_size=(2, 2))(x)\n","\n","    return x\n","\n","def modify_wave_mlp(input_shape, num_classes):\n","    # Load the WaveMLP model without the top layers (head)\n","    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n","\n","    # Stage 1 with HDC\n","    x = stage_block(mm_headless.output, filters=64, num_blocks=3, hdc=True)\n","\n","    # Stage 2 with HDC\n","    x = stage_block(x, filters=128, num_blocks=3, hdc=True)\n","\n","    # Stage 3 with HDC\n","    x = stage_block(x, filters=256, num_blocks=3, hdc=True)\n","\n","    # Stage 4 (Residual blocks)\n","    x = stage_block(x, filters=512, num_blocks=3, attention=False, hdc=False)\n","\n","    # Custom head\n","    head_output = custom_head(x, num_classes)\n","\n","    # Create the custom model by combining the base model and the custom head\n","    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n","\n","    return custom_model\n","\n","# Example usage:\n","input_shape = (112, 112, 3)\n","num_classes = 26  # Adjust based on your task\n","\n","custom_model = modify_wave_mlp(input_shape, num_classes)\n","custom_model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T08:38:22.615587Z","iopub.status.busy":"2023-12-10T08:38:22.615116Z","iopub.status.idle":"2023-12-10T08:38:27.785752Z","shell.execute_reply":"2023-12-10T08:38:27.784607Z","shell.execute_reply.started":"2023-12-10T08:38:22.615546Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Flatten, Dense, Dropout, Layer, Add, Conv2D, BatchNormalization, Activation, MaxPooling2D\n","from tensorflow.keras import Input, Model\n","# Import your wave_mlp module\n","\n","class SelfAttention(Layer):\n","    def __init__(self, **kwargs):\n","        super(SelfAttention, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.W_q = self.add_weight(name='W_q', shape=(input_shape[-1], input_shape[-1]),\n","                                   initializer='uniform', trainable=True)\n","        self.W_k = self.add_weight(name='W_k', shape=(input_shape[-1], input_shape[-1]),\n","                                   initializer='uniform', trainable=True)\n","        self.W_v = self.add_weight(name='W_v', shape=(input_shape[-1], input_shape[-1]),\n","                                   initializer='uniform', trainable=True)\n","        super(SelfAttention, self).build(input_shape)\n","\n","    def call(self, x):\n","        q = x @ self.W_q\n","        k = x @ self.W_k\n","        v = x @ self.W_v\n","\n","        attn_score = tf.matmul(q, k, transpose_b=True)\n","        attn_score = tf.nn.softmax(attn_score, axis=-1)\n","\n","        output = attn_score @ v\n","        return output\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","class PositionalEncoding(Layer):\n","    def __init__(self, input_shape, **kwargs):\n","        super(PositionalEncoding, self).__init__(**kwargs)\n","        self.input_shape_ = input_shape\n","\n","    def build(self, input_shape):\n","        self.positional_encoding = self.add_weight(name='positional_encoding',\n","                                                   shape=(1, *self.input_shape_[1:], 1),\n","                                                   initializer='uniform',\n","                                                   trainable=True)\n","        super(PositionalEncoding, self).build(input_shape)\n","\n","    def call(self, x):\n","        return x + self.positional_encoding\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","def custom_head(input_tensor, num_classes):\n","    x = Flatten()(input_tensor)\n","    x = Dense(256, activation='relu')(x)  # Add your own dense layers\n","    x = Dropout(0.5)(x)  # Add dropout for regularization\n","\n","    # Add self-attention mechanism\n","    self_attention = SelfAttention()(x)\n","    x = Add()([x, self_attention])\n","\n","    # Add 2D positional encoding\n","    position_encoding = PositionalEncoding(input_shape=(1, 1, 256))(x)  # Adjust the input_shape\n","    x = Add()([x, position_encoding])\n","\n","    x = Dense(num_classes, activation='softmax')(x)\n","    return x\n","\n","def stage_block(x, filters, num_blocks, attention=True):\n","    # Check if the input is flattened\n","    if len(x.shape) == 2:\n","        # Reshape to 4D tensor\n","        x = tf.reshape(x, [-1, int(x.shape[1] ** 0.5), int(x.shape[1] ** 0.5), 1])\n","\n","    for _ in range(num_blocks):\n","        # Residual block\n","        residual = x\n","\n","        # Global self-attention\n","        if attention:\n","            x = SelfAttention()(x)\n","\n","        # Convolutional layer\n","        x = Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","\n","        # Residual connection (adjusting the residual size)\n","        residual = Conv2D(filters, kernel_size=(1, 1), padding='same')(residual)\n","        x = Add()([x, residual])\n","\n","    # MaxPooling for downsampling\n","    x = MaxPooling2D(pool_size=(2, 2))(x)\n","\n","    return x\n","\n","\n","\n","def modify_wave_mlp(input_shape, num_classes):\n","    # Load the WaveMLP model without the top layers (head)\n","    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n","\n","    # Stage 1\n","    x = stage_block(mm_headless.output, filters=64, num_blocks=3)\n","\n","    # Stage 2\n","    x = stage_block(x, filters=128, num_blocks=3)\n","\n","    # Stage 3\n","    x = stage_block(x, filters=256, num_blocks=3)\n","\n","    # Stage 4 (Residual blocks)\n","    x = stage_block(x, filters=512, num_blocks=3, attention=False)\n","\n","    # Custom head\n","    head_output = custom_head(x, num_classes)\n","\n","    # Create the custom model by combining the base model and the custom head\n","    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n","\n","    return custom_model\n","\n","# Example usage:\n","input_shape = (112, 112, 3)\n","num_classes = 26  # Adjust based on your task\n","\n","custom_model = modify_wave_mlp(input_shape, num_classes)\n","custom_model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T10:55:57.062627Z","iopub.status.busy":"2023-12-10T10:55:57.061802Z","iopub.status.idle":"2023-12-10T10:56:01.537445Z","shell.execute_reply":"2023-12-10T10:56:01.536322Z","shell.execute_reply.started":"2023-12-10T10:55:57.062593Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Flatten, Dense, Dropout, Layer, Add, Conv2D, BatchNormalization, Activation, MaxPooling2D\n","from tensorflow.keras import Input, Model\n","\n","# Define the MultiheadSelfAttentionBlock as a custom layer\n","class MultiheadSelfAttentionBlock(Layer):\n","    def __init__(self, embedding_dim=256, num_heads=4, attn_dropout=0.1):\n","        super(MultiheadSelfAttentionBlock, self).__init__()\n","\n","        # Create the Layer Normalization (LN)\n","        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        # Create the Multi-Head Attention (MSA) layer\n","        self.multihead_attn = tf.keras.layers.MultiHeadAttention(\n","            num_heads=num_heads,\n","            key_dim=embedding_dim // num_heads,\n","            dropout=attn_dropout\n","        )\n","\n","    def call(self, x):\n","        # Reshape to (batch_size, sequence_length, features)\n","        original_shape = tf.shape(x)\n","        if len(original_shape) == 2:\n","            x = tf.expand_dims(x, axis=1)\n","        x = self.layer_norm(x)\n","        attn_output = self.multihead_attn(x, x, x)\n","\n","        # Reshape back to the original shape\n","        attn_output = tf.reshape(attn_output, original_shape)\n","\n","        return attn_output\n","\n","def stage_block(x, filters, num_blocks, attention=True):\n","    # Check if the input is flattened\n","    if len(x.shape) == 2:\n","        # Reshape to 4D tensor\n","        x = tf.reshape(x, [-1, int(x.shape[1] ** 0.5), int(x.shape[1] ** 0.5), 1])\n","\n","    for _ in range(num_blocks):\n","        # Residual block\n","        residual = x\n","\n","        # Global self-attention\n","        if attention:\n","            x = MultiheadSelfAttentionBlock()(x)\n","\n","        # Convolutional layer\n","        x = Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","\n","        # Residual connection (adjusting the residual size)\n","        residual = Conv2D(filters, kernel_size=(1, 1), padding='same')(residual)\n","        x = Add()([x, residual])\n","\n","    # MaxPooling for downsampling\n","    x = MaxPooling2D(pool_size=(2, 2))(x)\n","\n","    return x\n","\n","\n","def custom_head(input_tensor, num_classes):\n","    x = Flatten()(input_tensor)\n","    x = Dense(256, activation='relu')(x)\n","    x = Dropout(0.5)(x)\n","\n","    # Add self-attention mechanism\n","    self_attention = MultiheadSelfAttentionBlock()(x)\n","    x = Add()([x, self_attention])\n","\n","    # Add 2D positional encoding\n","    position_encoding = PositionalEncoding(input_shape=(1, 1, 256))(x)\n","    x = Add()([x, position_encoding])\n","\n","    # Add Multihead Self-Attention Block\n","    multihead_attention_block = MultiheadSelfAttentionBlock(embedding_dim=256, num_heads=4, attn_dropout=0.1)\n","    x = multihead_attention_block(x)\n","\n","    x = Dense(num_classes, activation='softmax')(x)\n","    return x\n","\n","def modify_wave_mlp(input_shape, num_classes):\n","    # Load the WaveMLP model without the top layers (head)\n","    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n","\n","    # Stage 1\n","    x = stage_block(mm_headless.output, filters=64, num_blocks=3)\n","\n","    # Stage 2\n","    x = stage_block(x, filters=128, num_blocks=3)\n","\n","    # Stage 3\n","    x = stage_block(x, filters=256, num_blocks=3)\n","\n","    # Stage 4 (Residual blocks)\n","    x = stage_block(x, filters=512, num_blocks=3, attention=False)\n","\n","    # Custom head\n","    head_output = custom_head(x, num_classes)\n","\n","    # Create the custom model by combining the base model and the custom head\n","    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n","\n","    return custom_model\n","\n","# Example usage:\n","input_shape = (112, 112, 3)\n","num_classes = 26  # Adjust based on your task\n","\n","custom_model = modify_wave_mlp(input_shape, num_classes)\n","custom_model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Flatten, Dense, Dropout, Layer, Add, Conv2D, BatchNormalization, Activation, MaxPooling2D\n","from tensorflow.keras import Input, Model\n","\n","# Define the MultiheadSelfAttentionBlock as a custom layer\n","class MultiheadSelfAttentionBlock(Layer):\n","    def __init__(self, embedding_dim=256, num_heads=4, attn_dropout=0.1):\n","        super(MultiheadSelfAttentionBlock, self).__init__()\n","\n","        # Create the Layer Normalization (LN)\n","        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        # Create the Multi-Head Attention (MSA) layer\n","        self.multihead_attn = tf.keras.layers.MultiHeadAttention(\n","            num_heads=num_heads,\n","            key_dim=embedding_dim // num_heads,\n","            dropout=attn_dropout\n","        )\n","\n","    def call(self, x):\n","        x = self.layer_norm(x)\n","        attn_output = self.multihead_attn(x, x, x)\n","        return attn_output\n","\n","# Your existing code...\n","\n","def custom_head(input_tensor, num_classes):\n","    x = Flatten()(input_tensor)\n","    x = Dense(256, activation='relu')(x)\n","    x = Dropout(0.5)(x)\n","\n","    # Add self-attention mechanism\n","    self_attention = SelfAttention()(x)\n","    x = Add()([x, self_attention])\n","\n","    # Add 2D positional encoding\n","    position_encoding = PositionalEncoding(input_shape=(1, 1, 256))(x)\n","    x = Add()([x, position_encoding])\n","\n","    # Add Multihead Self-Attention Block\n","    multihead_attention_block = MultiheadSelfAttentionBlock(embedding_dim=256, num_heads=4, attn_dropout=0.1)\n","    x = multihead_attention_block(x)\n","\n","    x = Dense(num_classes, activation='softmax')(x)\n","    return x\n","\n","def modify_wave_mlp(input_shape, num_classes):\n","    # Load the WaveMLP model without the top layers (head)\n","    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n","\n","    # Stage 1\n","    x = stage_block(mm_headless.output, filters=64, num_blocks=3)\n","\n","    # Stage 2\n","    x = stage_block(x, filters=128, num_blocks=3)\n","\n","    # Stage 3\n","    x = stage_block(x, filters=256, num_blocks=3)\n","\n","    # Stage 4 (Residual blocks)\n","    x = stage_block(x, filters=512, num_blocks=3, attention=False)\n","\n","    # Custom head\n","    head_output = custom_head(x, num_classes)\n","\n","    # Create the custom model by combining the base model and the custom head\n","    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n","\n","    return custom_model\n","\n","# Example usage:\n","input_shape = (112, 112, 3)\n","num_classes = 26  # Adjust based on your task\n","\n","custom_model = modify_wave_mlp(input_shape, num_classes)\n","custom_model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T21:47:01.175011Z","iopub.status.busy":"2023-12-09T21:47:01.174616Z","iopub.status.idle":"2023-12-09T21:47:01.358735Z","shell.execute_reply":"2023-12-09T21:47:01.357466Z","shell.execute_reply.started":"2023-12-09T21:47:01.174978Z"},"trusted":true},"outputs":[],"source":["from keras_self_attention import MultiHeadAttention\n","\n","def custom_head(input_tensor, num_classes):\n","    x = Flatten()(input_tensor)\n","    \n","    # Add multi-headed self-attention layer\n","    attention = MultiHeadAttention(head_size=128, num_heads=4)(input_tensor)\n","    x = Concatenate()([input_tensor, attention])  # Concatenate attention output with input\n","    \n","    x = Dense(256, activation='relu')(x)  # Add your own dense layers\n","    x = Dropout(0.5)(x)  # Add dropout for regularization\n","    x = Dense(num_classes, activation='softmax')(x)\n","    return x\n","\n","def modify_wave_mlp(input_shape, num_classes):\n","    # Load the WaveMLP model without the top layers (head)\n","    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n","\n","    # Add your custom head\n","    head_output = custom_head(mm_headless.output, num_classes)\n","\n","    # Create the custom model by combining the base model and the custom head\n","    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n","\n","    return custom_model\n","\n","# Example usage:\n","input_shape = (112, 112, 3)\n","num_classes = 26  # Adjust based on your task\n","\n","custom_model = modify_wave_mlp(input_shape, num_classes)\n","custom_model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T07:24:53.094423Z","iopub.status.busy":"2023-12-10T07:24:53.094154Z","iopub.status.idle":"2023-12-10T07:25:12.393224Z","shell.execute_reply":"2023-12-10T07:25:12.391820Z","shell.execute_reply.started":"2023-12-10T07:24:53.094398Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.layers import Flatten, Dense, Dropout, LayerNormalization, MultiHeadAttention, Add, Input, Embedding\n","from tensorflow.keras.models import Model\n","\n","def custom_head(input_tensor, num_classes):\n","    x = Flatten()(input_tensor)\n","    x = Dense(256, activation='relu')(x)\n","    x = Dropout(0.5)(x)\n","\n","    # Self-Attention Mechanism\n","    att_input = Input(shape=(x.shape[1],))\n","    att_output = MultiHeadAttention(num_heads=8, key_dim=16)([att_input, att_input])\n","    att_output = Add()([att_input, att_output])\n","    att_output = LayerNormalization(epsilon=1e-6)(att_output)\n","\n","    # Positional Encoding\n","    pos_input = Input(shape=(x.shape[1],))\n","    pos_output = Embedding(input_dim=x.shape[1], output_dim=256)(pos_input)\n","    pos_output = Add()([att_output, pos_output])\n","    pos_output = LayerNormalization(epsilon=1e-6)(pos_output)\n","\n","    # Combine attention and positional encoding\n","    x = Add()([att_output, pos_output])\n","\n","    # Fully connected layer for classification\n","    x = Dense(num_classes, activation='softmax')(x)\n","    return x\n","\n","def modify_wave_mlp(input_shape, num_classes):\n","    # Load the WaveMLP model without the top layers (head)\n","    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n","\n","    # Add your custom head\n","    head_output = custom_head(mm_headless.output, num_classes)\n","\n","    # Create the custom model by combining the base model and the custom head\n","    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n","\n","    return custom_model\n","\n","# Example usage:\n","input_shape = (112, 112, 3)\n","num_classes = 26  # Adjust based on your task\n","\n","custom_model = modify_wave_mlp(input_shape, num_classes)\n","# custom_model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T22:03:08.930847Z","iopub.status.busy":"2023-12-09T22:03:08.930440Z","iopub.status.idle":"2023-12-09T22:03:12.648882Z","shell.execute_reply":"2023-12-09T22:03:12.648066Z","shell.execute_reply.started":"2023-12-09T22:03:08.930816Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Flatten, Dense, Dropout, Layer, Add\n","from tensorflow.keras import Input, Model\n"," # Import your wave_mlp module\n","\n","class SelfAttention(Layer):\n","    def __init__(self, **kwargs):\n","        super(SelfAttention, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.W_q = self.add_weight(name='W_q', shape=(input_shape[-1], input_shape[-1]),\n","                                   initializer='uniform', trainable=True)\n","        self.W_k = self.add_weight(name='W_k', shape=(input_shape[-1], input_shape[-1]),\n","                                   initializer='uniform', trainable=True)\n","        self.W_v = self.add_weight(name='W_v', shape=(input_shape[-1], input_shape[-1]),\n","                                   initializer='uniform', trainable=True)\n","        super(SelfAttention, self).build(input_shape)\n","\n","    def call(self, x):\n","        q = x @ self.W_q\n","        k = x @ self.W_k\n","        v = x @ self.W_v\n","\n","        attn_score = tf.matmul(q, k, transpose_b=True)\n","        attn_score = tf.nn.softmax(attn_score, axis=-1)\n","\n","        output = attn_score @ v\n","        return output\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","class PositionalEncoding(Layer):\n","    def __init__(self, input_shape, **kwargs):\n","        super(PositionalEncoding, self).__init__(**kwargs)\n","        self.input_shape_ = input_shape\n","\n","    def build(self, input_shape):\n","        self.positional_encoding = self.add_weight(name='positional_encoding',\n","                                                   shape=(1, *self.input_shape_[1:], 1),\n","                                                   initializer='uniform',\n","                                                   trainable=True)\n","        super(PositionalEncoding, self).build(input_shape)\n","\n","    def call(self, x):\n","        return x + self.positional_encoding\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","def custom_head(input_tensor, num_classes):\n","    x = Flatten()(input_tensor)\n","    x = Dense(256, activation='relu')(x)  # Add your own dense layers\n","    x = Dropout(0.5)(x)  # Add dropout for regularization\n","\n","    # Add self-attention mechanism\n","    self_attention = SelfAttention()(x)\n","    x = Add()([x, self_attention])\n","\n","    # Add 2D positional encoding\n","    position_encoding = PositionalEncoding(input_shape=(1, 1, 256))(x)  # Adjust the input_shape\n","    x = Add()([x, position_encoding])\n","\n","    x = Dense(num_classes, activation='softmax')(x)\n","    return x\n","\n","def modify_wave_mlp(input_shape, num_classes):\n","    # Load the WaveMLP model without the top layers (head)\n","    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n","\n","    # Add your custom head\n","    head_output = custom_head(mm_headless.output, num_classes)\n","\n","    # Create the custom model by combining the base model and the custom head\n","    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n","\n","    return custom_model\n","\n","# Example usage:\n","input_shape = (112, 112, 3)\n","num_classes = 26  # Adjust based on your task\n","\n","custom_model = modify_wave_mlp(input_shape, num_classes)\n","# custom_model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def custom_head(input_tensor, num_classes):\n","    x = Flatten()(input_tensor)\n","    x = Dense(256, activation='relu')(x)  # Add your own dense layers\n","    x = Dropout(0.5)(x)  # Add dropout for regularization\n","    x = Dense(num_classes, activation='softmax')(x)\n","    return x\n","\n","\n","\n","def modify_wave_mlp(input_shape, num_classes):\n","    # Load the WaveMLP model without the top layers (head)\n","    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n","\n","    # Add your custom head\n","#     head_output = custom_head(mm_headless.output, num_classes)\n","    print(\"mm_headless.output shape:\", mm_headless.output_shape)\n","    head_output = custom_head(mm_headless.output, num_classes)\n","\n","\n","    # Create the custom model by combining the base model and the custom head\n","    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n","\n","    return custom_model\n","\n","# Example usage:\n","input_shape = (112, 112, 3)\n","num_classes = 26  # Adjust based on your task\n","\n","custom_model = modify_wave_mlp(input_shape, num_classes)\n","# custom_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T22:02:09.762360Z","iopub.status.busy":"2023-12-09T22:02:09.761642Z","iopub.status.idle":"2023-12-09T22:02:09.768121Z","shell.execute_reply":"2023-12-09T22:02:09.767158Z","shell.execute_reply.started":"2023-12-09T22:02:09.762324Z"},"trusted":true},"outputs":[],"source":["# def custom_head(input_tensor, num_classes):\n","#     x = Flatten()(input_tensor)\n","#     x = Dense(512, activation='relu')(x)\n","#     x = Dropout(0.3)(x)\n","#     x = Dense(256, activation='relu')(x)\n","#     x = Dropout(0.3)(x)\n","#     x = Dense(num_classes, activation='softmax')(x)\n","#     return x\n","\n","# def modify_wave_mlp(input_shape, num_classes):\n","#     # Load the WaveMLP model without the top layers (head)\n","#     mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n","\n","#     # Add your custom head\n","#     head_output = custom_head(mm_headless.output, num_classes)\n","\n","#     # Create the custom model by combining the base model and the custom head\n","#     custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n","\n","#     # Fine-tune the last few layers of the base model\n","#     for layer in mm_headless.layers[:-5]:\n","#         layer.trainable = True\n","\n","#     # Compile the model with a custom learning rate and metrics\n","#     custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n","\n","#     return custom_model\n","\n","# # Example usage:\n","# input_shape = (112, 112, 3)\n","# num_classes = 26  # Adjust based on your task\n","\n","# custom_model = modify_wave_mlp(input_shape, num_classes)\n","# custom_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T08:03:09.072312Z","iopub.status.busy":"2023-12-10T08:03:09.071363Z","iopub.status.idle":"2023-12-10T08:03:09.115589Z","shell.execute_reply":"2023-12-10T08:03:09.114825Z","shell.execute_reply.started":"2023-12-10T08:03:09.072273Z"},"trusted":true},"outputs":[],"source":["mm_last_layer = custom_model .get_layer('avg_pool').output\n","#out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\n","#out = Dense(11, activation='softmax', name='prediction1')(out)\n","mm_custom = Model(custom_model .input, mm_last_layer)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T08:03:13.851450Z","iopub.status.busy":"2023-12-10T08:03:13.851065Z","iopub.status.idle":"2023-12-10T08:03:14.953123Z","shell.execute_reply":"2023-12-10T08:03:14.952214Z","shell.execute_reply.started":"2023-12-10T08:03:13.851419Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras import layers\n","inputs = keras.Input(shape=(112,112,3))\n","outputs = layers.average([mm_custom(inputs)])\n","\n","avg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\n","avg_ensemble_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"mm_headless.output shape:\", mm_headless.output_shape)\n","head_output = custom_head(mm_headless.output, num_classes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from keras_cv_attention_models import coatnet\n","mm = coatnet.CoAtNet0(input_shape=(112, 112, 3), pretrained=\"imagenet\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from keras_cv_attention_models import coatnet\n","mm = coatnet.CoAtNet0(input_shape=(112, 112, 3), pretrained=\"imagenet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from keras_cv_attention_models import res_mlp\n","# mm = res_mlp.ResMLP12()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mm = res_mlp.ResMLP12(input_shape=(112, 112, 3), pretrained=\"imagenet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from keras_cv_attention_models import wave_mlp\n","mm = wave_mlp.WaveMLP_T(input_shape=(112, 112, 3), pretrained=\"imagenet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from keras_cv_attention_models import mobilevit\n","mm = mobilevit.MobileViTBasePatch16(input_shape=(112, 112, 3))\n","mm2 = mobilevit.MobileViTBasePatch16(input_shape=(112, 112, 3))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# mm.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from keras_cv_attention_models import swin_transformer_v2"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mm2 = swin_transformer_v2.SwinTransformerV2Tiny_window8(input_shape=(112, 112, 3))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# mm2.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","transfer_layer = mm.get_layer('avg_pool')\n","conv_model = Model(inputs=mm.input, outputs=transfer_layer.output)\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","#for layer in conv_model.layers:\n","#    layer.trainable = False\n","    \n","# Start a new Keras Sequential model.\n","new_model = Sequential()\n","\n","# Add the convolutional part of the VGG16 model from above.\n","new_model.add(conv_model)\n","\n","\n","# Add the final layer for the actual classification.\n","new_model.add(Dense(2, activation='softmax'))\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from keras_cv_attention_models import beit\n","mm2 = beit.BeitBasePatch16(input_shape=(112, 112, 3))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mm_last_layer = mm.get_layer('avg_pool').output\n","#out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\n","#out = Dense(11, activation='softmax', name='prediction1')(out)\n","mm_custom = Model(mm.input, mm_last_layer)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mm2_last_layer = mm2.get_layer('out_ln').output\n","#out2 = Dense(256, activation='relu', name='dense_1')(mm2_last_layer)\n","#out2 = Dense(11, activation='softmax', name='prediction1')(out2)\n","mm2_custom = Model(mm2.input, mm2_last_layer)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define a custom linear attention layer\n","class LinearAttentionLayer(keras.layers.Layer):\n","    def __init__(self, units, **kwargs):\n","        super(LinearAttentionLayer, self).__init__(**kwargs)\n","        self.units = units\n","\n","    def build(self, input_shape):\n","        self.W_q = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n","        self.W_k = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n","        self.W_v = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n","\n","    def call(self, inputs):\n","        Q = tf.matmul(inputs, self.W_q)\n","        K = tf.matmul(inputs, self.W_k)\n","        V = tf.matmul(inputs, self.W_v)\n","\n","        attn_scores = tf.matmul(Q, K, transpose_b=True)\n","        attn_scores = tf.nn.softmax(attn_scores / tf.math.sqrt(tf.cast(self.units, tf.float32)), axis=-1)\n","        output = tf.matmul(attn_scores, V)\n","\n","        return output\n","\n","# ... Continue with your code ...\n","\n","# Add the attention layer where needed in your model\n","num_classes = 2\n","avg_ensemble_model_last_layer = avg_ensemble_model.get_layer('average').output\n","\n","# Add Linear Attention Layer here (for example, just before the output layer)\n","attention_output = LinearAttentionLayer(64)(avg_ensemble_model_last_layer)\n","\n","output_layer = Dense(num_classes, activation='softmax', name='output_1')(attention_output)\n","final_model = Model(avg_ensemble_model.input, output_layer)\n","\n","final_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras import layers\n","inputs = keras.Input(shape=(112,112,3))\n","outputs = layers.average([mm_custom(inputs), mm2_custom(inputs)])\n","\n","avg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\n","avg_ensemble_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T22:04:01.428153Z","iopub.status.busy":"2023-12-09T22:04:01.427751Z","iopub.status.idle":"2023-12-09T22:04:01.468384Z","shell.execute_reply":"2023-12-09T22:04:01.467037Z","shell.execute_reply.started":"2023-12-09T22:04:01.428111Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras import layers\n","inputs = keras.Input(shape=(112,112,3))\n","outputs = layers.average([mm_custom(inputs)])\n","\n","avg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\n","avg_ensemble_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T08:03:57.619160Z","iopub.status.busy":"2023-12-10T08:03:57.618364Z","iopub.status.idle":"2023-12-10T08:03:57.685820Z","shell.execute_reply":"2023-12-10T08:03:57.684904Z","shell.execute_reply.started":"2023-12-10T08:03:57.619122Z"},"trusted":true},"outputs":[],"source":["num_classes = 26\n","avg_ensemble_model_last_layer = avg_ensemble_model.get_layer('average_1').output\n","output_layer = Dense(num_classes, activation='softmax', name='output_1')(avg_ensemble_model_last_layer)\n","final_model = Model(avg_ensemble_model.input, output_layer)\n","\n","final_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T08:04:02.439921Z","iopub.status.busy":"2023-12-10T08:04:02.439530Z","iopub.status.idle":"2023-12-10T08:04:02.505447Z","shell.execute_reply":"2023-12-10T08:04:02.504704Z","shell.execute_reply.started":"2023-12-10T08:04:02.439888Z"},"trusted":true},"outputs":[],"source":["optimizer = Adam(lr=1e-5)\n","loss = 'categorical_crossentropy'\n","# metrics = ['categorical_accuracy']\n","metrics = ['accuracy', 'categorical_accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), \n","           tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), \n","           tf.keras.metrics.FalseNegatives(), tfa.metrics.CohenKappa(num_classes = num_classes), \n","           tfa.metrics.F1Score(num_classes = num_classes)]\n","\n","final_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","\n","# Delete the existing HDF5 file if it exists\n","if os.path.exists('Best_DenseNet201.h5'):\n","    os.remove('Best_DenseNet201.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T08:04:17.499620Z","iopub.status.busy":"2023-12-10T08:04:17.498729Z","iopub.status.idle":"2023-12-10T08:04:17.505813Z","shell.execute_reply":"2023-12-10T08:04:17.504901Z","shell.execute_reply.started":"2023-12-10T08:04:17.499569Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1,\n","    patience=9, mode=\"max\", min_delta=0.0001, min_lr=0.00001, verbose=1)\n","checkpoint = ModelCheckpoint(filepath='Best_DenseNet201_v23.h5', save_best_only=True, monitor = 'val_accuracy', verbose=1)\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, restore_best_weights=True)\n","\n","callbacks = [lr, checkpoint, early_stopping]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T08:04:25.366742Z","iopub.status.busy":"2023-12-10T08:04:25.366376Z","iopub.status.idle":"2023-12-10T08:36:02.988272Z","shell.execute_reply":"2023-12-10T08:36:02.986623Z","shell.execute_reply.started":"2023-12-10T08:04:25.366711Z"},"trusted":true},"outputs":[],"source":["epochs = 30\n","\n","steps_per_epoch = generator_train.n / batch_size\n","steps_test = generator_test.n / batch_size\n","\n","history = final_model.fit_generator(generator=generator_train,\n","                                  epochs=epochs,\n","                                  steps_per_epoch=steps_per_epoch,\n","                                  validation_data=generator_test,\n","                                  validation_steps=steps_test,\n","                                   callbacks=callbacks, class_weight =class_weights)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-16T21:13:26.977709Z","iopub.status.busy":"2023-11-16T21:13:26.977420Z","iopub.status.idle":"2023-11-16T21:14:03.780954Z","shell.execute_reply":"2023-11-16T21:14:03.780023Z","shell.execute_reply.started":"2023-11-16T21:13:26.977682Z"},"trusted":true},"outputs":[],"source":["evaluate_(final_model, generator_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-16T21:14:03.782492Z","iopub.status.busy":"2023-11-16T21:14:03.782196Z","iopub.status.idle":"2023-11-16T21:14:04.092807Z","shell.execute_reply":"2023-11-16T21:14:04.091925Z","shell.execute_reply.started":"2023-11-16T21:14:03.782466Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.plot(history.history['categorical_accuracy'])\n","plt.plot(history.history['val_categorical_accuracy'])\n","plt.title('Model Accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-16T21:14:04.094718Z","iopub.status.busy":"2023-11-16T21:14:04.094424Z","iopub.status.idle":"2023-11-16T21:14:04.315164Z","shell.execute_reply":"2023-11-16T21:14:04.314425Z","shell.execute_reply.started":"2023-11-16T21:14:04.094692Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install --upgrade scipy scikit-image\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image_size = 128\n","batch_size = 8\n","\n","train_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\n","val_dir = r\"/kaggle/input/mango-leaf/mango-prepo/val\"\n","test_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\n","\n","\n","datagen_train = ImageDataGenerator(rescale=1./255, width_shift_range=0.1, height_shift_range=0.1,\n","                                  horizontal_flip=True,  vertical_flip=False)\n","datagen_test = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = datagen_train.flow_from_directory(directory=train_dir, target_size=(image_size, image_size),\n","                                                    batch_size=batch_size, shuffle=True)\n","val_generator = datagen_test.flow_from_directory(directory=val_dir, target_size=(image_size, image_size),\n","                                                  batch_size=batch_size, shuffle=False)\n","test_generator = datagen_test.flow_from_directory(directory=test_dir, target_size=(image_size, image_size),\n","                                                  batch_size=batch_size, shuffle=False)\n","\n","#Define the number of classes in your dataset\n","num_classes = train_generator.num_classes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras.layers import Layer, Attention\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class LinearAttention(Layer):\n","    def __init__(self, units):\n","        super(LinearAttention, self).__init__()\n","        self.units = units\n","\n","    def build(self, input_shape):\n","        self.W_q = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n","        self.W_k = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n","        self.W_v = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n","\n","    def call(self, inputs):\n","        Q = tf.matmul(inputs, self.W_q)\n","        K = tf.matmul(inputs, self.W_k)\n","        V = tf.matmul(inputs, self.W_v)\n","\n","        attn_scores = tf.matmul(Q, K, transpose_b=True)\n","        attn_scores = tf.nn.softmax(attn_scores / tf.math.sqrt(tf.cast(self.units, tf.float32)), axis=-1)\n","        output = tf.matmul(attn_scores, V)\n","\n","        return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define your model function with attention\n","def modelfunction_with_attention(base):\n","    x = base.output\n","\n","    # Add Self-Attention Layer\n","    att_output = LinearAttention(128)(x)\n","\n","    # Add more layers if needed\n","    x = tf.keras.layers.GlobalAveragePooling2D()(att_output)\n","    x = tf.keras.layers.Dropout(0.4)(x)\n","    predictions = tf.keras.layers.Dense(units=num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.02, l2=0.02))(x)\n","    model = Model(inputs=base.input, outputs=predictions)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def modelfunction(base):\n","    x = base.output\n","    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n","    x = tf.keras.layers.Dropout(0.4)(x)\n","    predictions = tf.keras.layers.Dense(units=num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.02, l2=0.02))(x)\n","    model = Model(inputs=base.input, outputs=predictions)\n","    return model\n","\n","def get_callbacks(weight):\n","    checkpoint = ModelCheckpoint(weight, monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n","    learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=5, verbose=1, factor=0.2, min_lr=0.0002)\n","    callbacks = [checkpoint, learning_rate_reduction]\n","    return callbacks\n","\n","def evaluate(model, generator_test):\n","    model.evaluate(generator_test)\n","\n","    y_pred = model.predict(generator_test)\n","    y_pred_classes = np.argmax(y_pred, axis=1)\n","    y_true = generator_test.classes\n","    class_labels = list(generator_test.class_indices.keys())\n","\n","    print(classification_report(y_true, y_pred_classes))\n","    cm = confusion_matrix(y_true, y_pred_classes)\n","\n","    # Plotting the confusion matrix\n","    plt.figure(figsize=(8, 8))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n","    plt.show()\n","\n","def model_training(base, weight, epochs):\n","    model = modelfunction(base)\n","    print(\"\\n\\n\\n-------------------- Model Initialized --------------------\")\n","\n","    callbacks = get_callbacks(weight)\n","    metrics = ['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(),\n","               tfa.metrics.CohenKappa(num_classes=num_classes), tfa.metrics.F1Score(num_classes=num_classes)]\n","    model.compile(tf.keras.optimizers.Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=metrics)\n","\n","    history = model.fit(train_generator, steps_per_epoch=366 // batch_size,\n","                        validation_data=val_generator,  # Add this line\n","                        epochs=epochs, callbacks=callbacks)\n","    # Plotting accuracy and loss curves\n","    plt.figure(figsize=(12, 4))\n","\n","    # Plot accuracy\n","    plt.subplot(1, 2, 1)\n","    plt.plot(history.history['accuracy'], label='accuracy')\n","    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.title('Accuracy Over Epochs')\n","    plt.legend()\n","\n","    # Plot loss\n","    plt.subplot(1, 2, 2)\n","    plt.plot(history.history['loss'], label='loss')\n","    plt.plot(history.history['val_loss'], label='val_loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('Loss Over Epochs')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    print(\"\\n\\n\\n-------------------- Evaluation --------------------\")\n","    evaluate(model, val_generator)\n","\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create and train the model with attention\n","VGG19 = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_tensor=None, input_shape=None)\n","VGG19_model_with_attention = model_training(VGG19, 'VGG19_with_attention.h5', 10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import lime\n","from lime import lime_image\n","from skimage.segmentation import mark_boundaries\n","import matplotlib.pyplot as plt\n","import random\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from lime import lime_image"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from skimage.segmentation import mark_boundaries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":3891934,"sourceId":6762217,"sourceType":"datasetVersion"}],"dockerImageVersionId":30616,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}

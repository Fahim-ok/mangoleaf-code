{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6762217,"sourceType":"datasetVersion","datasetId":3891934}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))","metadata":{"execution":{"iopub.status.busy":"2023-12-12T18:55:43.194234Z","iopub.execute_input":"2023-12-12T18:55:43.194756Z","iopub.status.idle":"2023-12-12T18:56:01.035686Z","shell.execute_reply.started":"2023-12-12T18:55:43.194730Z","shell.execute_reply":"2023-12-12T18:56:01.034762Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Device mapping:\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def evaluate_(model, generator_test):\n    model.evaluate(generator_test)\n    \n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n    \n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-12T18:57:18.808831Z","iopub.execute_input":"2023-12-12T18:57:18.809484Z","iopub.status.idle":"2023-12-12T18:57:18.816159Z","shell.execute_reply.started":"2023-12-12T18:57:18.809453Z","shell.execute_reply":"2023-12-12T18:57:18.815247Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef evaluate_(model, generator_test):\n    model.evaluate(generator_test)\n    \n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n    \n    # Plotting the confusion matrix\n    plt.figure(figsize=(12, 4))\n    \n    plt.subplot(1, 2, 1)\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.title('Confusion Matrix')\n    \n    # ROC curve\n    plt.subplot(1, 2, 2)\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n\n    for i in range(len(class_labels)):\n        fpr[i], tpr[i], _ = roc_curve(y_true == i, y_pred[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    for i in range(len(class_labels)):\n        plt.plot(fpr[i], tpr[i], label=f'{class_labels[i]} (AUC = {roc_auc[i]:.2f})')\n\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc=\"lower right\")\n\n    plt.tight_layout()\n    plt.show()\n\n# Call the function with your model and test generator\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T18:57:23.311550Z","iopub.execute_input":"2023-12-12T18:57:23.312245Z","iopub.status.idle":"2023-12-12T18:57:23.323309Z","shell.execute_reply.started":"2023-12-12T18:57:23.312212Z","shell.execute_reply":"2023-12-12T18:57:23.322343Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\ndatagen_train = ImageDataGenerator(rescale=1./255,\n                                  width_shift_range=0.1,\n                                  height_shift_range=0.1,\n                                  horizontal_flip=True,\n                                  vertical_flip=False)\n\n\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\n\nbatch_size = 16\ngenerator_train = datagen_train.flow_from_directory(directory=train_dir,\n                                                    target_size=(112, 112),\n                                                    batch_size=batch_size,\n                                                    shuffle=True)\n\ngenerator_test = datagen_test.flow_from_directory(directory=test_dir,\n                                                  target_size=(112, 112),\n                                                  batch_size=batch_size,\n                                                  shuffle=False)\n# Calculate class weights\nlabels = generator_train.classes\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\nclass_weights = dict(zip(np.unique(labels), class_weights))\nprint(class_weights)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T18:57:26.755825Z","iopub.execute_input":"2023-12-12T18:57:26.756785Z","iopub.status.idle":"2023-12-12T18:57:28.085798Z","shell.execute_reply.started":"2023-12-12T18:57:26.756750Z","shell.execute_reply":"2023-12-12T18:57:28.084952Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Found 13307 images belonging to 26 classes.\nFound 1684 images belonging to 26 classes.\n{0: 1.0640492563569486, 1: 1.0662660256410257, 2: 1.0662660256410257, 3: 1.0402595372107568, 4: 1.2273565762774397, 5: 1.0381494772975504, 6: 1.053102247546692, 7: 1.2795192307692307, 8: 1.0487862547288778, 9: 1.053102247546692, 10: 0.6486789509603198, 11: 1.0662660256410257, 12: 1.4623076923076923, 13: 0.8809082483781279, 14: 0.7571119708693673, 15: 1.0466414975617429, 16: 1.7527660695468914, 17: 0.5415954415954416, 18: 1.0662660256410257, 19: 0.9042538733351454, 20: 0.789826685660019, 21: 1.5462468045549616, 22: 1.0256667180514876, 23: 1.8085077466702908, 24: 1.0445054945054946, 25: 0.7259683578832515}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras_cv_attention_models ","metadata":{"execution":{"iopub.status.busy":"2023-12-12T18:57:31.881026Z","iopub.execute_input":"2023-12-12T18:57:31.881368Z","iopub.status.idle":"2023-12-12T18:57:46.093669Z","shell.execute_reply.started":"2023-12-12T18:57:31.881344Z","shell.execute_reply":"2023-12-12T18:57:46.092648Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting keras_cv_attention_models\n  Obtaining dependency information for keras_cv_attention_models from https://files.pythonhosted.org/packages/67/b4/a581ae34f6a37b021e32d1d874b8b7df4664cbb8a71659886193caf341e0/keras_cv_attention_models-1.3.22-py3-none-any.whl.metadata\n  Downloading keras_cv_attention_models-1.3.22-py3-none-any.whl.metadata (183 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.8/183.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (10.1.0)\nCollecting ftfy (from keras_cv_attention_models)\n  Obtaining dependency information for ftfy from https://files.pythonhosted.org/packages/91/f8/dfa32d06cfcbdb76bc46e0f5d69c537de33f4cedb1a15cd4746ab45a6a26/ftfy-6.1.3-py3-none-any.whl.metadata\n  Downloading ftfy-6.1.3-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (2023.8.8)\nRequirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (4.9.2)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (2.13.0)\nCollecting wcwidth<0.3.0,>=0.2.12 (from ftfy->keras_cv_attention_models)\n  Obtaining dependency information for wcwidth<0.3.0,>=0.2.12 from https://files.pythonhosted.org/packages/31/b1/a59de0ad3aabb17523a39804f4c6df3ae87ead053a4e25362ae03d73d03a/wcwidth-0.2.12-py2.py3-none-any.whl.metadata\n  Downloading wcwidth-0.2.12-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.1.21 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (3.9.0)\nRequirement already satisfied: keras<2.14,>=2.13.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (2.13.1)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (16.0.6)\nRequirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.24.3)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (68.1.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.16.0)\nRequirement already satisfied: tensorboard<2.14,>=2.13 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (2.13.0)\nRequirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (2.13.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (2.3.0)\nRequirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (4.5.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.15.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (0.34.0)\nRequirement already satisfied: array-record in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (0.4.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (8.1.7)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (0.1.8)\nRequirement already satisfied: etils[enp,epath]>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (1.4.1)\nRequirement already satisfied: promise in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (2.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (5.9.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (2.31.0)\nRequirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (0.14.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (0.10.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (4.66.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow->keras_cv_attention_models) (0.41.2)\nRequirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->keras_cv_attention_models) (5.13.0)\nRequirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->keras_cv_attention_models) (3.16.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (2023.11.17)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (2.22.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (3.4.4)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (3.0.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow->keras_cv_attention_models) (3.0.9)\nRequirement already satisfied: googleapis-common-protos in /opt/conda/lib/python3.10/site-packages (from tensorflow-metadata->tensorflow-datasets->keras_cv_attention_models) (1.60.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->keras_cv_attention_models) (3.2.2)\nDownloading keras_cv_attention_models-1.3.22-py3-none-any.whl (773 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.2/773.2 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\nInstalling collected packages: wcwidth, ftfy, keras_cv_attention_models\n  Attempting uninstall: wcwidth\n    Found existing installation: wcwidth 0.2.6\n    Uninstalling wcwidth-0.2.6:\n      Successfully uninstalled wcwidth-0.2.6\nSuccessfully installed ftfy-6.1.3 keras_cv_attention_models-1.3.22 wcwidth-0.2.12\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T19:09:23.491279Z","iopub.execute_input":"2023-11-16T19:09:23.491565Z","iopub.status.idle":"2023-11-16T19:09:23.505707Z","shell.execute_reply.started":"2023-11-16T19:09:23.491539Z","shell.execute_reply":"2023-11-16T19:09:23.504783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import wave_mlp","metadata":{"execution":{"iopub.status.busy":"2023-12-12T18:58:19.979560Z","iopub.execute_input":"2023-12-12T18:58:19.980233Z","iopub.status.idle":"2023-12-12T18:58:19.984155Z","shell.execute_reply.started":"2023-12-12T18:58:19.980199Z","shell.execute_reply":"2023-12-12T18:58:19.983218Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"pip install keras-self-attention\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T10:49:58.764769Z","iopub.execute_input":"2023-12-10T10:49:58.765357Z","iopub.status.idle":"2023-12-10T10:50:12.966644Z","shell.execute_reply.started":"2023-12-10T10:49:58.765324Z","shell.execute_reply":"2023-12-10T10:50:12.965518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, Layer, Add, Input, Conv2D, BatchNormalization, Activation, GlobalAveragePooling2D, Reshape, Concatenate, Lambda, Permute\nfrom tensorflow.keras import Model\n\nclass SelfAttention(Layer):\n    def __init__(self, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.W_q = self.add_weight(name='W_q', shape=(input_shape[-1], input_shape[-1]),\n                                   initializer='uniform', trainable=True)\n        self.W_k = self.add_weight(name='W_k', shape=(input_shape[-1], input_shape[-1]),\n                                   initializer='uniform', trainable=True)\n        self.W_v = self.add_weight(name='W_v', shape=(input_shape[-1], input_shape[-1]),\n                                   initializer='uniform', trainable=True)\n        super(SelfAttention, self).build(input_shape)\n\n    def call(self, x):\n        q = x @ self.W_q\n        k = x @ self.W_k\n        v = x @ self.W_v\n\n        attn_score = tf.matmul(q, k, transpose_b=True)\n        attn_score = tf.nn.softmax(attn_score, axis=-1)\n\n        output = attn_score @ v\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\nclass PositionalEncoding(Layer):\n    def __init__(self, input_shape, **kwargs):\n        super(PositionalEncoding, self).__init__(**kwargs)\n        self.input_shape_ = input_shape\n\n    def build(self, input_shape):\n        self.positional_encoding = self.add_weight(name='positional_encoding',\n                                                   shape=(1, *self.input_shape_[1:], 1),\n                                                   initializer='uniform',\n                                                   trainable=True)\n        super(PositionalEncoding, self).build(input_shape)\n\n    def call(self, x):\n        return x + self.positional_encoding\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\ndef shufflenet_block(x, groups):\n    _, _, _, c = x.get_shape().as_list()\n\n    # Channel split\n    x = Reshape((-1, groups))(x)\n    x = Permute((2, 1))(x)\n    x = Reshape((-1, c))(x)\n\n    # Channel shuffle\n    x = Lambda(lambda z: tf.keras.backend.batch_flatten(tf.keras.backend.permute_dimensions(z, (0, 2, 1))))(x)\n\n    # Pointwise group convolution\n    x = Reshape((1, 1, c))(x)\n    x = Conv2D(c, kernel_size=(1, 1), groups=groups, use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    # Global Average Pooling\n    x = GlobalAveragePooling2D()(x)\n\n    # Reshape for concatenation\n    x = Reshape((1, 1, c))(x)\n\n    # Concatenate with the original input\n    x = Concatenate(axis=-1)([x, x])\n\n    return x\n\ndef custom_head_with_shufflenet(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.5)(x)\n\n    # Add ShuffleNet block\n    shufflenet_output = shufflenet_block(tf.keras.layers.Reshape((1, 1, 256))(x), groups=4)\n    x = Add()([input_tensor, shufflenet_output])\n\n    # Add self-attention mechanism\n    self_attention = SelfAttention()(x)\n    x = Add()([x, self_attention])\n\n    # Add 2D positional encoding\n    position_encoding = PositionalEncoding(input_shape=(1, 1, 256))(x)\n    x = Add()([x, position_encoding])\n\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n    head_output = custom_head_with_shufflenet(mm_headless.output, num_classes)\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T07:50:34.076284Z","iopub.execute_input":"2023-12-10T07:50:34.077348Z","iopub.status.idle":"2023-12-10T07:50:37.949949Z","shell.execute_reply.started":"2023-12-10T07:50:34.077311Z","shell.execute_reply":"2023-12-10T07:50:37.948644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, Layer, Add\nfrom tensorflow.keras import Input, Model\n# Import your wave_mlp module\n\nclass SelfAttention(Layer):\n    def __init__(self, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.W_q = self.add_weight(name='W_q', shape=(input_shape[-1], input_shape[-1]),\n                                   initializer='uniform', trainable=True)\n        self.W_k = self.add_weight(name='W_k', shape=(input_shape[-1], input_shape[-1]),\n                                   initializer='uniform', trainable=True)\n        self.W_v = self.add_weight(name='W_v', shape=(input_shape[-1], input_shape[-1]),\n                                   initializer='uniform', trainable=True)\n        super(SelfAttention, self).build(input_shape)\n\n    def call(self, x):\n        q = x @ self.W_q\n        k = x @ self.W_k\n        v = x @ self.W_v\n\n        attn_score = tf.matmul(q, k, transpose_b=True)\n        attn_score = tf.nn.softmax(attn_score, axis=-1)\n\n        output = attn_score @ v\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\nclass PositionalEncoding(Layer):\n    def __init__(self, input_shape, **kwargs):\n        super(PositionalEncoding, self).__init__(**kwargs)\n        self.input_shape_ = input_shape\n\n    def build(self, input_shape):\n        self.positional_encoding = self.add_weight(name='positional_encoding',\n                                                   shape=(1, *self.input_shape_[1:], 1),\n                                                   initializer='uniform',\n                                                   trainable=True)\n        super(PositionalEncoding, self).build(input_shape)\n\n    def call(self, x):\n        return x + self.positional_encoding\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\nclass ResidualBlock(Layer):\n    def __init__(self, **kwargs):\n        super(ResidualBlock, self).__init__(**kwargs)\n        self.conv1 = tf.keras.layers.Conv2D(filters=None, kernel_size=(3, 3), padding='same', activation='relu')\n        self.conv2 = tf.keras.layers.Conv2D(filters=None, kernel_size=(3, 3), padding='same', activation='relu')\n        self.add = Add()\n\n    def build(self, input_shape):\n        # Infer the number of filters dynamically based on the input shape\n        num_filters = input_shape[-1]\n        self.conv1.filters = num_filters\n        self.conv2.filters = num_filters\n        super(ResidualBlock, self).build(input_shape)\n\n    def call(self, inputs):\n        x = self.conv1(inputs)\n        x = self.conv2(x)\n        return self.add([inputs, x])\n\n\n\ndef custom_head_with_stages(input_tensor, num_classes):\n    # Stage 1: Global Self-Attention\n    x = SelfAttention()(input_tensor)\n\n    # Stage 2: Global Self-Attention\n    x = SelfAttention()(x)\n\n    # Stage 3: Global Self-Attention\n    x = SelfAttention()(x)\n\n    # Stage 4: Residual Blocks\n    x = ResidualBlock(256, (3, 3))(x)\n    x = ResidualBlock(256, (3, 3))(x)\n\n    # Flatten and add your custom dense layers\n    x = Flatten()(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.5)(x)\n\n    # Add self-attention mechanism\n    self_attention = SelfAttention()(x)\n    x = Add()([x, self_attention])\n\n    # Add 2D positional encoding\n    position_encoding = PositionalEncoding(input_shape=(1, 1, 256))(x)\n    x = Add()([x, position_encoding])\n\n    # Final Dense layer\n    x = Dense(num_classes, activation='softmax')(x)\n\n    return x\n\ndef modify_wave_mlp_with_stages(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head with stages\n    head_output = custom_head_with_stages(mm_headless.output, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model_with_stages = modify_wave_mlp_with_stages(input_shape, num_classes)\ncustom_model_with_stages.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T07:48:25.112994Z","iopub.execute_input":"2023-12-10T07:48:25.113968Z","iopub.status.idle":"2023-12-10T07:48:28.485141Z","shell.execute_reply.started":"2023-12-10T07:48:25.113926Z","shell.execute_reply":"2023-12-10T07:48:28.483792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, Layer, Add, Conv2D, BatchNormalization, Activation, MaxPooling2D\nfrom tensorflow.keras import Input, Model\n# Import your wave_mlp module\n\nclass SelfAttention(Layer):\n    def __init__(self, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.W_q = self.add_weight(name='W_q', shape=(input_shape[-1], input_shape[-1]),\n                                   initializer='uniform', trainable=True)\n        self.W_k = self.add_weight(name='W_k', shape=(input_shape[-1], input_shape[-1]),\n                                   initializer='uniform', trainable=True)\n        self.W_v = self.add_weight(name='W_v', shape=(input_shape[-1], input_shape[-1]),\n                                   initializer='uniform', trainable=True)\n        super(SelfAttention, self).build(input_shape)\n\n    def call(self, x):\n        q = x @ self.W_q\n        k = x @ self.W_k\n        v = x @ self.W_v\n\n        attn_score = tf.matmul(q, k, transpose_b=True)\n        attn_score = tf.nn.softmax(attn_score, axis=-1)\n\n        output = attn_score @ v\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\nclass PositionalEncoding(Layer):\n    def __init__(self, input_shape, **kwargs):\n        super(PositionalEncoding, self).__init__(**kwargs)\n        self.input_shape_ = input_shape\n\n    def build(self, input_shape):\n        self.positional_encoding = self.add_weight(name='positional_encoding',\n                                                   shape=(1, *self.input_shape_[1:], 1),\n                                                   initializer='uniform',\n                                                   trainable=True)\n        super(PositionalEncoding, self).build(input_shape)\n\n    def call(self, x):\n        return x + self.positional_encoding\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(256, activation='relu')(x)  # Add your own dense layers\n    x = Dropout(0.5)(x)  # Add dropout for regularization\n\n    # Add self-attention mechanism\n    self_attention = SelfAttention()(x)\n    x = Add()([x, self_attention])\n\n    # Add 2D positional encoding\n    position_encoding = PositionalEncoding(input_shape=(1, 1, 256))(x)  # Adjust the input_shape\n    x = Add()([x, position_encoding])\n\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef stage_block(x, filters, num_blocks, attention=True, hdc=False):\n    # Check if the input is flattened\n    if len(x.shape) == 2:\n        # Reshape to 4D tensor\n        x = tf.reshape(x, [-1, int(x.shape[1] ** 0.5), int(x.shape[1] ** 0.5), 1])\n\n    for _ in range(num_blocks):\n        # Residual block\n        residual = x\n\n        # Global self-attention\n        if attention:\n            x = SelfAttention()(x)\n\n        # Convolutional layer with Hybrid Dilated Convolution (HDC)\n        if hdc:\n            x = Conv2D(filters, kernel_size=(3, 3), dilation_rate=(2, 2), padding='same')(x)\n            x = BatchNormalization()(x)\n            x = Activation('relu')(x)\n            x = Conv2D(filters, kernel_size=(3, 3), dilation_rate=(3, 3), padding='same')(x)\n            x = BatchNormalization()(x)\n            x = Activation('relu')(x)\n        else:\n            x = Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n            x = BatchNormalization()(x)\n            x = Activation('relu')(x)\n\n        # Residual connection (adjusting the residual size)\n        residual = Conv2D(filters, kernel_size=(1, 1), padding='same')(residual)\n        x = Add()([x, residual])\n\n    # MaxPooling for downsampling\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Stage 1 with HDC\n    x = stage_block(mm_headless.output, filters=64, num_blocks=3, hdc=True)\n\n    # Stage 2 with HDC\n    x = stage_block(x, filters=128, num_blocks=3, hdc=True)\n\n    # Stage 3 with HDC\n    x = stage_block(x, filters=256, num_blocks=3, hdc=True)\n\n    # Stage 4 (Residual blocks)\n    x = stage_block(x, filters=512, num_blocks=3, attention=False, hdc=False)\n\n    # Custom head\n    head_output = custom_head(x, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T19:00:32.951831Z","iopub.execute_input":"2023-12-12T19:00:32.952547Z","iopub.status.idle":"2023-12-12T19:00:40.582722Z","shell.execute_reply.started":"2023-12-12T19:00:32.952504Z","shell.execute_reply":"2023-12-12T19:00:40.581787Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Downloading data from https://github.com/leondgarse/keras_cv_attention_models/releases/download/mlp_family/wavemlp_t_imagenet.h5\n69906376/69906376 [==============================] - 1s 0us/step\n>>>> Load pretrained from: /root/.keras/models/wavemlp_t_imagenet.h5\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_1 (InputLayer)        [(None, 112, 112, 3)]        0         []                            \n                                                                                                  \n stem_pad (ZeroPadding2D)    (None, 116, 116, 3)          0         ['input_1[0][0]']             \n                                                                                                  \n stem_conv (Conv2D)          (None, 28, 28, 64)           9472      ['stem_pad[0][0]']            \n                                                                                                  \n stem_bn (BatchNormalizatio  (None, 28, 28, 64)           256       ['stem_conv[0][0]']           \n n)                                                                                               \n                                                                                                  \n stack1_block1_attn_bn (Bat  (None, 28, 28, 64)           256       ['stem_bn[0][0]']             \n chNormalization)                                                                                 \n                                                                                                  \n stack1_block1_attn_theta_h  (None, 28, 28, 64)           4160      ['stack1_block1_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack1_block1_attn_theta_w  (None, 28, 28, 64)           4160      ['stack1_block1_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack1_block1_attn_theta_h  (None, 28, 28, 64)           256       ['stack1_block1_attn_theta_h_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack1_block1_attn_theta_w  (None, 28, 28, 64)           256       ['stack1_block1_attn_theta_w_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack1_block1_attn_theta_h  (None, 28, 28, 64)           0         ['stack1_block1_attn_theta_h_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack1_block1_attn_theta_w  (None, 28, 28, 64)           0         ['stack1_block1_attn_theta_w_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack1_block1_attn_height_  (None, 28, 28, 64)           4096      ['stack1_block1_attn_bn[0][0]'\n conv (Conv2D)                                                      ]                             \n                                                                                                  \n tf.math.cos (TFOpLambda)    (None, 28, 28, 64)           0         ['stack1_block1_attn_theta_h_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n tf.math.sin (TFOpLambda)    (None, 28, 28, 64)           0         ['stack1_block1_attn_theta_h_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n stack1_block1_attn_width_c  (None, 28, 28, 64)           4096      ['stack1_block1_attn_bn[0][0]'\n onv (Conv2D)                                                       ]                             \n                                                                                                  \n tf.math.cos_1 (TFOpLambda)  (None, 28, 28, 64)           0         ['stack1_block1_attn_theta_w_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n tf.math.sin_1 (TFOpLambda)  (None, 28, 28, 64)           0         ['stack1_block1_attn_theta_w_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n multiply (Multiply)         (None, 28, 28, 64)           0         ['stack1_block1_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.cos[0][0]']         \n                                                                                                  \n multiply_1 (Multiply)       (None, 28, 28, 64)           0         ['stack1_block1_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.sin[0][0]']         \n                                                                                                  \n multiply_2 (Multiply)       (None, 28, 28, 64)           0         ['stack1_block1_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.cos_1[0][0]']       \n                                                                                                  \n multiply_3 (Multiply)       (None, 28, 28, 64)           0         ['stack1_block1_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.sin_1[0][0]']       \n                                                                                                  \n concatenate (Concatenate)   (None, 28, 28, 128)          0         ['multiply[0][0]',            \n                                                                     'multiply_1[0][0]']          \n                                                                                                  \n concatenate_1 (Concatenate  (None, 28, 28, 128)          0         ['multiply_2[0][0]',          \n )                                                                   'multiply_3[0][0]']          \n                                                                                                  \n stack1_block1_attn_height_  (None, 28, 34, 128)          0         ['concatenate[0][0]']         \n down_pad (ZeroPadding2D)                                                                         \n                                                                                                  \n stack1_block1_attn_width_d  (None, 34, 28, 128)          0         ['concatenate_1[0][0]']       \n own_pad (ZeroPadding2D)                                                                          \n                                                                                                  \n stack1_block1_attn_height_  (None, 28, 28, 64)           896       ['stack1_block1_attn_height_do\n down_conv (Conv2D)                                                 wn_pad[0][0]']                \n                                                                                                  \n stack1_block1_attn_width_d  (None, 28, 28, 64)           896       ['stack1_block1_attn_width_dow\n own_conv (Conv2D)                                                  n_pad[0][0]']                 \n                                                                                                  \n stack1_block1_attn_channel  (None, 28, 28, 64)           4096      ['stack1_block1_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack1_block1_attn_combine  (None, 28, 28, 64)           0         ['stack1_block1_attn_height_do\n  (Add)                                                             wn_conv[0][0]',               \n                                                                     'stack1_block1_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'stack1_block1_attn_channel_c\n                                                                    onv[0][0]']                   \n                                                                                                  \n global_average_pooling2d (  (None, 1, 1, 64)             0         ['stack1_block1_attn_combine[0\n GlobalAveragePooling2D)                                            ][0]']                        \n                                                                                                  \n stack1_block1_attn_reweigh  (None, 1, 1, 16)             1040      ['global_average_pooling2d[0][\n t_Conv_0 (Conv2D)                                                  0]']                          \n                                                                                                  \n stack1_block1_attn_reweigh  (None, 1, 1, 16)             0         ['stack1_block1_attn_reweight_\n t_gelugelu (Activation)                                            Conv_0[0][0]']                \n                                                                                                  \n stack1_block1_attn_reweigh  (None, 1, 1, 192)            3264      ['stack1_block1_attn_reweight_\n t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n                                                                                                  \n reshape (Reshape)           (None, 1, 1, 64, 3)          0         ['stack1_block1_attn_reweight_\n                                                                    Conv_1[0][0]']                \n                                                                                                  \n stack1_block1_attn_attenti  (None, 1, 1, 64, 3)          0         ['reshape[0][0]']             \n on_scores (Softmax)                                                                              \n                                                                                                  \n tf.unstack (TFOpLambda)     [(None, 1, 1, 64),           0         ['stack1_block1_attn_attention\n                              (None, 1, 1, 64),                     _scores[0][0]']               \n                              (None, 1, 1, 64)]                                                   \n                                                                                                  \n multiply_4 (Multiply)       (None, 28, 28, 64)           0         ['stack1_block1_attn_height_do\n                                                                    wn_conv[0][0]',               \n                                                                     'tf.unstack[0][0]']          \n                                                                                                  \n multiply_5 (Multiply)       (None, 28, 28, 64)           0         ['stack1_block1_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'tf.unstack[0][1]']          \n                                                                                                  \n multiply_6 (Multiply)       (None, 28, 28, 64)           0         ['stack1_block1_attn_channel_c\n                                                                    onv[0][0]',                   \n                                                                     'tf.unstack[0][2]']          \n                                                                                                  \n add (Add)                   (None, 28, 28, 64)           0         ['multiply_4[0][0]',          \n                                                                     'multiply_5[0][0]',          \n                                                                     'multiply_6[0][0]']          \n                                                                                                  \n stack1_block1_attn_out_con  (None, 28, 28, 64)           4160      ['add[0][0]']                 \n v (Conv2D)                                                                                       \n                                                                                                  \n stack1_block1_attn_out (Ad  (None, 28, 28, 64)           0         ['stem_bn[0][0]',             \n d)                                                                  'stack1_block1_attn_out_conv[\n                                                                    0][0]']                       \n                                                                                                  \n stack1_block1_mlp_bn (Batc  (None, 28, 28, 64)           256       ['stack1_block1_attn_out[0][0]\n hNormalization)                                                    ']                            \n                                                                                                  \n stack1_block1_mlp_Conv_0 (  (None, 28, 28, 256)          16640     ['stack1_block1_mlp_bn[0][0]']\n Conv2D)                                                                                          \n                                                                                                  \n stack1_block1_mlp_gelugelu  (None, 28, 28, 256)          0         ['stack1_block1_mlp_Conv_0[0][\n  (Activation)                                                      0]']                          \n                                                                                                  \n stack1_block1_mlp_Conv_1 (  (None, 28, 28, 64)           16448     ['stack1_block1_mlp_gelugelu[0\n Conv2D)                                                            ][0]']                        \n                                                                                                  \n stack1_block1_mlp_out (Add  (None, 28, 28, 64)           0         ['stack1_block1_attn_out[0][0]\n )                                                                  ',                            \n                                                                     'stack1_block1_mlp_Conv_1[0][\n                                                                    0]']                          \n                                                                                                  \n stack1_block2_attn_bn (Bat  (None, 28, 28, 64)           256       ['stack1_block1_mlp_out[0][0]'\n chNormalization)                                                   ]                             \n                                                                                                  \n stack1_block2_attn_theta_h  (None, 28, 28, 64)           4160      ['stack1_block2_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack1_block2_attn_theta_w  (None, 28, 28, 64)           4160      ['stack1_block2_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack1_block2_attn_theta_h  (None, 28, 28, 64)           256       ['stack1_block2_attn_theta_h_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack1_block2_attn_theta_w  (None, 28, 28, 64)           256       ['stack1_block2_attn_theta_w_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack1_block2_attn_theta_h  (None, 28, 28, 64)           0         ['stack1_block2_attn_theta_h_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack1_block2_attn_theta_w  (None, 28, 28, 64)           0         ['stack1_block2_attn_theta_w_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack1_block2_attn_height_  (None, 28, 28, 64)           4096      ['stack1_block2_attn_bn[0][0]'\n conv (Conv2D)                                                      ]                             \n                                                                                                  \n tf.math.cos_2 (TFOpLambda)  (None, 28, 28, 64)           0         ['stack1_block2_attn_theta_h_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n tf.math.sin_2 (TFOpLambda)  (None, 28, 28, 64)           0         ['stack1_block2_attn_theta_h_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n stack1_block2_attn_width_c  (None, 28, 28, 64)           4096      ['stack1_block2_attn_bn[0][0]'\n onv (Conv2D)                                                       ]                             \n                                                                                                  \n tf.math.cos_3 (TFOpLambda)  (None, 28, 28, 64)           0         ['stack1_block2_attn_theta_w_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n tf.math.sin_3 (TFOpLambda)  (None, 28, 28, 64)           0         ['stack1_block2_attn_theta_w_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n multiply_7 (Multiply)       (None, 28, 28, 64)           0         ['stack1_block2_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.cos_2[0][0]']       \n                                                                                                  \n multiply_8 (Multiply)       (None, 28, 28, 64)           0         ['stack1_block2_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.sin_2[0][0]']       \n                                                                                                  \n multiply_9 (Multiply)       (None, 28, 28, 64)           0         ['stack1_block2_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.cos_3[0][0]']       \n                                                                                                  \n multiply_10 (Multiply)      (None, 28, 28, 64)           0         ['stack1_block2_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.sin_3[0][0]']       \n                                                                                                  \n concatenate_2 (Concatenate  (None, 28, 28, 128)          0         ['multiply_7[0][0]',          \n )                                                                   'multiply_8[0][0]']          \n                                                                                                  \n concatenate_3 (Concatenate  (None, 28, 28, 128)          0         ['multiply_9[0][0]',          \n )                                                                   'multiply_10[0][0]']         \n                                                                                                  \n stack1_block2_attn_height_  (None, 28, 34, 128)          0         ['concatenate_2[0][0]']       \n down_pad (ZeroPadding2D)                                                                         \n                                                                                                  \n stack1_block2_attn_width_d  (None, 34, 28, 128)          0         ['concatenate_3[0][0]']       \n own_pad (ZeroPadding2D)                                                                          \n                                                                                                  \n stack1_block2_attn_height_  (None, 28, 28, 64)           896       ['stack1_block2_attn_height_do\n down_conv (Conv2D)                                                 wn_pad[0][0]']                \n                                                                                                  \n stack1_block2_attn_width_d  (None, 28, 28, 64)           896       ['stack1_block2_attn_width_dow\n own_conv (Conv2D)                                                  n_pad[0][0]']                 \n                                                                                                  \n stack1_block2_attn_channel  (None, 28, 28, 64)           4096      ['stack1_block2_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack1_block2_attn_combine  (None, 28, 28, 64)           0         ['stack1_block2_attn_height_do\n  (Add)                                                             wn_conv[0][0]',               \n                                                                     'stack1_block2_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'stack1_block2_attn_channel_c\n                                                                    onv[0][0]']                   \n                                                                                                  \n global_average_pooling2d_1  (None, 1, 1, 64)             0         ['stack1_block2_attn_combine[0\n  (GlobalAveragePooling2D)                                          ][0]']                        \n                                                                                                  \n stack1_block2_attn_reweigh  (None, 1, 1, 16)             1040      ['global_average_pooling2d_1[0\n t_Conv_0 (Conv2D)                                                  ][0]']                        \n                                                                                                  \n stack1_block2_attn_reweigh  (None, 1, 1, 16)             0         ['stack1_block2_attn_reweight_\n t_gelugelu (Activation)                                            Conv_0[0][0]']                \n                                                                                                  \n stack1_block2_attn_reweigh  (None, 1, 1, 192)            3264      ['stack1_block2_attn_reweight_\n t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n                                                                                                  \n reshape_1 (Reshape)         (None, 1, 1, 64, 3)          0         ['stack1_block2_attn_reweight_\n                                                                    Conv_1[0][0]']                \n                                                                                                  \n stack1_block2_attn_attenti  (None, 1, 1, 64, 3)          0         ['reshape_1[0][0]']           \n on_scores (Softmax)                                                                              \n                                                                                                  \n tf.unstack_1 (TFOpLambda)   [(None, 1, 1, 64),           0         ['stack1_block2_attn_attention\n                              (None, 1, 1, 64),                     _scores[0][0]']               \n                              (None, 1, 1, 64)]                                                   \n                                                                                                  \n multiply_11 (Multiply)      (None, 28, 28, 64)           0         ['stack1_block2_attn_height_do\n                                                                    wn_conv[0][0]',               \n                                                                     'tf.unstack_1[0][0]']        \n                                                                                                  \n multiply_12 (Multiply)      (None, 28, 28, 64)           0         ['stack1_block2_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'tf.unstack_1[0][1]']        \n                                                                                                  \n multiply_13 (Multiply)      (None, 28, 28, 64)           0         ['stack1_block2_attn_channel_c\n                                                                    onv[0][0]',                   \n                                                                     'tf.unstack_1[0][2]']        \n                                                                                                  \n add_1 (Add)                 (None, 28, 28, 64)           0         ['multiply_11[0][0]',         \n                                                                     'multiply_12[0][0]',         \n                                                                     'multiply_13[0][0]']         \n                                                                                                  \n stack1_block2_attn_out_con  (None, 28, 28, 64)           4160      ['add_1[0][0]']               \n v (Conv2D)                                                                                       \n                                                                                                  \n stack1_block2_attn_out (Ad  (None, 28, 28, 64)           0         ['stack1_block1_mlp_out[0][0]'\n d)                                                                 , 'stack1_block2_attn_out_conv\n                                                                    [0][0]']                      \n                                                                                                  \n stack1_block2_mlp_bn (Batc  (None, 28, 28, 64)           256       ['stack1_block2_attn_out[0][0]\n hNormalization)                                                    ']                            \n                                                                                                  \n stack1_block2_mlp_Conv_0 (  (None, 28, 28, 256)          16640     ['stack1_block2_mlp_bn[0][0]']\n Conv2D)                                                                                          \n                                                                                                  \n stack1_block2_mlp_gelugelu  (None, 28, 28, 256)          0         ['stack1_block2_mlp_Conv_0[0][\n  (Activation)                                                      0]']                          \n                                                                                                  \n stack1_block2_mlp_Conv_1 (  (None, 28, 28, 64)           16448     ['stack1_block2_mlp_gelugelu[0\n Conv2D)                                                            ][0]']                        \n                                                                                                  \n stack1_block2_mlp_out (Add  (None, 28, 28, 64)           0         ['stack1_block2_attn_out[0][0]\n )                                                                  ',                            \n                                                                     'stack1_block2_mlp_Conv_1[0][\n                                                                    0]']                          \n                                                                                                  \n stack2_down_sample_pad (Ze  (None, 30, 30, 64)           0         ['stack1_block2_mlp_out[0][0]'\n roPadding2D)                                                       ]                             \n                                                                                                  \n stack2_down_sample_conv (C  (None, 14, 14, 128)          73856     ['stack2_down_sample_pad[0][0]\n onv2D)                                                             ']                            \n                                                                                                  \n stack2_down_sample_bn (Bat  (None, 14, 14, 128)          512       ['stack2_down_sample_conv[0][0\n chNormalization)                                                   ]']                           \n                                                                                                  \n stack2_block1_attn_bn (Bat  (None, 14, 14, 128)          512       ['stack2_down_sample_bn[0][0]'\n chNormalization)                                                   ]                             \n                                                                                                  \n stack2_block1_attn_theta_h  (None, 14, 14, 128)          16512     ['stack2_block1_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack2_block1_attn_theta_w  (None, 14, 14, 128)          16512     ['stack2_block1_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack2_block1_attn_theta_h  (None, 14, 14, 128)          512       ['stack2_block1_attn_theta_h_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack2_block1_attn_theta_w  (None, 14, 14, 128)          512       ['stack2_block1_attn_theta_w_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack2_block1_attn_theta_h  (None, 14, 14, 128)          0         ['stack2_block1_attn_theta_h_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack2_block1_attn_theta_w  (None, 14, 14, 128)          0         ['stack2_block1_attn_theta_w_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack2_block1_attn_height_  (None, 14, 14, 128)          16384     ['stack2_block1_attn_bn[0][0]'\n conv (Conv2D)                                                      ]                             \n                                                                                                  \n tf.math.cos_4 (TFOpLambda)  (None, 14, 14, 128)          0         ['stack2_block1_attn_theta_h_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n tf.math.sin_4 (TFOpLambda)  (None, 14, 14, 128)          0         ['stack2_block1_attn_theta_h_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n stack2_block1_attn_width_c  (None, 14, 14, 128)          16384     ['stack2_block1_attn_bn[0][0]'\n onv (Conv2D)                                                       ]                             \n                                                                                                  \n tf.math.cos_5 (TFOpLambda)  (None, 14, 14, 128)          0         ['stack2_block1_attn_theta_w_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n tf.math.sin_5 (TFOpLambda)  (None, 14, 14, 128)          0         ['stack2_block1_attn_theta_w_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n multiply_14 (Multiply)      (None, 14, 14, 128)          0         ['stack2_block1_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.cos_4[0][0]']       \n                                                                                                  \n multiply_15 (Multiply)      (None, 14, 14, 128)          0         ['stack2_block1_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.sin_4[0][0]']       \n                                                                                                  \n multiply_16 (Multiply)      (None, 14, 14, 128)          0         ['stack2_block1_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.cos_5[0][0]']       \n                                                                                                  \n multiply_17 (Multiply)      (None, 14, 14, 128)          0         ['stack2_block1_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.sin_5[0][0]']       \n                                                                                                  \n concatenate_4 (Concatenate  (None, 14, 14, 256)          0         ['multiply_14[0][0]',         \n )                                                                   'multiply_15[0][0]']         \n                                                                                                  \n concatenate_5 (Concatenate  (None, 14, 14, 256)          0         ['multiply_16[0][0]',         \n )                                                                   'multiply_17[0][0]']         \n                                                                                                  \n stack2_block1_attn_height_  (None, 14, 20, 256)          0         ['concatenate_4[0][0]']       \n down_pad (ZeroPadding2D)                                                                         \n                                                                                                  \n stack2_block1_attn_width_d  (None, 20, 14, 256)          0         ['concatenate_5[0][0]']       \n own_pad (ZeroPadding2D)                                                                          \n                                                                                                  \n stack2_block1_attn_height_  (None, 14, 14, 128)          1792      ['stack2_block1_attn_height_do\n down_conv (Conv2D)                                                 wn_pad[0][0]']                \n                                                                                                  \n stack2_block1_attn_width_d  (None, 14, 14, 128)          1792      ['stack2_block1_attn_width_dow\n own_conv (Conv2D)                                                  n_pad[0][0]']                 \n                                                                                                  \n stack2_block1_attn_channel  (None, 14, 14, 128)          16384     ['stack2_block1_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack2_block1_attn_combine  (None, 14, 14, 128)          0         ['stack2_block1_attn_height_do\n  (Add)                                                             wn_conv[0][0]',               \n                                                                     'stack2_block1_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'stack2_block1_attn_channel_c\n                                                                    onv[0][0]']                   \n                                                                                                  \n global_average_pooling2d_2  (None, 1, 1, 128)            0         ['stack2_block1_attn_combine[0\n  (GlobalAveragePooling2D)                                          ][0]']                        \n                                                                                                  \n stack2_block1_attn_reweigh  (None, 1, 1, 32)             4128      ['global_average_pooling2d_2[0\n t_Conv_0 (Conv2D)                                                  ][0]']                        \n                                                                                                  \n stack2_block1_attn_reweigh  (None, 1, 1, 32)             0         ['stack2_block1_attn_reweight_\n t_gelugelu (Activation)                                            Conv_0[0][0]']                \n                                                                                                  \n stack2_block1_attn_reweigh  (None, 1, 1, 384)            12672     ['stack2_block1_attn_reweight_\n t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n                                                                                                  \n reshape_2 (Reshape)         (None, 1, 1, 128, 3)         0         ['stack2_block1_attn_reweight_\n                                                                    Conv_1[0][0]']                \n                                                                                                  \n stack2_block1_attn_attenti  (None, 1, 1, 128, 3)         0         ['reshape_2[0][0]']           \n on_scores (Softmax)                                                                              \n                                                                                                  \n tf.unstack_2 (TFOpLambda)   [(None, 1, 1, 128),          0         ['stack2_block1_attn_attention\n                              (None, 1, 1, 128),                    _scores[0][0]']               \n                              (None, 1, 1, 128)]                                                  \n                                                                                                  \n multiply_18 (Multiply)      (None, 14, 14, 128)          0         ['stack2_block1_attn_height_do\n                                                                    wn_conv[0][0]',               \n                                                                     'tf.unstack_2[0][0]']        \n                                                                                                  \n multiply_19 (Multiply)      (None, 14, 14, 128)          0         ['stack2_block1_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'tf.unstack_2[0][1]']        \n                                                                                                  \n multiply_20 (Multiply)      (None, 14, 14, 128)          0         ['stack2_block1_attn_channel_c\n                                                                    onv[0][0]',                   \n                                                                     'tf.unstack_2[0][2]']        \n                                                                                                  \n add_2 (Add)                 (None, 14, 14, 128)          0         ['multiply_18[0][0]',         \n                                                                     'multiply_19[0][0]',         \n                                                                     'multiply_20[0][0]']         \n                                                                                                  \n stack2_block1_attn_out_con  (None, 14, 14, 128)          16512     ['add_2[0][0]']               \n v (Conv2D)                                                                                       \n                                                                                                  \n stack2_block1_attn_out (Ad  (None, 14, 14, 128)          0         ['stack2_down_sample_bn[0][0]'\n d)                                                                 , 'stack2_block1_attn_out_conv\n                                                                    [0][0]']                      \n                                                                                                  \n stack2_block1_mlp_bn (Batc  (None, 14, 14, 128)          512       ['stack2_block1_attn_out[0][0]\n hNormalization)                                                    ']                            \n                                                                                                  \n stack2_block1_mlp_Conv_0 (  (None, 14, 14, 512)          66048     ['stack2_block1_mlp_bn[0][0]']\n Conv2D)                                                                                          \n                                                                                                  \n stack2_block1_mlp_gelugelu  (None, 14, 14, 512)          0         ['stack2_block1_mlp_Conv_0[0][\n  (Activation)                                                      0]']                          \n                                                                                                  \n stack2_block1_mlp_Conv_1 (  (None, 14, 14, 128)          65664     ['stack2_block1_mlp_gelugelu[0\n Conv2D)                                                            ][0]']                        \n                                                                                                  \n stack2_block1_mlp_out (Add  (None, 14, 14, 128)          0         ['stack2_block1_attn_out[0][0]\n )                                                                  ',                            \n                                                                     'stack2_block1_mlp_Conv_1[0][\n                                                                    0]']                          \n                                                                                                  \n stack2_block2_attn_bn (Bat  (None, 14, 14, 128)          512       ['stack2_block1_mlp_out[0][0]'\n chNormalization)                                                   ]                             \n                                                                                                  \n stack2_block2_attn_theta_h  (None, 14, 14, 128)          16512     ['stack2_block2_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack2_block2_attn_theta_w  (None, 14, 14, 128)          16512     ['stack2_block2_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack2_block2_attn_theta_h  (None, 14, 14, 128)          512       ['stack2_block2_attn_theta_h_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack2_block2_attn_theta_w  (None, 14, 14, 128)          512       ['stack2_block2_attn_theta_w_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack2_block2_attn_theta_h  (None, 14, 14, 128)          0         ['stack2_block2_attn_theta_h_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack2_block2_attn_theta_w  (None, 14, 14, 128)          0         ['stack2_block2_attn_theta_w_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack2_block2_attn_height_  (None, 14, 14, 128)          16384     ['stack2_block2_attn_bn[0][0]'\n conv (Conv2D)                                                      ]                             \n                                                                                                  \n tf.math.cos_6 (TFOpLambda)  (None, 14, 14, 128)          0         ['stack2_block2_attn_theta_h_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n tf.math.sin_6 (TFOpLambda)  (None, 14, 14, 128)          0         ['stack2_block2_attn_theta_h_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n stack2_block2_attn_width_c  (None, 14, 14, 128)          16384     ['stack2_block2_attn_bn[0][0]'\n onv (Conv2D)                                                       ]                             \n                                                                                                  \n tf.math.cos_7 (TFOpLambda)  (None, 14, 14, 128)          0         ['stack2_block2_attn_theta_w_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n tf.math.sin_7 (TFOpLambda)  (None, 14, 14, 128)          0         ['stack2_block2_attn_theta_w_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n multiply_21 (Multiply)      (None, 14, 14, 128)          0         ['stack2_block2_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.cos_6[0][0]']       \n                                                                                                  \n multiply_22 (Multiply)      (None, 14, 14, 128)          0         ['stack2_block2_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.sin_6[0][0]']       \n                                                                                                  \n multiply_23 (Multiply)      (None, 14, 14, 128)          0         ['stack2_block2_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.cos_7[0][0]']       \n                                                                                                  \n multiply_24 (Multiply)      (None, 14, 14, 128)          0         ['stack2_block2_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.sin_7[0][0]']       \n                                                                                                  \n concatenate_6 (Concatenate  (None, 14, 14, 256)          0         ['multiply_21[0][0]',         \n )                                                                   'multiply_22[0][0]']         \n                                                                                                  \n concatenate_7 (Concatenate  (None, 14, 14, 256)          0         ['multiply_23[0][0]',         \n )                                                                   'multiply_24[0][0]']         \n                                                                                                  \n stack2_block2_attn_height_  (None, 14, 20, 256)          0         ['concatenate_6[0][0]']       \n down_pad (ZeroPadding2D)                                                                         \n                                                                                                  \n stack2_block2_attn_width_d  (None, 20, 14, 256)          0         ['concatenate_7[0][0]']       \n own_pad (ZeroPadding2D)                                                                          \n                                                                                                  \n stack2_block2_attn_height_  (None, 14, 14, 128)          1792      ['stack2_block2_attn_height_do\n down_conv (Conv2D)                                                 wn_pad[0][0]']                \n                                                                                                  \n stack2_block2_attn_width_d  (None, 14, 14, 128)          1792      ['stack2_block2_attn_width_dow\n own_conv (Conv2D)                                                  n_pad[0][0]']                 \n                                                                                                  \n stack2_block2_attn_channel  (None, 14, 14, 128)          16384     ['stack2_block2_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack2_block2_attn_combine  (None, 14, 14, 128)          0         ['stack2_block2_attn_height_do\n  (Add)                                                             wn_conv[0][0]',               \n                                                                     'stack2_block2_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'stack2_block2_attn_channel_c\n                                                                    onv[0][0]']                   \n                                                                                                  \n global_average_pooling2d_3  (None, 1, 1, 128)            0         ['stack2_block2_attn_combine[0\n  (GlobalAveragePooling2D)                                          ][0]']                        \n                                                                                                  \n stack2_block2_attn_reweigh  (None, 1, 1, 32)             4128      ['global_average_pooling2d_3[0\n t_Conv_0 (Conv2D)                                                  ][0]']                        \n                                                                                                  \n stack2_block2_attn_reweigh  (None, 1, 1, 32)             0         ['stack2_block2_attn_reweight_\n t_gelugelu (Activation)                                            Conv_0[0][0]']                \n                                                                                                  \n stack2_block2_attn_reweigh  (None, 1, 1, 384)            12672     ['stack2_block2_attn_reweight_\n t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n                                                                                                  \n reshape_3 (Reshape)         (None, 1, 1, 128, 3)         0         ['stack2_block2_attn_reweight_\n                                                                    Conv_1[0][0]']                \n                                                                                                  \n stack2_block2_attn_attenti  (None, 1, 1, 128, 3)         0         ['reshape_3[0][0]']           \n on_scores (Softmax)                                                                              \n                                                                                                  \n tf.unstack_3 (TFOpLambda)   [(None, 1, 1, 128),          0         ['stack2_block2_attn_attention\n                              (None, 1, 1, 128),                    _scores[0][0]']               \n                              (None, 1, 1, 128)]                                                  \n                                                                                                  \n multiply_25 (Multiply)      (None, 14, 14, 128)          0         ['stack2_block2_attn_height_do\n                                                                    wn_conv[0][0]',               \n                                                                     'tf.unstack_3[0][0]']        \n                                                                                                  \n multiply_26 (Multiply)      (None, 14, 14, 128)          0         ['stack2_block2_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'tf.unstack_3[0][1]']        \n                                                                                                  \n multiply_27 (Multiply)      (None, 14, 14, 128)          0         ['stack2_block2_attn_channel_c\n                                                                    onv[0][0]',                   \n                                                                     'tf.unstack_3[0][2]']        \n                                                                                                  \n add_3 (Add)                 (None, 14, 14, 128)          0         ['multiply_25[0][0]',         \n                                                                     'multiply_26[0][0]',         \n                                                                     'multiply_27[0][0]']         \n                                                                                                  \n stack2_block2_attn_out_con  (None, 14, 14, 128)          16512     ['add_3[0][0]']               \n v (Conv2D)                                                                                       \n                                                                                                  \n stack2_block2_attn_out (Ad  (None, 14, 14, 128)          0         ['stack2_block1_mlp_out[0][0]'\n d)                                                                 , 'stack2_block2_attn_out_conv\n                                                                    [0][0]']                      \n                                                                                                  \n stack2_block2_mlp_bn (Batc  (None, 14, 14, 128)          512       ['stack2_block2_attn_out[0][0]\n hNormalization)                                                    ']                            \n                                                                                                  \n stack2_block2_mlp_Conv_0 (  (None, 14, 14, 512)          66048     ['stack2_block2_mlp_bn[0][0]']\n Conv2D)                                                                                          \n                                                                                                  \n stack2_block2_mlp_gelugelu  (None, 14, 14, 512)          0         ['stack2_block2_mlp_Conv_0[0][\n  (Activation)                                                      0]']                          \n                                                                                                  \n stack2_block2_mlp_Conv_1 (  (None, 14, 14, 128)          65664     ['stack2_block2_mlp_gelugelu[0\n Conv2D)                                                            ][0]']                        \n                                                                                                  \n stack2_block2_mlp_out (Add  (None, 14, 14, 128)          0         ['stack2_block2_attn_out[0][0]\n )                                                                  ',                            \n                                                                     'stack2_block2_mlp_Conv_1[0][\n                                                                    0]']                          \n                                                                                                  \n stack3_down_sample_pad (Ze  (None, 16, 16, 128)          0         ['stack2_block2_mlp_out[0][0]'\n roPadding2D)                                                       ]                             \n                                                                                                  \n stack3_down_sample_conv (C  (None, 7, 7, 320)            368960    ['stack3_down_sample_pad[0][0]\n onv2D)                                                             ']                            \n                                                                                                  \n stack3_down_sample_bn (Bat  (None, 7, 7, 320)            1280      ['stack3_down_sample_conv[0][0\n chNormalization)                                                   ]']                           \n                                                                                                  \n stack3_block1_attn_bn (Bat  (None, 7, 7, 320)            1280      ['stack3_down_sample_bn[0][0]'\n chNormalization)                                                   ]                             \n                                                                                                  \n stack3_block1_attn_theta_h  (None, 7, 7, 320)            102720    ['stack3_block1_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack3_block1_attn_theta_w  (None, 7, 7, 320)            102720    ['stack3_block1_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack3_block1_attn_theta_h  (None, 7, 7, 320)            1280      ['stack3_block1_attn_theta_h_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack3_block1_attn_theta_w  (None, 7, 7, 320)            1280      ['stack3_block1_attn_theta_w_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack3_block1_attn_theta_h  (None, 7, 7, 320)            0         ['stack3_block1_attn_theta_h_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack3_block1_attn_theta_w  (None, 7, 7, 320)            0         ['stack3_block1_attn_theta_w_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack3_block1_attn_height_  (None, 7, 7, 320)            102400    ['stack3_block1_attn_bn[0][0]'\n conv (Conv2D)                                                      ]                             \n                                                                                                  \n tf.math.cos_8 (TFOpLambda)  (None, 7, 7, 320)            0         ['stack3_block1_attn_theta_h_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n tf.math.sin_8 (TFOpLambda)  (None, 7, 7, 320)            0         ['stack3_block1_attn_theta_h_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n stack3_block1_attn_width_c  (None, 7, 7, 320)            102400    ['stack3_block1_attn_bn[0][0]'\n onv (Conv2D)                                                       ]                             \n                                                                                                  \n tf.math.cos_9 (TFOpLambda)  (None, 7, 7, 320)            0         ['stack3_block1_attn_theta_w_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n tf.math.sin_9 (TFOpLambda)  (None, 7, 7, 320)            0         ['stack3_block1_attn_theta_w_r\n                                                                    elu[0][0]']                   \n                                                                                                  \n multiply_28 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block1_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.cos_8[0][0]']       \n                                                                                                  \n multiply_29 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block1_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.sin_8[0][0]']       \n                                                                                                  \n multiply_30 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block1_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.cos_9[0][0]']       \n                                                                                                  \n multiply_31 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block1_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.sin_9[0][0]']       \n                                                                                                  \n concatenate_8 (Concatenate  (None, 7, 7, 640)            0         ['multiply_28[0][0]',         \n )                                                                   'multiply_29[0][0]']         \n                                                                                                  \n concatenate_9 (Concatenate  (None, 7, 7, 640)            0         ['multiply_30[0][0]',         \n )                                                                   'multiply_31[0][0]']         \n                                                                                                  \n stack3_block1_attn_height_  (None, 7, 13, 640)           0         ['concatenate_8[0][0]']       \n down_pad (ZeroPadding2D)                                                                         \n                                                                                                  \n stack3_block1_attn_width_d  (None, 13, 7, 640)           0         ['concatenate_9[0][0]']       \n own_pad (ZeroPadding2D)                                                                          \n                                                                                                  \n stack3_block1_attn_height_  (None, 7, 7, 320)            4480      ['stack3_block1_attn_height_do\n down_conv (Conv2D)                                                 wn_pad[0][0]']                \n                                                                                                  \n stack3_block1_attn_width_d  (None, 7, 7, 320)            4480      ['stack3_block1_attn_width_dow\n own_conv (Conv2D)                                                  n_pad[0][0]']                 \n                                                                                                  \n stack3_block1_attn_channel  (None, 7, 7, 320)            102400    ['stack3_block1_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack3_block1_attn_combine  (None, 7, 7, 320)            0         ['stack3_block1_attn_height_do\n  (Add)                                                             wn_conv[0][0]',               \n                                                                     'stack3_block1_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'stack3_block1_attn_channel_c\n                                                                    onv[0][0]']                   \n                                                                                                  \n global_average_pooling2d_4  (None, 1, 1, 320)            0         ['stack3_block1_attn_combine[0\n  (GlobalAveragePooling2D)                                          ][0]']                        \n                                                                                                  \n stack3_block1_attn_reweigh  (None, 1, 1, 80)             25680     ['global_average_pooling2d_4[0\n t_Conv_0 (Conv2D)                                                  ][0]']                        \n                                                                                                  \n stack3_block1_attn_reweigh  (None, 1, 1, 80)             0         ['stack3_block1_attn_reweight_\n t_gelugelu (Activation)                                            Conv_0[0][0]']                \n                                                                                                  \n stack3_block1_attn_reweigh  (None, 1, 1, 960)            77760     ['stack3_block1_attn_reweight_\n t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n                                                                                                  \n reshape_4 (Reshape)         (None, 1, 1, 320, 3)         0         ['stack3_block1_attn_reweight_\n                                                                    Conv_1[0][0]']                \n                                                                                                  \n stack3_block1_attn_attenti  (None, 1, 1, 320, 3)         0         ['reshape_4[0][0]']           \n on_scores (Softmax)                                                                              \n                                                                                                  \n tf.unstack_4 (TFOpLambda)   [(None, 1, 1, 320),          0         ['stack3_block1_attn_attention\n                              (None, 1, 1, 320),                    _scores[0][0]']               \n                              (None, 1, 1, 320)]                                                  \n                                                                                                  \n multiply_32 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block1_attn_height_do\n                                                                    wn_conv[0][0]',               \n                                                                     'tf.unstack_4[0][0]']        \n                                                                                                  \n multiply_33 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block1_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'tf.unstack_4[0][1]']        \n                                                                                                  \n multiply_34 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block1_attn_channel_c\n                                                                    onv[0][0]',                   \n                                                                     'tf.unstack_4[0][2]']        \n                                                                                                  \n add_4 (Add)                 (None, 7, 7, 320)            0         ['multiply_32[0][0]',         \n                                                                     'multiply_33[0][0]',         \n                                                                     'multiply_34[0][0]']         \n                                                                                                  \n stack3_block1_attn_out_con  (None, 7, 7, 320)            102720    ['add_4[0][0]']               \n v (Conv2D)                                                                                       \n                                                                                                  \n stack3_block1_attn_out (Ad  (None, 7, 7, 320)            0         ['stack3_down_sample_bn[0][0]'\n d)                                                                 , 'stack3_block1_attn_out_conv\n                                                                    [0][0]']                      \n                                                                                                  \n stack3_block1_mlp_bn (Batc  (None, 7, 7, 320)            1280      ['stack3_block1_attn_out[0][0]\n hNormalization)                                                    ']                            \n                                                                                                  \n stack3_block1_mlp_Conv_0 (  (None, 7, 7, 1280)           410880    ['stack3_block1_mlp_bn[0][0]']\n Conv2D)                                                                                          \n                                                                                                  \n stack3_block1_mlp_gelugelu  (None, 7, 7, 1280)           0         ['stack3_block1_mlp_Conv_0[0][\n  (Activation)                                                      0]']                          \n                                                                                                  \n stack3_block1_mlp_Conv_1 (  (None, 7, 7, 320)            409920    ['stack3_block1_mlp_gelugelu[0\n Conv2D)                                                            ][0]']                        \n                                                                                                  \n stack3_block1_mlp_out (Add  (None, 7, 7, 320)            0         ['stack3_block1_attn_out[0][0]\n )                                                                  ',                            \n                                                                     'stack3_block1_mlp_Conv_1[0][\n                                                                    0]']                          \n                                                                                                  \n stack3_block2_attn_bn (Bat  (None, 7, 7, 320)            1280      ['stack3_block1_mlp_out[0][0]'\n chNormalization)                                                   ]                             \n                                                                                                  \n stack3_block2_attn_theta_h  (None, 7, 7, 320)            102720    ['stack3_block2_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack3_block2_attn_theta_w  (None, 7, 7, 320)            102720    ['stack3_block2_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack3_block2_attn_theta_h  (None, 7, 7, 320)            1280      ['stack3_block2_attn_theta_h_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack3_block2_attn_theta_w  (None, 7, 7, 320)            1280      ['stack3_block2_attn_theta_w_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack3_block2_attn_theta_h  (None, 7, 7, 320)            0         ['stack3_block2_attn_theta_h_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack3_block2_attn_theta_w  (None, 7, 7, 320)            0         ['stack3_block2_attn_theta_w_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack3_block2_attn_height_  (None, 7, 7, 320)            102400    ['stack3_block2_attn_bn[0][0]'\n conv (Conv2D)                                                      ]                             \n                                                                                                  \n tf.math.cos_10 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block2_attn_theta_h_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n tf.math.sin_10 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block2_attn_theta_h_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n stack3_block2_attn_width_c  (None, 7, 7, 320)            102400    ['stack3_block2_attn_bn[0][0]'\n onv (Conv2D)                                                       ]                             \n                                                                                                  \n tf.math.cos_11 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block2_attn_theta_w_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n tf.math.sin_11 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block2_attn_theta_w_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n multiply_35 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block2_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.cos_10[0][0]']      \n                                                                                                  \n multiply_36 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block2_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.sin_10[0][0]']      \n                                                                                                  \n multiply_37 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block2_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.cos_11[0][0]']      \n                                                                                                  \n multiply_38 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block2_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.sin_11[0][0]']      \n                                                                                                  \n concatenate_10 (Concatenat  (None, 7, 7, 640)            0         ['multiply_35[0][0]',         \n e)                                                                  'multiply_36[0][0]']         \n                                                                                                  \n concatenate_11 (Concatenat  (None, 7, 7, 640)            0         ['multiply_37[0][0]',         \n e)                                                                  'multiply_38[0][0]']         \n                                                                                                  \n stack3_block2_attn_height_  (None, 7, 13, 640)           0         ['concatenate_10[0][0]']      \n down_pad (ZeroPadding2D)                                                                         \n                                                                                                  \n stack3_block2_attn_width_d  (None, 13, 7, 640)           0         ['concatenate_11[0][0]']      \n own_pad (ZeroPadding2D)                                                                          \n                                                                                                  \n stack3_block2_attn_height_  (None, 7, 7, 320)            4480      ['stack3_block2_attn_height_do\n down_conv (Conv2D)                                                 wn_pad[0][0]']                \n                                                                                                  \n stack3_block2_attn_width_d  (None, 7, 7, 320)            4480      ['stack3_block2_attn_width_dow\n own_conv (Conv2D)                                                  n_pad[0][0]']                 \n                                                                                                  \n stack3_block2_attn_channel  (None, 7, 7, 320)            102400    ['stack3_block2_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack3_block2_attn_combine  (None, 7, 7, 320)            0         ['stack3_block2_attn_height_do\n  (Add)                                                             wn_conv[0][0]',               \n                                                                     'stack3_block2_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'stack3_block2_attn_channel_c\n                                                                    onv[0][0]']                   \n                                                                                                  \n global_average_pooling2d_5  (None, 1, 1, 320)            0         ['stack3_block2_attn_combine[0\n  (GlobalAveragePooling2D)                                          ][0]']                        \n                                                                                                  \n stack3_block2_attn_reweigh  (None, 1, 1, 80)             25680     ['global_average_pooling2d_5[0\n t_Conv_0 (Conv2D)                                                  ][0]']                        \n                                                                                                  \n stack3_block2_attn_reweigh  (None, 1, 1, 80)             0         ['stack3_block2_attn_reweight_\n t_gelugelu (Activation)                                            Conv_0[0][0]']                \n                                                                                                  \n stack3_block2_attn_reweigh  (None, 1, 1, 960)            77760     ['stack3_block2_attn_reweight_\n t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n                                                                                                  \n reshape_5 (Reshape)         (None, 1, 1, 320, 3)         0         ['stack3_block2_attn_reweight_\n                                                                    Conv_1[0][0]']                \n                                                                                                  \n stack3_block2_attn_attenti  (None, 1, 1, 320, 3)         0         ['reshape_5[0][0]']           \n on_scores (Softmax)                                                                              \n                                                                                                  \n tf.unstack_5 (TFOpLambda)   [(None, 1, 1, 320),          0         ['stack3_block2_attn_attention\n                              (None, 1, 1, 320),                    _scores[0][0]']               \n                              (None, 1, 1, 320)]                                                  \n                                                                                                  \n multiply_39 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block2_attn_height_do\n                                                                    wn_conv[0][0]',               \n                                                                     'tf.unstack_5[0][0]']        \n                                                                                                  \n multiply_40 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block2_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'tf.unstack_5[0][1]']        \n                                                                                                  \n multiply_41 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block2_attn_channel_c\n                                                                    onv[0][0]',                   \n                                                                     'tf.unstack_5[0][2]']        \n                                                                                                  \n add_5 (Add)                 (None, 7, 7, 320)            0         ['multiply_39[0][0]',         \n                                                                     'multiply_40[0][0]',         \n                                                                     'multiply_41[0][0]']         \n                                                                                                  \n stack3_block2_attn_out_con  (None, 7, 7, 320)            102720    ['add_5[0][0]']               \n v (Conv2D)                                                                                       \n                                                                                                  \n stack3_block2_attn_out (Ad  (None, 7, 7, 320)            0         ['stack3_block1_mlp_out[0][0]'\n d)                                                                 , 'stack3_block2_attn_out_conv\n                                                                    [0][0]']                      \n                                                                                                  \n stack3_block2_mlp_bn (Batc  (None, 7, 7, 320)            1280      ['stack3_block2_attn_out[0][0]\n hNormalization)                                                    ']                            \n                                                                                                  \n stack3_block2_mlp_Conv_0 (  (None, 7, 7, 1280)           410880    ['stack3_block2_mlp_bn[0][0]']\n Conv2D)                                                                                          \n                                                                                                  \n stack3_block2_mlp_gelugelu  (None, 7, 7, 1280)           0         ['stack3_block2_mlp_Conv_0[0][\n  (Activation)                                                      0]']                          \n                                                                                                  \n stack3_block2_mlp_Conv_1 (  (None, 7, 7, 320)            409920    ['stack3_block2_mlp_gelugelu[0\n Conv2D)                                                            ][0]']                        \n                                                                                                  \n stack3_block2_mlp_out (Add  (None, 7, 7, 320)            0         ['stack3_block2_attn_out[0][0]\n )                                                                  ',                            \n                                                                     'stack3_block2_mlp_Conv_1[0][\n                                                                    0]']                          \n                                                                                                  \n stack3_block3_attn_bn (Bat  (None, 7, 7, 320)            1280      ['stack3_block2_mlp_out[0][0]'\n chNormalization)                                                   ]                             \n                                                                                                  \n stack3_block3_attn_theta_h  (None, 7, 7, 320)            102720    ['stack3_block3_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack3_block3_attn_theta_w  (None, 7, 7, 320)            102720    ['stack3_block3_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack3_block3_attn_theta_h  (None, 7, 7, 320)            1280      ['stack3_block3_attn_theta_h_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack3_block3_attn_theta_w  (None, 7, 7, 320)            1280      ['stack3_block3_attn_theta_w_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack3_block3_attn_theta_h  (None, 7, 7, 320)            0         ['stack3_block3_attn_theta_h_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack3_block3_attn_theta_w  (None, 7, 7, 320)            0         ['stack3_block3_attn_theta_w_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack3_block3_attn_height_  (None, 7, 7, 320)            102400    ['stack3_block3_attn_bn[0][0]'\n conv (Conv2D)                                                      ]                             \n                                                                                                  \n tf.math.cos_12 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block3_attn_theta_h_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n tf.math.sin_12 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block3_attn_theta_h_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n stack3_block3_attn_width_c  (None, 7, 7, 320)            102400    ['stack3_block3_attn_bn[0][0]'\n onv (Conv2D)                                                       ]                             \n                                                                                                  \n tf.math.cos_13 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block3_attn_theta_w_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n tf.math.sin_13 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block3_attn_theta_w_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n multiply_42 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block3_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.cos_12[0][0]']      \n                                                                                                  \n multiply_43 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block3_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.sin_12[0][0]']      \n                                                                                                  \n multiply_44 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block3_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.cos_13[0][0]']      \n                                                                                                  \n multiply_45 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block3_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.sin_13[0][0]']      \n                                                                                                  \n concatenate_12 (Concatenat  (None, 7, 7, 640)            0         ['multiply_42[0][0]',         \n e)                                                                  'multiply_43[0][0]']         \n                                                                                                  \n concatenate_13 (Concatenat  (None, 7, 7, 640)            0         ['multiply_44[0][0]',         \n e)                                                                  'multiply_45[0][0]']         \n                                                                                                  \n stack3_block3_attn_height_  (None, 7, 13, 640)           0         ['concatenate_12[0][0]']      \n down_pad (ZeroPadding2D)                                                                         \n                                                                                                  \n stack3_block3_attn_width_d  (None, 13, 7, 640)           0         ['concatenate_13[0][0]']      \n own_pad (ZeroPadding2D)                                                                          \n                                                                                                  \n stack3_block3_attn_height_  (None, 7, 7, 320)            4480      ['stack3_block3_attn_height_do\n down_conv (Conv2D)                                                 wn_pad[0][0]']                \n                                                                                                  \n stack3_block3_attn_width_d  (None, 7, 7, 320)            4480      ['stack3_block3_attn_width_dow\n own_conv (Conv2D)                                                  n_pad[0][0]']                 \n                                                                                                  \n stack3_block3_attn_channel  (None, 7, 7, 320)            102400    ['stack3_block3_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack3_block3_attn_combine  (None, 7, 7, 320)            0         ['stack3_block3_attn_height_do\n  (Add)                                                             wn_conv[0][0]',               \n                                                                     'stack3_block3_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'stack3_block3_attn_channel_c\n                                                                    onv[0][0]']                   \n                                                                                                  \n global_average_pooling2d_6  (None, 1, 1, 320)            0         ['stack3_block3_attn_combine[0\n  (GlobalAveragePooling2D)                                          ][0]']                        \n                                                                                                  \n stack3_block3_attn_reweigh  (None, 1, 1, 80)             25680     ['global_average_pooling2d_6[0\n t_Conv_0 (Conv2D)                                                  ][0]']                        \n                                                                                                  \n stack3_block3_attn_reweigh  (None, 1, 1, 80)             0         ['stack3_block3_attn_reweight_\n t_gelugelu (Activation)                                            Conv_0[0][0]']                \n                                                                                                  \n stack3_block3_attn_reweigh  (None, 1, 1, 960)            77760     ['stack3_block3_attn_reweight_\n t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n                                                                                                  \n reshape_6 (Reshape)         (None, 1, 1, 320, 3)         0         ['stack3_block3_attn_reweight_\n                                                                    Conv_1[0][0]']                \n                                                                                                  \n stack3_block3_attn_attenti  (None, 1, 1, 320, 3)         0         ['reshape_6[0][0]']           \n on_scores (Softmax)                                                                              \n                                                                                                  \n tf.unstack_6 (TFOpLambda)   [(None, 1, 1, 320),          0         ['stack3_block3_attn_attention\n                              (None, 1, 1, 320),                    _scores[0][0]']               \n                              (None, 1, 1, 320)]                                                  \n                                                                                                  \n multiply_46 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block3_attn_height_do\n                                                                    wn_conv[0][0]',               \n                                                                     'tf.unstack_6[0][0]']        \n                                                                                                  \n multiply_47 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block3_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'tf.unstack_6[0][1]']        \n                                                                                                  \n multiply_48 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block3_attn_channel_c\n                                                                    onv[0][0]',                   \n                                                                     'tf.unstack_6[0][2]']        \n                                                                                                  \n add_6 (Add)                 (None, 7, 7, 320)            0         ['multiply_46[0][0]',         \n                                                                     'multiply_47[0][0]',         \n                                                                     'multiply_48[0][0]']         \n                                                                                                  \n stack3_block3_attn_out_con  (None, 7, 7, 320)            102720    ['add_6[0][0]']               \n v (Conv2D)                                                                                       \n                                                                                                  \n stack3_block3_attn_out (Ad  (None, 7, 7, 320)            0         ['stack3_block2_mlp_out[0][0]'\n d)                                                                 , 'stack3_block3_attn_out_conv\n                                                                    [0][0]']                      \n                                                                                                  \n stack3_block3_mlp_bn (Batc  (None, 7, 7, 320)            1280      ['stack3_block3_attn_out[0][0]\n hNormalization)                                                    ']                            \n                                                                                                  \n stack3_block3_mlp_Conv_0 (  (None, 7, 7, 1280)           410880    ['stack3_block3_mlp_bn[0][0]']\n Conv2D)                                                                                          \n                                                                                                  \n stack3_block3_mlp_gelugelu  (None, 7, 7, 1280)           0         ['stack3_block3_mlp_Conv_0[0][\n  (Activation)                                                      0]']                          \n                                                                                                  \n stack3_block3_mlp_Conv_1 (  (None, 7, 7, 320)            409920    ['stack3_block3_mlp_gelugelu[0\n Conv2D)                                                            ][0]']                        \n                                                                                                  \n stack3_block3_mlp_out (Add  (None, 7, 7, 320)            0         ['stack3_block3_attn_out[0][0]\n )                                                                  ',                            \n                                                                     'stack3_block3_mlp_Conv_1[0][\n                                                                    0]']                          \n                                                                                                  \n stack3_block4_attn_bn (Bat  (None, 7, 7, 320)            1280      ['stack3_block3_mlp_out[0][0]'\n chNormalization)                                                   ]                             \n                                                                                                  \n stack3_block4_attn_theta_h  (None, 7, 7, 320)            102720    ['stack3_block4_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack3_block4_attn_theta_w  (None, 7, 7, 320)            102720    ['stack3_block4_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack3_block4_attn_theta_h  (None, 7, 7, 320)            1280      ['stack3_block4_attn_theta_h_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack3_block4_attn_theta_w  (None, 7, 7, 320)            1280      ['stack3_block4_attn_theta_w_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack3_block4_attn_theta_h  (None, 7, 7, 320)            0         ['stack3_block4_attn_theta_h_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack3_block4_attn_theta_w  (None, 7, 7, 320)            0         ['stack3_block4_attn_theta_w_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack3_block4_attn_height_  (None, 7, 7, 320)            102400    ['stack3_block4_attn_bn[0][0]'\n conv (Conv2D)                                                      ]                             \n                                                                                                  \n tf.math.cos_14 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block4_attn_theta_h_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n tf.math.sin_14 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block4_attn_theta_h_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n stack3_block4_attn_width_c  (None, 7, 7, 320)            102400    ['stack3_block4_attn_bn[0][0]'\n onv (Conv2D)                                                       ]                             \n                                                                                                  \n tf.math.cos_15 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block4_attn_theta_w_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n tf.math.sin_15 (TFOpLambda  (None, 7, 7, 320)            0         ['stack3_block4_attn_theta_w_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n multiply_49 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block4_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.cos_14[0][0]']      \n                                                                                                  \n multiply_50 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block4_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.sin_14[0][0]']      \n                                                                                                  \n multiply_51 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block4_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.cos_15[0][0]']      \n                                                                                                  \n multiply_52 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block4_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.sin_15[0][0]']      \n                                                                                                  \n concatenate_14 (Concatenat  (None, 7, 7, 640)            0         ['multiply_49[0][0]',         \n e)                                                                  'multiply_50[0][0]']         \n                                                                                                  \n concatenate_15 (Concatenat  (None, 7, 7, 640)            0         ['multiply_51[0][0]',         \n e)                                                                  'multiply_52[0][0]']         \n                                                                                                  \n stack3_block4_attn_height_  (None, 7, 13, 640)           0         ['concatenate_14[0][0]']      \n down_pad (ZeroPadding2D)                                                                         \n                                                                                                  \n stack3_block4_attn_width_d  (None, 13, 7, 640)           0         ['concatenate_15[0][0]']      \n own_pad (ZeroPadding2D)                                                                          \n                                                                                                  \n stack3_block4_attn_height_  (None, 7, 7, 320)            4480      ['stack3_block4_attn_height_do\n down_conv (Conv2D)                                                 wn_pad[0][0]']                \n                                                                                                  \n stack3_block4_attn_width_d  (None, 7, 7, 320)            4480      ['stack3_block4_attn_width_dow\n own_conv (Conv2D)                                                  n_pad[0][0]']                 \n                                                                                                  \n stack3_block4_attn_channel  (None, 7, 7, 320)            102400    ['stack3_block4_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack3_block4_attn_combine  (None, 7, 7, 320)            0         ['stack3_block4_attn_height_do\n  (Add)                                                             wn_conv[0][0]',               \n                                                                     'stack3_block4_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'stack3_block4_attn_channel_c\n                                                                    onv[0][0]']                   \n                                                                                                  \n global_average_pooling2d_7  (None, 1, 1, 320)            0         ['stack3_block4_attn_combine[0\n  (GlobalAveragePooling2D)                                          ][0]']                        \n                                                                                                  \n stack3_block4_attn_reweigh  (None, 1, 1, 80)             25680     ['global_average_pooling2d_7[0\n t_Conv_0 (Conv2D)                                                  ][0]']                        \n                                                                                                  \n stack3_block4_attn_reweigh  (None, 1, 1, 80)             0         ['stack3_block4_attn_reweight_\n t_gelugelu (Activation)                                            Conv_0[0][0]']                \n                                                                                                  \n stack3_block4_attn_reweigh  (None, 1, 1, 960)            77760     ['stack3_block4_attn_reweight_\n t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n                                                                                                  \n reshape_7 (Reshape)         (None, 1, 1, 320, 3)         0         ['stack3_block4_attn_reweight_\n                                                                    Conv_1[0][0]']                \n                                                                                                  \n stack3_block4_attn_attenti  (None, 1, 1, 320, 3)         0         ['reshape_7[0][0]']           \n on_scores (Softmax)                                                                              \n                                                                                                  \n tf.unstack_7 (TFOpLambda)   [(None, 1, 1, 320),          0         ['stack3_block4_attn_attention\n                              (None, 1, 1, 320),                    _scores[0][0]']               \n                              (None, 1, 1, 320)]                                                  \n                                                                                                  \n multiply_53 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block4_attn_height_do\n                                                                    wn_conv[0][0]',               \n                                                                     'tf.unstack_7[0][0]']        \n                                                                                                  \n multiply_54 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block4_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'tf.unstack_7[0][1]']        \n                                                                                                  \n multiply_55 (Multiply)      (None, 7, 7, 320)            0         ['stack3_block4_attn_channel_c\n                                                                    onv[0][0]',                   \n                                                                     'tf.unstack_7[0][2]']        \n                                                                                                  \n add_7 (Add)                 (None, 7, 7, 320)            0         ['multiply_53[0][0]',         \n                                                                     'multiply_54[0][0]',         \n                                                                     'multiply_55[0][0]']         \n                                                                                                  \n stack3_block4_attn_out_con  (None, 7, 7, 320)            102720    ['add_7[0][0]']               \n v (Conv2D)                                                                                       \n                                                                                                  \n stack3_block4_attn_out (Ad  (None, 7, 7, 320)            0         ['stack3_block3_mlp_out[0][0]'\n d)                                                                 , 'stack3_block4_attn_out_conv\n                                                                    [0][0]']                      \n                                                                                                  \n stack3_block4_mlp_bn (Batc  (None, 7, 7, 320)            1280      ['stack3_block4_attn_out[0][0]\n hNormalization)                                                    ']                            \n                                                                                                  \n stack3_block4_mlp_Conv_0 (  (None, 7, 7, 1280)           410880    ['stack3_block4_mlp_bn[0][0]']\n Conv2D)                                                                                          \n                                                                                                  \n stack3_block4_mlp_gelugelu  (None, 7, 7, 1280)           0         ['stack3_block4_mlp_Conv_0[0][\n  (Activation)                                                      0]']                          \n                                                                                                  \n stack3_block4_mlp_Conv_1 (  (None, 7, 7, 320)            409920    ['stack3_block4_mlp_gelugelu[0\n Conv2D)                                                            ][0]']                        \n                                                                                                  \n stack3_block4_mlp_out (Add  (None, 7, 7, 320)            0         ['stack3_block4_attn_out[0][0]\n )                                                                  ',                            \n                                                                     'stack3_block4_mlp_Conv_1[0][\n                                                                    0]']                          \n                                                                                                  \n stack4_down_sample_pad (Ze  (None, 9, 9, 320)            0         ['stack3_block4_mlp_out[0][0]'\n roPadding2D)                                                       ]                             \n                                                                                                  \n stack4_down_sample_conv (C  (None, 4, 4, 512)            1475072   ['stack4_down_sample_pad[0][0]\n onv2D)                                                             ']                            \n                                                                                                  \n stack4_down_sample_bn (Bat  (None, 4, 4, 512)            2048      ['stack4_down_sample_conv[0][0\n chNormalization)                                                   ]']                           \n                                                                                                  \n stack4_block1_attn_bn (Bat  (None, 4, 4, 512)            2048      ['stack4_down_sample_bn[0][0]'\n chNormalization)                                                   ]                             \n                                                                                                  \n stack4_block1_attn_theta_h  (None, 4, 4, 512)            262656    ['stack4_block1_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack4_block1_attn_theta_w  (None, 4, 4, 512)            262656    ['stack4_block1_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack4_block1_attn_theta_h  (None, 4, 4, 512)            2048      ['stack4_block1_attn_theta_h_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack4_block1_attn_theta_w  (None, 4, 4, 512)            2048      ['stack4_block1_attn_theta_w_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack4_block1_attn_theta_h  (None, 4, 4, 512)            0         ['stack4_block1_attn_theta_h_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack4_block1_attn_theta_w  (None, 4, 4, 512)            0         ['stack4_block1_attn_theta_w_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack4_block1_attn_height_  (None, 4, 4, 512)            262144    ['stack4_block1_attn_bn[0][0]'\n conv (Conv2D)                                                      ]                             \n                                                                                                  \n tf.math.cos_16 (TFOpLambda  (None, 4, 4, 512)            0         ['stack4_block1_attn_theta_h_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n tf.math.sin_16 (TFOpLambda  (None, 4, 4, 512)            0         ['stack4_block1_attn_theta_h_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n stack4_block1_attn_width_c  (None, 4, 4, 512)            262144    ['stack4_block1_attn_bn[0][0]'\n onv (Conv2D)                                                       ]                             \n                                                                                                  \n tf.math.cos_17 (TFOpLambda  (None, 4, 4, 512)            0         ['stack4_block1_attn_theta_w_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n tf.math.sin_17 (TFOpLambda  (None, 4, 4, 512)            0         ['stack4_block1_attn_theta_w_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n multiply_56 (Multiply)      (None, 4, 4, 512)            0         ['stack4_block1_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.cos_16[0][0]']      \n                                                                                                  \n multiply_57 (Multiply)      (None, 4, 4, 512)            0         ['stack4_block1_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.sin_16[0][0]']      \n                                                                                                  \n multiply_58 (Multiply)      (None, 4, 4, 512)            0         ['stack4_block1_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.cos_17[0][0]']      \n                                                                                                  \n multiply_59 (Multiply)      (None, 4, 4, 512)            0         ['stack4_block1_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.sin_17[0][0]']      \n                                                                                                  \n concatenate_16 (Concatenat  (None, 4, 4, 1024)           0         ['multiply_56[0][0]',         \n e)                                                                  'multiply_57[0][0]']         \n                                                                                                  \n concatenate_17 (Concatenat  (None, 4, 4, 1024)           0         ['multiply_58[0][0]',         \n e)                                                                  'multiply_59[0][0]']         \n                                                                                                  \n stack4_block1_attn_height_  (None, 4, 10, 1024)          0         ['concatenate_16[0][0]']      \n down_pad (ZeroPadding2D)                                                                         \n                                                                                                  \n stack4_block1_attn_width_d  (None, 10, 4, 1024)          0         ['concatenate_17[0][0]']      \n own_pad (ZeroPadding2D)                                                                          \n                                                                                                  \n stack4_block1_attn_height_  (None, 4, 4, 512)            7168      ['stack4_block1_attn_height_do\n down_conv (Conv2D)                                                 wn_pad[0][0]']                \n                                                                                                  \n stack4_block1_attn_width_d  (None, 4, 4, 512)            7168      ['stack4_block1_attn_width_dow\n own_conv (Conv2D)                                                  n_pad[0][0]']                 \n                                                                                                  \n stack4_block1_attn_channel  (None, 4, 4, 512)            262144    ['stack4_block1_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack4_block1_attn_combine  (None, 4, 4, 512)            0         ['stack4_block1_attn_height_do\n  (Add)                                                             wn_conv[0][0]',               \n                                                                     'stack4_block1_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'stack4_block1_attn_channel_c\n                                                                    onv[0][0]']                   \n                                                                                                  \n global_average_pooling2d_8  (None, 1, 1, 512)            0         ['stack4_block1_attn_combine[0\n  (GlobalAveragePooling2D)                                          ][0]']                        \n                                                                                                  \n stack4_block1_attn_reweigh  (None, 1, 1, 128)            65664     ['global_average_pooling2d_8[0\n t_Conv_0 (Conv2D)                                                  ][0]']                        \n                                                                                                  \n stack4_block1_attn_reweigh  (None, 1, 1, 128)            0         ['stack4_block1_attn_reweight_\n t_gelugelu (Activation)                                            Conv_0[0][0]']                \n                                                                                                  \n stack4_block1_attn_reweigh  (None, 1, 1, 1536)           198144    ['stack4_block1_attn_reweight_\n t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n                                                                                                  \n reshape_8 (Reshape)         (None, 1, 1, 512, 3)         0         ['stack4_block1_attn_reweight_\n                                                                    Conv_1[0][0]']                \n                                                                                                  \n stack4_block1_attn_attenti  (None, 1, 1, 512, 3)         0         ['reshape_8[0][0]']           \n on_scores (Softmax)                                                                              \n                                                                                                  \n tf.unstack_8 (TFOpLambda)   [(None, 1, 1, 512),          0         ['stack4_block1_attn_attention\n                              (None, 1, 1, 512),                    _scores[0][0]']               \n                              (None, 1, 1, 512)]                                                  \n                                                                                                  \n multiply_60 (Multiply)      (None, 4, 4, 512)            0         ['stack4_block1_attn_height_do\n                                                                    wn_conv[0][0]',               \n                                                                     'tf.unstack_8[0][0]']        \n                                                                                                  \n multiply_61 (Multiply)      (None, 4, 4, 512)            0         ['stack4_block1_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'tf.unstack_8[0][1]']        \n                                                                                                  \n multiply_62 (Multiply)      (None, 4, 4, 512)            0         ['stack4_block1_attn_channel_c\n                                                                    onv[0][0]',                   \n                                                                     'tf.unstack_8[0][2]']        \n                                                                                                  \n add_8 (Add)                 (None, 4, 4, 512)            0         ['multiply_60[0][0]',         \n                                                                     'multiply_61[0][0]',         \n                                                                     'multiply_62[0][0]']         \n                                                                                                  \n stack4_block1_attn_out_con  (None, 4, 4, 512)            262656    ['add_8[0][0]']               \n v (Conv2D)                                                                                       \n                                                                                                  \n stack4_block1_attn_out (Ad  (None, 4, 4, 512)            0         ['stack4_down_sample_bn[0][0]'\n d)                                                                 , 'stack4_block1_attn_out_conv\n                                                                    [0][0]']                      \n                                                                                                  \n stack4_block1_mlp_bn (Batc  (None, 4, 4, 512)            2048      ['stack4_block1_attn_out[0][0]\n hNormalization)                                                    ']                            \n                                                                                                  \n stack4_block1_mlp_Conv_0 (  (None, 4, 4, 2048)           1050624   ['stack4_block1_mlp_bn[0][0]']\n Conv2D)                                                                                          \n                                                                                                  \n stack4_block1_mlp_gelugelu  (None, 4, 4, 2048)           0         ['stack4_block1_mlp_Conv_0[0][\n  (Activation)                                                      0]']                          \n                                                                                                  \n stack4_block1_mlp_Conv_1 (  (None, 4, 4, 512)            1049088   ['stack4_block1_mlp_gelugelu[0\n Conv2D)                                                            ][0]']                        \n                                                                                                  \n stack4_block1_mlp_out (Add  (None, 4, 4, 512)            0         ['stack4_block1_attn_out[0][0]\n )                                                                  ',                            \n                                                                     'stack4_block1_mlp_Conv_1[0][\n                                                                    0]']                          \n                                                                                                  \n stack4_block2_attn_bn (Bat  (None, 4, 4, 512)            2048      ['stack4_block1_mlp_out[0][0]'\n chNormalization)                                                   ]                             \n                                                                                                  \n stack4_block2_attn_theta_h  (None, 4, 4, 512)            262656    ['stack4_block2_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack4_block2_attn_theta_w  (None, 4, 4, 512)            262656    ['stack4_block2_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack4_block2_attn_theta_h  (None, 4, 4, 512)            2048      ['stack4_block2_attn_theta_h_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack4_block2_attn_theta_w  (None, 4, 4, 512)            2048      ['stack4_block2_attn_theta_w_c\n _bn (BatchNormalization)                                           onv[0][0]']                   \n                                                                                                  \n stack4_block2_attn_theta_h  (None, 4, 4, 512)            0         ['stack4_block2_attn_theta_h_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack4_block2_attn_theta_w  (None, 4, 4, 512)            0         ['stack4_block2_attn_theta_w_b\n _relu (Activation)                                                 n[0][0]']                     \n                                                                                                  \n stack4_block2_attn_height_  (None, 4, 4, 512)            262144    ['stack4_block2_attn_bn[0][0]'\n conv (Conv2D)                                                      ]                             \n                                                                                                  \n tf.math.cos_18 (TFOpLambda  (None, 4, 4, 512)            0         ['stack4_block2_attn_theta_h_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n tf.math.sin_18 (TFOpLambda  (None, 4, 4, 512)            0         ['stack4_block2_attn_theta_h_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n stack4_block2_attn_width_c  (None, 4, 4, 512)            262144    ['stack4_block2_attn_bn[0][0]'\n onv (Conv2D)                                                       ]                             \n                                                                                                  \n tf.math.cos_19 (TFOpLambda  (None, 4, 4, 512)            0         ['stack4_block2_attn_theta_w_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n tf.math.sin_19 (TFOpLambda  (None, 4, 4, 512)            0         ['stack4_block2_attn_theta_w_r\n )                                                                  elu[0][0]']                   \n                                                                                                  \n multiply_63 (Multiply)      (None, 4, 4, 512)            0         ['stack4_block2_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.cos_18[0][0]']      \n                                                                                                  \n multiply_64 (Multiply)      (None, 4, 4, 512)            0         ['stack4_block2_attn_height_co\n                                                                    nv[0][0]',                    \n                                                                     'tf.math.sin_18[0][0]']      \n                                                                                                  \n multiply_65 (Multiply)      (None, 4, 4, 512)            0         ['stack4_block2_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.cos_19[0][0]']      \n                                                                                                  \n multiply_66 (Multiply)      (None, 4, 4, 512)            0         ['stack4_block2_attn_width_con\n                                                                    v[0][0]',                     \n                                                                     'tf.math.sin_19[0][0]']      \n                                                                                                  \n concatenate_18 (Concatenat  (None, 4, 4, 1024)           0         ['multiply_63[0][0]',         \n e)                                                                  'multiply_64[0][0]']         \n                                                                                                  \n concatenate_19 (Concatenat  (None, 4, 4, 1024)           0         ['multiply_65[0][0]',         \n e)                                                                  'multiply_66[0][0]']         \n                                                                                                  \n stack4_block2_attn_height_  (None, 4, 10, 1024)          0         ['concatenate_18[0][0]']      \n down_pad (ZeroPadding2D)                                                                         \n                                                                                                  \n stack4_block2_attn_width_d  (None, 10, 4, 1024)          0         ['concatenate_19[0][0]']      \n own_pad (ZeroPadding2D)                                                                          \n                                                                                                  \n stack4_block2_attn_height_  (None, 4, 4, 512)            7168      ['stack4_block2_attn_height_do\n down_conv (Conv2D)                                                 wn_pad[0][0]']                \n                                                                                                  \n stack4_block2_attn_width_d  (None, 4, 4, 512)            7168      ['stack4_block2_attn_width_dow\n own_conv (Conv2D)                                                  n_pad[0][0]']                 \n                                                                                                  \n stack4_block2_attn_channel  (None, 4, 4, 512)            262144    ['stack4_block2_attn_bn[0][0]'\n _conv (Conv2D)                                                     ]                             \n                                                                                                  \n stack4_block2_attn_combine  (None, 4, 4, 512)            0         ['stack4_block2_attn_height_do\n  (Add)                                                             wn_conv[0][0]',               \n                                                                     'stack4_block2_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'stack4_block2_attn_channel_c\n                                                                    onv[0][0]']                   \n                                                                                                  \n global_average_pooling2d_9  (None, 1, 1, 512)            0         ['stack4_block2_attn_combine[0\n  (GlobalAveragePooling2D)                                          ][0]']                        \n                                                                                                  \n stack4_block2_attn_reweigh  (None, 1, 1, 128)            65664     ['global_average_pooling2d_9[0\n t_Conv_0 (Conv2D)                                                  ][0]']                        \n                                                                                                  \n stack4_block2_attn_reweigh  (None, 1, 1, 128)            0         ['stack4_block2_attn_reweight_\n t_gelugelu (Activation)                                            Conv_0[0][0]']                \n                                                                                                  \n stack4_block2_attn_reweigh  (None, 1, 1, 1536)           198144    ['stack4_block2_attn_reweight_\n t_Conv_1 (Conv2D)                                                  gelugelu[0][0]']              \n                                                                                                  \n reshape_9 (Reshape)         (None, 1, 1, 512, 3)         0         ['stack4_block2_attn_reweight_\n                                                                    Conv_1[0][0]']                \n                                                                                                  \n stack4_block2_attn_attenti  (None, 1, 1, 512, 3)         0         ['reshape_9[0][0]']           \n on_scores (Softmax)                                                                              \n                                                                                                  \n tf.unstack_9 (TFOpLambda)   [(None, 1, 1, 512),          0         ['stack4_block2_attn_attention\n                              (None, 1, 1, 512),                    _scores[0][0]']               \n                              (None, 1, 1, 512)]                                                  \n                                                                                                  \n multiply_67 (Multiply)      (None, 4, 4, 512)            0         ['stack4_block2_attn_height_do\n                                                                    wn_conv[0][0]',               \n                                                                     'tf.unstack_9[0][0]']        \n                                                                                                  \n multiply_68 (Multiply)      (None, 4, 4, 512)            0         ['stack4_block2_attn_width_dow\n                                                                    n_conv[0][0]',                \n                                                                     'tf.unstack_9[0][1]']        \n                                                                                                  \n multiply_69 (Multiply)      (None, 4, 4, 512)            0         ['stack4_block2_attn_channel_c\n                                                                    onv[0][0]',                   \n                                                                     'tf.unstack_9[0][2]']        \n                                                                                                  \n add_9 (Add)                 (None, 4, 4, 512)            0         ['multiply_67[0][0]',         \n                                                                     'multiply_68[0][0]',         \n                                                                     'multiply_69[0][0]']         \n                                                                                                  \n stack4_block2_attn_out_con  (None, 4, 4, 512)            262656    ['add_9[0][0]']               \n v (Conv2D)                                                                                       \n                                                                                                  \n stack4_block2_attn_out (Ad  (None, 4, 4, 512)            0         ['stack4_block1_mlp_out[0][0]'\n d)                                                                 , 'stack4_block2_attn_out_conv\n                                                                    [0][0]']                      \n                                                                                                  \n stack4_block2_mlp_bn (Batc  (None, 4, 4, 512)            2048      ['stack4_block2_attn_out[0][0]\n hNormalization)                                                    ']                            \n                                                                                                  \n stack4_block2_mlp_Conv_0 (  (None, 4, 4, 2048)           1050624   ['stack4_block2_mlp_bn[0][0]']\n Conv2D)                                                                                          \n                                                                                                  \n stack4_block2_mlp_gelugelu  (None, 4, 4, 2048)           0         ['stack4_block2_mlp_Conv_0[0][\n  (Activation)                                                      0]']                          \n                                                                                                  \n stack4_block2_mlp_Conv_1 (  (None, 4, 4, 512)            1049088   ['stack4_block2_mlp_gelugelu[0\n Conv2D)                                                            ][0]']                        \n                                                                                                  \n stack4_block2_mlp_out (Add  (None, 4, 4, 512)            0         ['stack4_block2_attn_out[0][0]\n )                                                                  ',                            \n                                                                     'stack4_block2_mlp_Conv_1[0][\n                                                                    0]']                          \n                                                                                                  \n output_bn (BatchNormalizat  (None, 4, 4, 512)            2048      ['stack4_block2_mlp_out[0][0]'\n ion)                                                               ]                             \n                                                                                                  \n avg_pool (GlobalAveragePoo  (None, 512)                  0         ['output_bn[0][0]']           \n ling2D)                                                                                          \n                                                                                                  \n predictions (Dense)         (None, 1000)                 513000    ['avg_pool[0][0]']            \n                                                                                                  \n tf.reshape (TFOpLambda)     (None, 31, 31, 1)            0         ['predictions[0][0]']         \n                                                                                                  \n self_attention (SelfAttent  (None, 31, 31, 1)            3         ['tf.reshape[0][0]']          \n ion)                                                                                             \n                                                                                                  \n conv2d (Conv2D)             (None, 31, 31, 64)           640       ['self_attention[0][0]']      \n                                                                                                  \n batch_normalization (Batch  (None, 31, 31, 64)           256       ['conv2d[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n activation (Activation)     (None, 31, 31, 64)           0         ['batch_normalization[0][0]'] \n                                                                                                  \n conv2d_1 (Conv2D)           (None, 31, 31, 64)           36928     ['activation[0][0]']          \n                                                                                                  \n batch_normalization_1 (Bat  (None, 31, 31, 64)           256       ['conv2d_1[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_1 (Activation)   (None, 31, 31, 64)           0         ['batch_normalization_1[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_2 (Conv2D)           (None, 31, 31, 64)           128       ['tf.reshape[0][0]']          \n                                                                                                  \n add_10 (Add)                (None, 31, 31, 64)           0         ['activation_1[0][0]',        \n                                                                     'conv2d_2[0][0]']            \n                                                                                                  \n self_attention_1 (SelfAtte  (None, 31, 31, 64)           12288     ['add_10[0][0]']              \n ntion)                                                                                           \n                                                                                                  \n conv2d_3 (Conv2D)           (None, 31, 31, 64)           36928     ['self_attention_1[0][0]']    \n                                                                                                  \n batch_normalization_2 (Bat  (None, 31, 31, 64)           256       ['conv2d_3[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_2 (Activation)   (None, 31, 31, 64)           0         ['batch_normalization_2[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_4 (Conv2D)           (None, 31, 31, 64)           36928     ['activation_2[0][0]']        \n                                                                                                  \n batch_normalization_3 (Bat  (None, 31, 31, 64)           256       ['conv2d_4[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_3 (Activation)   (None, 31, 31, 64)           0         ['batch_normalization_3[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_5 (Conv2D)           (None, 31, 31, 64)           4160      ['add_10[0][0]']              \n                                                                                                  \n add_11 (Add)                (None, 31, 31, 64)           0         ['activation_3[0][0]',        \n                                                                     'conv2d_5[0][0]']            \n                                                                                                  \n self_attention_2 (SelfAtte  (None, 31, 31, 64)           12288     ['add_11[0][0]']              \n ntion)                                                                                           \n                                                                                                  \n conv2d_6 (Conv2D)           (None, 31, 31, 64)           36928     ['self_attention_2[0][0]']    \n                                                                                                  \n batch_normalization_4 (Bat  (None, 31, 31, 64)           256       ['conv2d_6[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_4 (Activation)   (None, 31, 31, 64)           0         ['batch_normalization_4[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_7 (Conv2D)           (None, 31, 31, 64)           36928     ['activation_4[0][0]']        \n                                                                                                  \n batch_normalization_5 (Bat  (None, 31, 31, 64)           256       ['conv2d_7[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_5 (Activation)   (None, 31, 31, 64)           0         ['batch_normalization_5[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_8 (Conv2D)           (None, 31, 31, 64)           4160      ['add_11[0][0]']              \n                                                                                                  \n add_12 (Add)                (None, 31, 31, 64)           0         ['activation_5[0][0]',        \n                                                                     'conv2d_8[0][0]']            \n                                                                                                  \n max_pooling2d (MaxPooling2  (None, 15, 15, 64)           0         ['add_12[0][0]']              \n D)                                                                                               \n                                                                                                  \n self_attention_3 (SelfAtte  (None, 15, 15, 64)           12288     ['max_pooling2d[0][0]']       \n ntion)                                                                                           \n                                                                                                  \n conv2d_9 (Conv2D)           (None, 15, 15, 128)          73856     ['self_attention_3[0][0]']    \n                                                                                                  \n batch_normalization_6 (Bat  (None, 15, 15, 128)          512       ['conv2d_9[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_6 (Activation)   (None, 15, 15, 128)          0         ['batch_normalization_6[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_10 (Conv2D)          (None, 15, 15, 128)          147584    ['activation_6[0][0]']        \n                                                                                                  \n batch_normalization_7 (Bat  (None, 15, 15, 128)          512       ['conv2d_10[0][0]']           \n chNormalization)                                                                                 \n                                                                                                  \n activation_7 (Activation)   (None, 15, 15, 128)          0         ['batch_normalization_7[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_11 (Conv2D)          (None, 15, 15, 128)          8320      ['max_pooling2d[0][0]']       \n                                                                                                  \n add_13 (Add)                (None, 15, 15, 128)          0         ['activation_7[0][0]',        \n                                                                     'conv2d_11[0][0]']           \n                                                                                                  \n self_attention_4 (SelfAtte  (None, 15, 15, 128)          49152     ['add_13[0][0]']              \n ntion)                                                                                           \n                                                                                                  \n conv2d_12 (Conv2D)          (None, 15, 15, 128)          147584    ['self_attention_4[0][0]']    \n                                                                                                  \n batch_normalization_8 (Bat  (None, 15, 15, 128)          512       ['conv2d_12[0][0]']           \n chNormalization)                                                                                 \n                                                                                                  \n activation_8 (Activation)   (None, 15, 15, 128)          0         ['batch_normalization_8[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_13 (Conv2D)          (None, 15, 15, 128)          147584    ['activation_8[0][0]']        \n                                                                                                  \n batch_normalization_9 (Bat  (None, 15, 15, 128)          512       ['conv2d_13[0][0]']           \n chNormalization)                                                                                 \n                                                                                                  \n activation_9 (Activation)   (None, 15, 15, 128)          0         ['batch_normalization_9[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_14 (Conv2D)          (None, 15, 15, 128)          16512     ['add_13[0][0]']              \n                                                                                                  \n add_14 (Add)                (None, 15, 15, 128)          0         ['activation_9[0][0]',        \n                                                                     'conv2d_14[0][0]']           \n                                                                                                  \n self_attention_5 (SelfAtte  (None, 15, 15, 128)          49152     ['add_14[0][0]']              \n ntion)                                                                                           \n                                                                                                  \n conv2d_15 (Conv2D)          (None, 15, 15, 128)          147584    ['self_attention_5[0][0]']    \n                                                                                                  \n batch_normalization_10 (Ba  (None, 15, 15, 128)          512       ['conv2d_15[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_10 (Activation)  (None, 15, 15, 128)          0         ['batch_normalization_10[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_16 (Conv2D)          (None, 15, 15, 128)          147584    ['activation_10[0][0]']       \n                                                                                                  \n batch_normalization_11 (Ba  (None, 15, 15, 128)          512       ['conv2d_16[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_11 (Activation)  (None, 15, 15, 128)          0         ['batch_normalization_11[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_17 (Conv2D)          (None, 15, 15, 128)          16512     ['add_14[0][0]']              \n                                                                                                  \n add_15 (Add)                (None, 15, 15, 128)          0         ['activation_11[0][0]',       \n                                                                     'conv2d_17[0][0]']           \n                                                                                                  \n max_pooling2d_1 (MaxPoolin  (None, 7, 7, 128)            0         ['add_15[0][0]']              \n g2D)                                                                                             \n                                                                                                  \n self_attention_6 (SelfAtte  (None, 7, 7, 128)            49152     ['max_pooling2d_1[0][0]']     \n ntion)                                                                                           \n                                                                                                  \n conv2d_18 (Conv2D)          (None, 7, 7, 256)            295168    ['self_attention_6[0][0]']    \n                                                                                                  \n batch_normalization_12 (Ba  (None, 7, 7, 256)            1024      ['conv2d_18[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_12 (Activation)  (None, 7, 7, 256)            0         ['batch_normalization_12[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_19 (Conv2D)          (None, 7, 7, 256)            590080    ['activation_12[0][0]']       \n                                                                                                  \n batch_normalization_13 (Ba  (None, 7, 7, 256)            1024      ['conv2d_19[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_13 (Activation)  (None, 7, 7, 256)            0         ['batch_normalization_13[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_20 (Conv2D)          (None, 7, 7, 256)            33024     ['max_pooling2d_1[0][0]']     \n                                                                                                  \n add_16 (Add)                (None, 7, 7, 256)            0         ['activation_13[0][0]',       \n                                                                     'conv2d_20[0][0]']           \n                                                                                                  \n self_attention_7 (SelfAtte  (None, 7, 7, 256)            196608    ['add_16[0][0]']              \n ntion)                                                                                           \n                                                                                                  \n conv2d_21 (Conv2D)          (None, 7, 7, 256)            590080    ['self_attention_7[0][0]']    \n                                                                                                  \n batch_normalization_14 (Ba  (None, 7, 7, 256)            1024      ['conv2d_21[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_14 (Activation)  (None, 7, 7, 256)            0         ['batch_normalization_14[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_22 (Conv2D)          (None, 7, 7, 256)            590080    ['activation_14[0][0]']       \n                                                                                                  \n batch_normalization_15 (Ba  (None, 7, 7, 256)            1024      ['conv2d_22[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_15 (Activation)  (None, 7, 7, 256)            0         ['batch_normalization_15[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_23 (Conv2D)          (None, 7, 7, 256)            65792     ['add_16[0][0]']              \n                                                                                                  \n add_17 (Add)                (None, 7, 7, 256)            0         ['activation_15[0][0]',       \n                                                                     'conv2d_23[0][0]']           \n                                                                                                  \n self_attention_8 (SelfAtte  (None, 7, 7, 256)            196608    ['add_17[0][0]']              \n ntion)                                                                                           \n                                                                                                  \n conv2d_24 (Conv2D)          (None, 7, 7, 256)            590080    ['self_attention_8[0][0]']    \n                                                                                                  \n batch_normalization_16 (Ba  (None, 7, 7, 256)            1024      ['conv2d_24[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_16 (Activation)  (None, 7, 7, 256)            0         ['batch_normalization_16[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_25 (Conv2D)          (None, 7, 7, 256)            590080    ['activation_16[0][0]']       \n                                                                                                  \n batch_normalization_17 (Ba  (None, 7, 7, 256)            1024      ['conv2d_25[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_17 (Activation)  (None, 7, 7, 256)            0         ['batch_normalization_17[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_26 (Conv2D)          (None, 7, 7, 256)            65792     ['add_17[0][0]']              \n                                                                                                  \n add_18 (Add)                (None, 7, 7, 256)            0         ['activation_17[0][0]',       \n                                                                     'conv2d_26[0][0]']           \n                                                                                                  \n max_pooling2d_2 (MaxPoolin  (None, 3, 3, 256)            0         ['add_18[0][0]']              \n g2D)                                                                                             \n                                                                                                  \n conv2d_27 (Conv2D)          (None, 3, 3, 512)            1180160   ['max_pooling2d_2[0][0]']     \n                                                                                                  \n batch_normalization_18 (Ba  (None, 3, 3, 512)            2048      ['conv2d_27[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_18 (Activation)  (None, 3, 3, 512)            0         ['batch_normalization_18[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_28 (Conv2D)          (None, 3, 3, 512)            131584    ['max_pooling2d_2[0][0]']     \n                                                                                                  \n add_19 (Add)                (None, 3, 3, 512)            0         ['activation_18[0][0]',       \n                                                                     'conv2d_28[0][0]']           \n                                                                                                  \n conv2d_29 (Conv2D)          (None, 3, 3, 512)            2359808   ['add_19[0][0]']              \n                                                                                                  \n batch_normalization_19 (Ba  (None, 3, 3, 512)            2048      ['conv2d_29[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_19 (Activation)  (None, 3, 3, 512)            0         ['batch_normalization_19[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_30 (Conv2D)          (None, 3, 3, 512)            262656    ['add_19[0][0]']              \n                                                                                                  \n add_20 (Add)                (None, 3, 3, 512)            0         ['activation_19[0][0]',       \n                                                                     'conv2d_30[0][0]']           \n                                                                                                  \n conv2d_31 (Conv2D)          (None, 3, 3, 512)            2359808   ['add_20[0][0]']              \n                                                                                                  \n batch_normalization_20 (Ba  (None, 3, 3, 512)            2048      ['conv2d_31[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_20 (Activation)  (None, 3, 3, 512)            0         ['batch_normalization_20[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_32 (Conv2D)          (None, 3, 3, 512)            262656    ['add_20[0][0]']              \n                                                                                                  \n add_21 (Add)                (None, 3, 3, 512)            0         ['activation_20[0][0]',       \n                                                                     'conv2d_32[0][0]']           \n                                                                                                  \n max_pooling2d_3 (MaxPoolin  (None, 1, 1, 512)            0         ['add_21[0][0]']              \n g2D)                                                                                             \n                                                                                                  \n flatten (Flatten)           (None, 512)                  0         ['max_pooling2d_3[0][0]']     \n                                                                                                  \n dense (Dense)               (None, 256)                  131328    ['flatten[0][0]']             \n                                                                                                  \n dropout (Dropout)           (None, 256)                  0         ['dense[0][0]']               \n                                                                                                  \n self_attention_9 (SelfAtte  (None, 256)                  196608    ['dropout[0][0]']             \n ntion)                                                                                           \n                                                                                                  \n add_22 (Add)                (None, 256)                  0         ['dropout[0][0]',             \n                                                                     'self_attention_9[0][0]']    \n                                                                                                  \n positional_encoding (Posit  (1, 1, 256, 256)             256       ['add_22[0][0]']              \n ionalEncoding)                                                                                   \n                                                                                                  \n add_23 (Add)                (None, 1, 256, 256)          0         ['add_22[0][0]',              \n                                                                     'positional_encoding[0][0]'] \n                                                                                                  \n dense_1 (Dense)             (None, 1, 256, 26)           6682      ['add_23[0][0]']              \n                                                                                                  \n==================================================================================================\nTotal params: 29160741 (111.24 MB)\nTrainable params: 29127717 (111.11 MB)\nNon-trainable params: 33024 (129.00 KB)\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, Layer, Add, Conv2D, BatchNormalization, Activation, MaxPooling2D\nfrom tensorflow.keras import Input, Model\n# Import your wave_mlp module\n\nclass SelfAttention(Layer):\n    def __init__(self, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.W_q = self.add_weight(name='W_q', shape=(input_shape[-1], input_shape[-1]),\n                                   initializer='uniform', trainable=True)\n        self.W_k = self.add_weight(name='W_k', shape=(input_shape[-1], input_shape[-1]),\n                                   initializer='uniform', trainable=True)\n        self.W_v = self.add_weight(name='W_v', shape=(input_shape[-1], input_shape[-1]),\n                                   initializer='uniform', trainable=True)\n        super(SelfAttention, self).build(input_shape)\n\n    def call(self, x):\n        q = x @ self.W_q\n        k = x @ self.W_k\n        v = x @ self.W_v\n\n        attn_score = tf.matmul(q, k, transpose_b=True)\n        attn_score = tf.nn.softmax(attn_score, axis=-1)\n\n        output = attn_score @ v\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\nclass PositionalEncoding(Layer):\n    def __init__(self, input_shape, **kwargs):\n        super(PositionalEncoding, self).__init__(**kwargs)\n        self.input_shape_ = input_shape\n\n    def build(self, input_shape):\n        self.positional_encoding = self.add_weight(name='positional_encoding',\n                                                   shape=(1, *self.input_shape_[1:], 1),\n                                                   initializer='uniform',\n                                                   trainable=True)\n        super(PositionalEncoding, self).build(input_shape)\n\n    def call(self, x):\n        return x + self.positional_encoding\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(256, activation='relu')(x)  # Add your own dense layers\n    x = Dropout(0.5)(x)  # Add dropout for regularization\n\n    # Add self-attention mechanism\n    self_attention = SelfAttention()(x)\n    x = Add()([x, self_attention])\n\n    # Add 2D positional encoding\n    position_encoding = PositionalEncoding(input_shape=(1, 1, 256))(x)  # Adjust the input_shape\n    x = Add()([x, position_encoding])\n\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef stage_block(x, filters, num_blocks, attention=True):\n    # Check if the input is flattened\n    if len(x.shape) == 2:\n        # Reshape to 4D tensor\n        x = tf.reshape(x, [-1, int(x.shape[1] ** 0.5), int(x.shape[1] ** 0.5), 1])\n\n    for _ in range(num_blocks):\n        # Residual block\n        residual = x\n\n        # Global self-attention\n        if attention:\n            x = SelfAttention()(x)\n\n        # Convolutional layer\n        x = Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n\n        # Residual connection (adjusting the residual size)\n        residual = Conv2D(filters, kernel_size=(1, 1), padding='same')(residual)\n        x = Add()([x, residual])\n\n    # MaxPooling for downsampling\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    return x\n\n\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Stage 1\n    x = stage_block(mm_headless.output, filters=64, num_blocks=3)\n\n    # Stage 2\n    x = stage_block(x, filters=128, num_blocks=3)\n\n    # Stage 3\n    x = stage_block(x, filters=256, num_blocks=3)\n\n    # Stage 4 (Residual blocks)\n    x = stage_block(x, filters=512, num_blocks=3, attention=False)\n\n    # Custom head\n    head_output = custom_head(x, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T08:38:22.615116Z","iopub.execute_input":"2023-12-10T08:38:22.615587Z","iopub.status.idle":"2023-12-10T08:38:27.785752Z","shell.execute_reply.started":"2023-12-10T08:38:22.615546Z","shell.execute_reply":"2023-12-10T08:38:27.784607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, Layer, Add, Conv2D, BatchNormalization, Activation, MaxPooling2D\nfrom tensorflow.keras import Input, Model\n\n# Define the MultiheadSelfAttentionBlock as a custom layer\nclass MultiheadSelfAttentionBlock(Layer):\n    def __init__(self, embedding_dim=256, num_heads=4, attn_dropout=0.1):\n        super(MultiheadSelfAttentionBlock, self).__init__()\n\n        # Create the Layer Normalization (LN)\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        # Create the Multi-Head Attention (MSA) layer\n        self.multihead_attn = tf.keras.layers.MultiHeadAttention(\n            num_heads=num_heads,\n            key_dim=embedding_dim // num_heads,\n            dropout=attn_dropout\n        )\n\n    def call(self, x):\n        # Reshape to (batch_size, sequence_length, features)\n        original_shape = tf.shape(x)\n        if len(original_shape) == 2:\n            x = tf.expand_dims(x, axis=1)\n        x = self.layer_norm(x)\n        attn_output = self.multihead_attn(x, x, x)\n\n        # Reshape back to the original shape\n        attn_output = tf.reshape(attn_output, original_shape)\n\n        return attn_output\n\ndef stage_block(x, filters, num_blocks, attention=True):\n    # Check if the input is flattened\n    if len(x.shape) == 2:\n        # Reshape to 4D tensor\n        x = tf.reshape(x, [-1, int(x.shape[1] ** 0.5), int(x.shape[1] ** 0.5), 1])\n\n    for _ in range(num_blocks):\n        # Residual block\n        residual = x\n\n        # Global self-attention\n        if attention:\n            x = MultiheadSelfAttentionBlock()(x)\n\n        # Convolutional layer\n        x = Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n\n        # Residual connection (adjusting the residual size)\n        residual = Conv2D(filters, kernel_size=(1, 1), padding='same')(residual)\n        x = Add()([x, residual])\n\n    # MaxPooling for downsampling\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    return x\n\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.5)(x)\n\n    # Add self-attention mechanism\n    self_attention = MultiheadSelfAttentionBlock()(x)\n    x = Add()([x, self_attention])\n\n    # Add 2D positional encoding\n    position_encoding = PositionalEncoding(input_shape=(1, 1, 256))(x)\n    x = Add()([x, position_encoding])\n\n    # Add Multihead Self-Attention Block\n    multihead_attention_block = MultiheadSelfAttentionBlock(embedding_dim=256, num_heads=4, attn_dropout=0.1)\n    x = multihead_attention_block(x)\n\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Stage 1\n    x = stage_block(mm_headless.output, filters=64, num_blocks=3)\n\n    # Stage 2\n    x = stage_block(x, filters=128, num_blocks=3)\n\n    # Stage 3\n    x = stage_block(x, filters=256, num_blocks=3)\n\n    # Stage 4 (Residual blocks)\n    x = stage_block(x, filters=512, num_blocks=3, attention=False)\n\n    # Custom head\n    head_output = custom_head(x, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T10:55:57.061802Z","iopub.execute_input":"2023-12-10T10:55:57.062627Z","iopub.status.idle":"2023-12-10T10:56:01.537445Z","shell.execute_reply.started":"2023-12-10T10:55:57.062593Z","shell.execute_reply":"2023-12-10T10:56:01.536322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, Layer, Add, Conv2D, BatchNormalization, Activation, MaxPooling2D\nfrom tensorflow.keras import Input, Model\n\n# Define the MultiheadSelfAttentionBlock as a custom layer\nclass MultiheadSelfAttentionBlock(Layer):\n    def __init__(self, embedding_dim=256, num_heads=4, attn_dropout=0.1):\n        super(MultiheadSelfAttentionBlock, self).__init__()\n\n        # Create the Layer Normalization (LN)\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        # Create the Multi-Head Attention (MSA) layer\n        self.multihead_attn = tf.keras.layers.MultiHeadAttention(\n            num_heads=num_heads,\n            key_dim=embedding_dim // num_heads,\n            dropout=attn_dropout\n        )\n\n    def call(self, x):\n        x = self.layer_norm(x)\n        attn_output = self.multihead_attn(x, x, x)\n        return attn_output\n\n# Your existing code...\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.5)(x)\n\n    # Add self-attention mechanism\n    self_attention = SelfAttention()(x)\n    x = Add()([x, self_attention])\n\n    # Add 2D positional encoding\n    position_encoding = PositionalEncoding(input_shape=(1, 1, 256))(x)\n    x = Add()([x, position_encoding])\n\n    # Add Multihead Self-Attention Block\n    multihead_attention_block = MultiheadSelfAttentionBlock(embedding_dim=256, num_heads=4, attn_dropout=0.1)\n    x = multihead_attention_block(x)\n\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Stage 1\n    x = stage_block(mm_headless.output, filters=64, num_blocks=3)\n\n    # Stage 2\n    x = stage_block(x, filters=128, num_blocks=3)\n\n    # Stage 3\n    x = stage_block(x, filters=256, num_blocks=3)\n\n    # Stage 4 (Residual blocks)\n    x = stage_block(x, filters=512, num_blocks=3, attention=False)\n\n    # Custom head\n    head_output = custom_head(x, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_self_attention import MultiHeadAttention\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    \n    # Add multi-headed self-attention layer\n    attention = MultiHeadAttention(head_size=128, num_heads=4)(input_tensor)\n    x = Concatenate()([input_tensor, attention])  # Concatenate attention output with input\n    \n    x = Dense(256, activation='relu')(x)  # Add your own dense layers\n    x = Dropout(0.5)(x)  # Add dropout for regularization\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n    head_output = custom_head(mm_headless.output, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T21:47:01.174616Z","iopub.execute_input":"2023-12-09T21:47:01.175011Z","iopub.status.idle":"2023-12-09T21:47:01.358735Z","shell.execute_reply.started":"2023-12-09T21:47:01.174978Z","shell.execute_reply":"2023-12-09T21:47:01.357466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Flatten, Dense, Dropout, LayerNormalization, MultiHeadAttention, Add, Input, Embedding\nfrom tensorflow.keras.models import Model\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.5)(x)\n\n    # Self-Attention Mechanism\n    att_input = Input(shape=(x.shape[1],))\n    att_output = MultiHeadAttention(num_heads=8, key_dim=16)([att_input, att_input])\n    att_output = Add()([att_input, att_output])\n    att_output = LayerNormalization(epsilon=1e-6)(att_output)\n\n    # Positional Encoding\n    pos_input = Input(shape=(x.shape[1],))\n    pos_output = Embedding(input_dim=x.shape[1], output_dim=256)(pos_input)\n    pos_output = Add()([att_output, pos_output])\n    pos_output = LayerNormalization(epsilon=1e-6)(pos_output)\n\n    # Combine attention and positional encoding\n    x = Add()([att_output, pos_output])\n\n    # Fully connected layer for classification\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n    head_output = custom_head(mm_headless.output, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\n# custom_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T07:24:53.094154Z","iopub.execute_input":"2023-12-10T07:24:53.094423Z","iopub.status.idle":"2023-12-10T07:25:12.393224Z","shell.execute_reply.started":"2023-12-10T07:24:53.094398Z","shell.execute_reply":"2023-12-10T07:25:12.391820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, Layer, Add\nfrom tensorflow.keras import Input, Model\n # Import your wave_mlp module\n\nclass SelfAttention(Layer):\n    def __init__(self, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.W_q = self.add_weight(name='W_q', shape=(input_shape[-1], input_shape[-1]),\n                                   initializer='uniform', trainable=True)\n        self.W_k = self.add_weight(name='W_k', shape=(input_shape[-1], input_shape[-1]),\n                                   initializer='uniform', trainable=True)\n        self.W_v = self.add_weight(name='W_v', shape=(input_shape[-1], input_shape[-1]),\n                                   initializer='uniform', trainable=True)\n        super(SelfAttention, self).build(input_shape)\n\n    def call(self, x):\n        q = x @ self.W_q\n        k = x @ self.W_k\n        v = x @ self.W_v\n\n        attn_score = tf.matmul(q, k, transpose_b=True)\n        attn_score = tf.nn.softmax(attn_score, axis=-1)\n\n        output = attn_score @ v\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\nclass PositionalEncoding(Layer):\n    def __init__(self, input_shape, **kwargs):\n        super(PositionalEncoding, self).__init__(**kwargs)\n        self.input_shape_ = input_shape\n\n    def build(self, input_shape):\n        self.positional_encoding = self.add_weight(name='positional_encoding',\n                                                   shape=(1, *self.input_shape_[1:], 1),\n                                                   initializer='uniform',\n                                                   trainable=True)\n        super(PositionalEncoding, self).build(input_shape)\n\n    def call(self, x):\n        return x + self.positional_encoding\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(256, activation='relu')(x)  # Add your own dense layers\n    x = Dropout(0.5)(x)  # Add dropout for regularization\n\n    # Add self-attention mechanism\n    self_attention = SelfAttention()(x)\n    x = Add()([x, self_attention])\n\n    # Add 2D positional encoding\n    position_encoding = PositionalEncoding(input_shape=(1, 1, 256))(x)  # Adjust the input_shape\n    x = Add()([x, position_encoding])\n\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n    head_output = custom_head(mm_headless.output, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\n# custom_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T22:03:08.930440Z","iopub.execute_input":"2023-12-09T22:03:08.930847Z","iopub.status.idle":"2023-12-09T22:03:12.648882Z","shell.execute_reply.started":"2023-12-09T22:03:08.930816Z","shell.execute_reply":"2023-12-09T22:03:12.648066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(256, activation='relu')(x)  # Add your own dense layers\n    x = Dropout(0.5)(x)  # Add dropout for regularization\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\n\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n#     head_output = custom_head(mm_headless.output, num_classes)\n    print(\"mm_headless.output shape:\", mm_headless.output_shape)\n    head_output = custom_head(mm_headless.output, num_classes)\n\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\n# custom_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def custom_head(input_tensor, num_classes):\n#     x = Flatten()(input_tensor)\n#     x = Dense(512, activation='relu')(x)\n#     x = Dropout(0.3)(x)\n#     x = Dense(256, activation='relu')(x)\n#     x = Dropout(0.3)(x)\n#     x = Dense(num_classes, activation='softmax')(x)\n#     return x\n\n# def modify_wave_mlp(input_shape, num_classes):\n#     # Load the WaveMLP model without the top layers (head)\n#     mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n#     # Add your custom head\n#     head_output = custom_head(mm_headless.output, num_classes)\n\n#     # Create the custom model by combining the base model and the custom head\n#     custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n#     # Fine-tune the last few layers of the base model\n#     for layer in mm_headless.layers[:-5]:\n#         layer.trainable = True\n\n#     # Compile the model with a custom learning rate and metrics\n#     custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n#     return custom_model\n\n# # Example usage:\n# input_shape = (112, 112, 3)\n# num_classes = 26  # Adjust based on your task\n\n# custom_model = modify_wave_mlp(input_shape, num_classes)\n# custom_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T22:02:09.761642Z","iopub.execute_input":"2023-12-09T22:02:09.762360Z","iopub.status.idle":"2023-12-09T22:02:09.768121Z","shell.execute_reply.started":"2023-12-09T22:02:09.762324Z","shell.execute_reply":"2023-12-09T22:02:09.767158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm_last_layer = custom_model .get_layer('avg_pool').output\n#out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\n#out = Dense(11, activation='softmax', name='prediction1')(out)\nmm_custom = Model(custom_model .input, mm_last_layer)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T08:03:09.071363Z","iopub.execute_input":"2023-12-10T08:03:09.072312Z","iopub.status.idle":"2023-12-10T08:03:09.115589Z","shell.execute_reply.started":"2023-12-10T08:03:09.072273Z","shell.execute_reply":"2023-12-10T08:03:09.114825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\ninputs = keras.Input(shape=(112,112,3))\noutputs = layers.average([mm_custom(inputs)])\n\navg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\navg_ensemble_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T08:03:13.851065Z","iopub.execute_input":"2023-12-10T08:03:13.851450Z","iopub.status.idle":"2023-12-10T08:03:14.953123Z","shell.execute_reply.started":"2023-12-10T08:03:13.851419Z","shell.execute_reply":"2023-12-10T08:03:14.952214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"mm_headless.output shape:\", mm_headless.output_shape)\nhead_output = custom_head(mm_headless.output, num_classes)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import coatnet\nmm = coatnet.CoAtNet0(input_shape=(112, 112, 3), pretrained=\"imagenet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import coatnet\nmm = coatnet.CoAtNet0(input_shape=(112, 112, 3), pretrained=\"imagenet\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import res_mlp\n# mm = res_mlp.ResMLP12()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm = res_mlp.ResMLP12(input_shape=(112, 112, 3), pretrained=\"imagenet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import wave_mlp\nmm = wave_mlp.WaveMLP_T(input_shape=(112, 112, 3), pretrained=\"imagenet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import mobilevit\nmm = mobilevit.MobileViTBasePatch16(input_shape=(112, 112, 3))\nmm2 = mobilevit.MobileViTBasePatch16(input_shape=(112, 112, 3))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mm.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import swin_transformer_v2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm2 = swin_transformer_v2.SwinTransformerV2Tiny_window8(input_shape=(112, 112, 3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mm2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ntransfer_layer = mm.get_layer('avg_pool')\nconv_model = Model(inputs=mm.input, outputs=transfer_layer.output)\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n#for layer in conv_model.layers:\n#    layer.trainable = False\n    \n# Start a new Keras Sequential model.\nnew_model = Sequential()\n\n# Add the convolutional part of the VGG16 model from above.\nnew_model.add(conv_model)\n\n\n# Add the final layer for the actual classification.\nnew_model.add(Dense(2, activation='softmax'))\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import beit\nmm2 = beit.BeitBasePatch16(input_shape=(112, 112, 3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm_last_layer = mm.get_layer('avg_pool').output\n#out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\n#out = Dense(11, activation='softmax', name='prediction1')(out)\nmm_custom = Model(mm.input, mm_last_layer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm2_last_layer = mm2.get_layer('out_ln').output\n#out2 = Dense(256, activation='relu', name='dense_1')(mm2_last_layer)\n#out2 = Dense(11, activation='softmax', name='prediction1')(out2)\nmm2_custom = Model(mm2.input, mm2_last_layer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a custom linear attention layer\nclass LinearAttentionLayer(keras.layers.Layer):\n    def __init__(self, units, **kwargs):\n        super(LinearAttentionLayer, self).__init__(**kwargs)\n        self.units = units\n\n    def build(self, input_shape):\n        self.W_q = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n        self.W_k = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n        self.W_v = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n\n    def call(self, inputs):\n        Q = tf.matmul(inputs, self.W_q)\n        K = tf.matmul(inputs, self.W_k)\n        V = tf.matmul(inputs, self.W_v)\n\n        attn_scores = tf.matmul(Q, K, transpose_b=True)\n        attn_scores = tf.nn.softmax(attn_scores / tf.math.sqrt(tf.cast(self.units, tf.float32)), axis=-1)\n        output = tf.matmul(attn_scores, V)\n\n        return output\n\n# ... Continue with your code ...\n\n# Add the attention layer where needed in your model\nnum_classes = 2\navg_ensemble_model_last_layer = avg_ensemble_model.get_layer('average').output\n\n# Add Linear Attention Layer here (for example, just before the output layer)\nattention_output = LinearAttentionLayer(64)(avg_ensemble_model_last_layer)\n\noutput_layer = Dense(num_classes, activation='softmax', name='output_1')(attention_output)\nfinal_model = Model(avg_ensemble_model.input, output_layer)\n\nfinal_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\ninputs = keras.Input(shape=(112,112,3))\noutputs = layers.average([mm_custom(inputs), mm2_custom(inputs)])\n\navg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\navg_ensemble_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\ninputs = keras.Input(shape=(112,112,3))\noutputs = layers.average([mm_custom(inputs)])\n\navg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\navg_ensemble_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T22:04:01.427751Z","iopub.execute_input":"2023-12-09T22:04:01.428153Z","iopub.status.idle":"2023-12-09T22:04:01.468384Z","shell.execute_reply.started":"2023-12-09T22:04:01.428111Z","shell.execute_reply":"2023-12-09T22:04:01.467037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 26\navg_ensemble_model_last_layer = avg_ensemble_model.get_layer('average_1').output\noutput_layer = Dense(num_classes, activation='softmax', name='output_1')(avg_ensemble_model_last_layer)\nfinal_model = Model(avg_ensemble_model.input, output_layer)\n\nfinal_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T08:03:57.618364Z","iopub.execute_input":"2023-12-10T08:03:57.619160Z","iopub.status.idle":"2023-12-10T08:03:57.685820Z","shell.execute_reply.started":"2023-12-10T08:03:57.619122Z","shell.execute_reply":"2023-12-10T08:03:57.684904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(lr=1e-5)\nloss = 'categorical_crossentropy'\n# metrics = ['categorical_accuracy']\nmetrics = ['accuracy', 'categorical_accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), \n           tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), \n           tf.keras.metrics.FalseNegatives(), tfa.metrics.CohenKappa(num_classes = num_classes), \n           tfa.metrics.F1Score(num_classes = num_classes)]\n\nfinal_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T08:04:02.439530Z","iopub.execute_input":"2023-12-10T08:04:02.439921Z","iopub.status.idle":"2023-12-10T08:04:02.505447Z","shell.execute_reply.started":"2023-12-10T08:04:02.439888Z","shell.execute_reply":"2023-12-10T08:04:02.504704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Delete the existing HDF5 file if it exists\nif os.path.exists('Best_DenseNet201.h5'):\n    os.remove('Best_DenseNet201.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nlr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1,\n    patience=9, mode=\"max\", min_delta=0.0001, min_lr=0.00001, verbose=1)\ncheckpoint = ModelCheckpoint(filepath='Best_DenseNet201_v23.h5', save_best_only=True, monitor = 'val_accuracy', verbose=1)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, restore_best_weights=True)\n\ncallbacks = [lr, checkpoint, early_stopping]","metadata":{"execution":{"iopub.status.busy":"2023-12-10T08:04:17.498729Z","iopub.execute_input":"2023-12-10T08:04:17.499620Z","iopub.status.idle":"2023-12-10T08:04:17.505813Z","shell.execute_reply.started":"2023-12-10T08:04:17.499569Z","shell.execute_reply":"2023-12-10T08:04:17.504901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 30\n\nsteps_per_epoch = generator_train.n / batch_size\nsteps_test = generator_test.n / batch_size\n\nhistory = final_model.fit_generator(generator=generator_train,\n                                  epochs=epochs,\n                                  steps_per_epoch=steps_per_epoch,\n                                  validation_data=generator_test,\n                                  validation_steps=steps_test,\n                                   callbacks=callbacks, class_weight =class_weights)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T08:04:25.366376Z","iopub.execute_input":"2023-12-10T08:04:25.366742Z","iopub.status.idle":"2023-12-10T08:36:02.988272Z","shell.execute_reply.started":"2023-12-10T08:04:25.366711Z","shell.execute_reply":"2023-12-10T08:36:02.986623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_(final_model, generator_test)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T21:13:26.977420Z","iopub.execute_input":"2023-11-16T21:13:26.977709Z","iopub.status.idle":"2023-11-16T21:14:03.780954Z","shell.execute_reply.started":"2023-11-16T21:13:26.977682Z","shell.execute_reply":"2023-11-16T21:14:03.780023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['categorical_accuracy'])\nplt.plot(history.history['val_categorical_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T21:14:03.782196Z","iopub.execute_input":"2023-11-16T21:14:03.782492Z","iopub.status.idle":"2023-11-16T21:14:04.092807Z","shell.execute_reply.started":"2023-11-16T21:14:03.782466Z","shell.execute_reply":"2023-11-16T21:14:04.091925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T21:14:04.094424Z","iopub.execute_input":"2023-11-16T21:14:04.094718Z","iopub.status.idle":"2023-11-16T21:14:04.315164Z","shell.execute_reply.started":"2023-11-16T21:14:04.094692Z","shell.execute_reply":"2023-11-16T21:14:04.314425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade scipy scikit-image\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_size = 128\nbatch_size = 8\n\ntrain_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\nval_dir = r\"/kaggle/input/mango-leaf/mango-prepo/val\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\n\n\ndatagen_train = ImageDataGenerator(rescale=1./255, width_shift_range=0.1, height_shift_range=0.1,\n                                  horizontal_flip=True,  vertical_flip=False)\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = datagen_train.flow_from_directory(directory=train_dir, target_size=(image_size, image_size),\n                                                    batch_size=batch_size, shuffle=True)\nval_generator = datagen_test.flow_from_directory(directory=val_dir, target_size=(image_size, image_size),\n                                                  batch_size=batch_size, shuffle=False)\ntest_generator = datagen_test.flow_from_directory(directory=test_dir, target_size=(image_size, image_size),\n                                                  batch_size=batch_size, shuffle=False)\n\n#Define the number of classes in your dataset\nnum_classes = train_generator.num_classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer, Attention\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LinearAttention(Layer):\n    def __init__(self, units):\n        super(LinearAttention, self).__init__()\n        self.units = units\n\n    def build(self, input_shape):\n        self.W_q = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n        self.W_k = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n        self.W_v = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n\n    def call(self, inputs):\n        Q = tf.matmul(inputs, self.W_q)\n        K = tf.matmul(inputs, self.W_k)\n        V = tf.matmul(inputs, self.W_v)\n\n        attn_scores = tf.matmul(Q, K, transpose_b=True)\n        attn_scores = tf.nn.softmax(attn_scores / tf.math.sqrt(tf.cast(self.units, tf.float32)), axis=-1)\n        output = tf.matmul(attn_scores, V)\n\n        return output\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define your model function with attention\ndef modelfunction_with_attention(base):\n    x = base.output\n\n    # Add Self-Attention Layer\n    att_output = LinearAttention(128)(x)\n\n    # Add more layers if needed\n    x = tf.keras.layers.GlobalAveragePooling2D()(att_output)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    predictions = tf.keras.layers.Dense(units=num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.02, l2=0.02))(x)\n    model = Model(inputs=base.input, outputs=predictions)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def modelfunction(base):\n    x = base.output\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    predictions = tf.keras.layers.Dense(units=num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.02, l2=0.02))(x)\n    model = Model(inputs=base.input, outputs=predictions)\n    return model\n\ndef get_callbacks(weight):\n    checkpoint = ModelCheckpoint(weight, monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n    learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=5, verbose=1, factor=0.2, min_lr=0.0002)\n    callbacks = [checkpoint, learning_rate_reduction]\n    return callbacks\n\ndef evaluate(model, generator_test):\n    model.evaluate(generator_test)\n\n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n\n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()\n\ndef model_training(base, weight, epochs):\n    model = modelfunction(base)\n    print(\"\\n\\n\\n-------------------- Model Initialized --------------------\")\n\n    callbacks = get_callbacks(weight)\n    metrics = ['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(),\n               tfa.metrics.CohenKappa(num_classes=num_classes), tfa.metrics.F1Score(num_classes=num_classes)]\n    model.compile(tf.keras.optimizers.Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=metrics)\n\n    history = model.fit(train_generator, steps_per_epoch=366 // batch_size,\n                        validation_data=val_generator,  # Add this line\n                        epochs=epochs, callbacks=callbacks)\n    # Plotting accuracy and loss curves\n    plt.figure(figsize=(12, 4))\n\n    # Plot accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='accuracy')\n    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Accuracy Over Epochs')\n    plt.legend()\n\n    # Plot loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Loss Over Epochs')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n\\n\\n-------------------- Evaluation --------------------\")\n    evaluate(model, val_generator)\n\n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create and train the model with attention\nVGG19 = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_tensor=None, input_shape=None)\nVGG19_model_with_attention = model_training(VGG19, 'VGG19_with_attention.h5', 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lime\nfrom lime import lime_image\nfrom skimage.segmentation import mark_boundaries\nimport matplotlib.pyplot as plt\nimport random\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lime import lime_image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skimage.segmentation import mark_boundaries","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6762217,"sourceType":"datasetVersion","datasetId":3891934}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:40:13.094665Z","iopub.execute_input":"2024-01-03T06:40:13.095129Z","iopub.status.idle":"2024-01-03T06:40:28.426936Z","shell.execute_reply.started":"2024-01-03T06:40:13.095103Z","shell.execute_reply":"2024-01-03T06:40:28.425989Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Device mapping:\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def evaluate_(model, generator_test):\n    model.evaluate(generator_test)\n    \n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n    \n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:41:03.017376Z","iopub.execute_input":"2024-01-03T06:41:03.018055Z","iopub.status.idle":"2024-01-03T06:41:03.024960Z","shell.execute_reply.started":"2024-01-03T06:41:03.018021Z","shell.execute_reply":"2024-01-03T06:41:03.024021Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef evaluate_(model, generator_test):\n    model.evaluate(generator_test)\n    \n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n    \n    # Plotting the confusion matrix\n    plt.figure(figsize=(12, 4))\n    \n    plt.subplot(1, 2, 1)\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.title('Confusion Matrix')\n    \n    # ROC curve\n    plt.subplot(1, 2, 2)\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n\n    for i in range(len(class_labels)):\n        fpr[i], tpr[i], _ = roc_curve(y_true == i, y_pred[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    for i in range(len(class_labels)):\n        plt.plot(fpr[i], tpr[i], label=f'{class_labels[i]} (AUC = {roc_auc[i]:.2f})')\n\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc=\"lower right\")\n\n    plt.tight_layout()\n    plt.show()\n\n# Call the function with your model and test generator\n","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:41:05.994424Z","iopub.execute_input":"2024-01-03T06:41:05.994802Z","iopub.status.idle":"2024-01-03T06:41:06.006690Z","shell.execute_reply.started":"2024-01-03T06:41:05.994765Z","shell.execute_reply":"2024-01-03T06:41:06.005697Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\ndatagen_train = ImageDataGenerator(rescale=1./255,\n                                  width_shift_range=0.1,\n                                  height_shift_range=0.1,\n                                  horizontal_flip=True,\n                                  vertical_flip=False)\n\n\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\n\nbatch_size = 16\ngenerator_train = datagen_train.flow_from_directory(directory=train_dir,\n                                                    target_size=(112, 112),\n                                                    batch_size=batch_size,\n                                                    shuffle=True)\n\ngenerator_test = datagen_test.flow_from_directory(directory=test_dir,\n                                                  target_size=(112, 112),\n                                                  batch_size=batch_size,\n                                                  shuffle=False)\n# Calculate class weights\nlabels = generator_train.classes\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\nclass_weights = dict(zip(np.unique(labels), class_weights))\nprint(class_weights)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:41:10.124991Z","iopub.execute_input":"2024-01-03T06:41:10.125711Z","iopub.status.idle":"2024-01-03T06:41:12.439087Z","shell.execute_reply.started":"2024-01-03T06:41:10.125681Z","shell.execute_reply":"2024-01-03T06:41:12.438196Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Found 13307 images belonging to 26 classes.\nFound 1684 images belonging to 26 classes.\n{0: 1.0640492563569486, 1: 1.0662660256410257, 2: 1.0662660256410257, 3: 1.0402595372107568, 4: 1.2273565762774397, 5: 1.0381494772975504, 6: 1.053102247546692, 7: 1.2795192307692307, 8: 1.0487862547288778, 9: 1.053102247546692, 10: 0.6486789509603198, 11: 1.0662660256410257, 12: 1.4623076923076923, 13: 0.8809082483781279, 14: 0.7571119708693673, 15: 1.0466414975617429, 16: 1.7527660695468914, 17: 0.5415954415954416, 18: 1.0662660256410257, 19: 0.9042538733351454, 20: 0.789826685660019, 21: 1.5462468045549616, 22: 1.0256667180514876, 23: 1.8085077466702908, 24: 1.0445054945054946, 25: 0.7259683578832515}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras_cv_attention_models ","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:41:29.853266Z","iopub.execute_input":"2024-01-03T06:41:29.853612Z","iopub.status.idle":"2024-01-03T06:41:44.744970Z","shell.execute_reply.started":"2024-01-03T06:41:29.853588Z","shell.execute_reply":"2024-01-03T06:41:44.743848Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting keras_cv_attention_models\n  Downloading keras_cv_attention_models-1.3.24-py3-none-any.whl (787 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m787.5/787.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (9.5.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (4.66.1)\nCollecting ftfy (from keras_cv_attention_models)\n  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (2023.6.3)\nRequirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (4.9.2)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (2.12.0)\nCollecting wcwidth<0.3.0,>=0.2.12 (from ftfy->keras_cv_attention_models)\n  Downloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (3.9.0)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (0.4.13)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (16.0.0)\nRequirement already satisfied: numpy<1.24,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.23.5)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (68.0.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.16.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (4.6.3)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (0.32.0)\nRequirement already satisfied: array-record in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (0.4.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (8.1.7)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (0.1.8)\nRequirement already satisfied: etils[enp,epath]>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (1.3.0)\nRequirement already satisfied: promise in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (2.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (5.9.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (2.31.0)\nRequirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (0.14.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (0.10.2)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow->keras_cv_attention_models) (0.40.0)\nRequirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->keras_cv_attention_models) (5.12.0)\nRequirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->keras_cv_attention_models) (3.15.0)\nRequirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow->keras_cv_attention_models) (0.2.0)\nRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow->keras_cv_attention_models) (1.11.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (2023.7.22)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (3.4.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (2.3.7)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow->keras_cv_attention_models) (3.0.9)\nRequirement already satisfied: googleapis-common-protos in /opt/conda/lib/python3.10/site-packages (from tensorflow-metadata->tensorflow-datasets->keras_cv_attention_models) (1.59.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (3.2.2)\nInstalling collected packages: wcwidth, ftfy, keras_cv_attention_models\n  Attempting uninstall: wcwidth\n    Found existing installation: wcwidth 0.2.6\n    Uninstalling wcwidth-0.2.6:\n      Successfully uninstalled wcwidth-0.2.6\nSuccessfully installed ftfy-6.1.3 keras_cv_attention_models-1.3.24 wcwidth-0.2.12\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras_cv_attention_models import wave_mlp","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:41:48.015462Z","iopub.execute_input":"2024-01-03T06:41:48.015908Z","iopub.status.idle":"2024-01-03T06:41:48.093323Z","shell.execute_reply.started":"2024-01-03T06:41:48.015870Z","shell.execute_reply":"2024-01-03T06:41:48.092392Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import coatnet","metadata":{"execution":{"iopub.status.busy":"2024-01-02T22:13:31.350341Z","iopub.execute_input":"2024-01-02T22:13:31.350718Z","iopub.status.idle":"2024-01-02T22:13:31.355220Z","shell.execute_reply.started":"2024-01-02T22:13:31.350686Z","shell.execute_reply":"2024-01-02T22:13:31.354213Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(256, activation='relu')(x)  # Add your own dense layers\n    x = Dropout(0.5)(x)  # Add dropout for regularization\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\n\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = coatnet.CoAtNet0(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n#     head_output = custom_head(mm_headless.output, num_classes)\n    print(\"mm_headless.output shape:\", mm_headless.output_shape)\n    head_output = custom_head(mm_headless.output, num_classes)\n\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\n# custom_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-02T22:11:59.489705Z","iopub.execute_input":"2024-01-02T22:11:59.490526Z","iopub.status.idle":"2024-01-02T22:12:00.307998Z","shell.execute_reply.started":"2024-01-02T22:11:59.490462Z","shell.execute_reply":"2024-01-02T22:12:00.306522Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     27\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m26\u001b[39m  \u001b[38;5;66;03m# Adjust based on your task\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m custom_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodify_wave_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# custom_model.summary()\u001b[39;00m\n","Cell \u001b[0;32mIn[7], line 12\u001b[0m, in \u001b[0;36mmodify_wave_mlp\u001b[0;34m(input_shape, num_classes)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodify_wave_mlp\u001b[39m(input_shape, num_classes):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Load the WaveMLP model without the top layers (head)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     mm_headless \u001b[38;5;241m=\u001b[39m \u001b[43mcoatnet\u001b[49m\u001b[38;5;241m.\u001b[39mCoAtNet0(input_shape\u001b[38;5;241m=\u001b[39minput_shape, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Add your custom head\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     head_output = custom_head(mm_headless.output, num_classes)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmm_headless.output shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mm_headless\u001b[38;5;241m.\u001b[39moutput_shape)\n","\u001b[0;31mNameError\u001b[0m: name 'coatnet' is not defined"],"ename":"NameError","evalue":"name 'coatnet' is not defined","output_type":"error"}]},{"cell_type":"code","source":"def custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = coatnet.CoAtNet0(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n    head_output = custom_head(mm_headless.output, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-100]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:41:55.877170Z","iopub.execute_input":"2024-01-03T06:41:55.877510Z","iopub.status.idle":"2024-01-03T06:41:56.666981Z","shell.execute_reply.started":"2024-01-03T06:41:55.877483Z","shell.execute_reply":"2024-01-03T06:41:56.665758Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     31\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m26\u001b[39m  \u001b[38;5;66;03m# Adjust based on your task\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m custom_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodify_wave_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m custom_model\u001b[38;5;241m.\u001b[39msummary()\n","Cell \u001b[0;32mIn[7], line 12\u001b[0m, in \u001b[0;36mmodify_wave_mlp\u001b[0;34m(input_shape, num_classes)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodify_wave_mlp\u001b[39m(input_shape, num_classes):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Load the WaveMLP model without the top layers (head)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     mm_headless \u001b[38;5;241m=\u001b[39m \u001b[43mcoatnet\u001b[49m\u001b[38;5;241m.\u001b[39mCoAtNet0(input_shape\u001b[38;5;241m=\u001b[39minput_shape, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Add your custom head\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     head_output \u001b[38;5;241m=\u001b[39m custom_head(mm_headless\u001b[38;5;241m.\u001b[39moutput, num_classes)\n","\u001b[0;31mNameError\u001b[0m: name 'coatnet' is not defined"],"ename":"NameError","evalue":"name 'coatnet' is not defined","output_type":"error"}]},{"cell_type":"code","source":"def custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n    head_output = custom_head(mm_headless.output, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\n# custom_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:42:04.944314Z","iopub.execute_input":"2024-01-03T06:42:04.944691Z","iopub.status.idle":"2024-01-03T06:42:09.593465Z","shell.execute_reply.started":"2024-01-03T06:42:04.944663Z","shell.execute_reply":"2024-01-03T06:42:09.592682Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Downloading data from https://github.com/leondgarse/keras_cv_attention_models/releases/download/mlp_family/wavemlp_t_imagenet.h5\n69906376/69906376 [==============================] - 0s 0us/step\n>>>> Load pretrained from: /root/.keras/models/wavemlp_t_imagenet.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"mm_last_layer = custom_model .get_layer('avg_pool').output\n#out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\n#out = Dense(11, activation='softmax', name='prediction1')(out)\nmm_custom = Model(custom_model .input, mm_last_layer)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:42:13.412832Z","iopub.execute_input":"2024-01-03T06:42:13.413811Z","iopub.status.idle":"2024-01-03T06:42:13.450571Z","shell.execute_reply.started":"2024-01-03T06:42:13.413770Z","shell.execute_reply":"2024-01-03T06:42:13.449766Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\ninputs = keras.Input(shape=(112,112,3))\noutputs = layers.average([mm_custom(inputs)])\n\navg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\navg_ensemble_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:42:16.299109Z","iopub.execute_input":"2024-01-03T06:42:16.299928Z","iopub.status.idle":"2024-01-03T06:42:17.296577Z","shell.execute_reply.started":"2024-01-03T06:42:16.299895Z","shell.execute_reply":"2024-01-03T06:42:17.295683Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Model: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 112, 112, 3)]     0         \n                                                                 \n model_1 (Functional)        (None, 512)               16704736  \n                                                                 \n average (Average)           (None, 512)               0         \n                                                                 \n=================================================================\nTotal params: 16,704,736\nTrainable params: 16,680,160\nNon-trainable params: 24,576\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 26\navg_ensemble_model_last_layer = avg_ensemble_model.get_layer('average').output\noutput_layer = Dense(num_classes, activation='softmax', name='output_1')(avg_ensemble_model_last_layer)\nfinal_model = Model(avg_ensemble_model.input, output_layer)\n\nfinal_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:42:19.625840Z","iopub.execute_input":"2024-01-03T06:42:19.626259Z","iopub.status.idle":"2024-01-03T06:42:19.683178Z","shell.execute_reply.started":"2024-01-03T06:42:19.626230Z","shell.execute_reply":"2024-01-03T06:42:19.682237Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Model: \"model_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 112, 112, 3)]     0         \n                                                                 \n model_1 (Functional)        (None, 512)               16704736  \n                                                                 \n average (Average)           (None, 512)               0         \n                                                                 \n output_1 (Dense)            (None, 26)                13338     \n                                                                 \n=================================================================\nTotal params: 16,718,074\nTrainable params: 16,693,498\nNon-trainable params: 24,576\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = Adam(lr=1e-5)\nloss = 'categorical_crossentropy'\n# metrics = ['categorical_accuracy']\nmetrics = ['accuracy', 'categorical_accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), \n           tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), \n           tf.keras.metrics.FalseNegatives(), tfa.metrics.CohenKappa(num_classes = num_classes), \n           tfa.metrics.F1Score(num_classes = num_classes)]\n\nfinal_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:42:28.605386Z","iopub.execute_input":"2024-01-03T06:42:28.605749Z","iopub.status.idle":"2024-01-03T06:42:28.662557Z","shell.execute_reply.started":"2024-01-03T06:42:28.605719Z","shell.execute_reply":"2024-01-03T06:42:28.661834Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nlr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1,\n    patience=9, mode=\"max\", min_delta=0.0001, min_lr=0.00001, verbose=1)\ncheckpoint = ModelCheckpoint(filepath='Best_DenseNet201_v23.h5', save_best_only=True, monitor = 'val_accuracy', verbose=1)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, restore_best_weights=True)\n\ncallbacks = [lr, checkpoint, early_stopping]","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:42:33.674265Z","iopub.execute_input":"2024-01-03T06:42:33.674636Z","iopub.status.idle":"2024-01-03T06:42:33.681227Z","shell.execute_reply.started":"2024-01-03T06:42:33.674606Z","shell.execute_reply":"2024-01-03T06:42:33.680258Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"epochs = 30\n\nsteps_per_epoch = generator_train.n / batch_size\nsteps_test = generator_test.n / batch_size\n\nhistory = final_model.fit_generator(generator=generator_train,\n                                  epochs=epochs,\n                                  steps_per_epoch=steps_per_epoch,\n                                  validation_data=generator_test,\n                                  validation_steps=steps_test,\n                                   callbacks=callbacks, class_weight =class_weights)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:42:41.683283Z","iopub.execute_input":"2024-01-03T06:42:41.683908Z","iopub.status.idle":"2024-01-03T08:34:22.229560Z","shell.execute_reply.started":"2024-01-03T06:42:41.683878Z","shell.execute_reply":"2024-01-03T08:34:22.228746Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/2027776823.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n  history = final_model.fit_generator(generator=generator_train,\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30\n832/831 [==============================] - ETA: 0s - loss: 1.4523 - accuracy: 0.5607 - categorical_accuracy: 0.5607 - auc: 0.9440 - precision: 0.7564 - recall: 0.4106 - true_positives: 5464.0000 - true_negatives: 330915.0000 - false_positives: 1760.0000 - false_negatives: 7843.0000 - cohen_kappa: 0.5424 - f1_score: 0.5549\nEpoch 1: val_accuracy improved from -inf to 0.32779, saving model to Best_DenseNet201_v23.h5\n831/831 [==============================] - 386s 376ms/step - loss: 1.4523 - accuracy: 0.5607 - categorical_accuracy: 0.5607 - auc: 0.9440 - precision: 0.7564 - recall: 0.4106 - true_positives: 5464.0000 - true_negatives: 330915.0000 - false_positives: 1760.0000 - false_negatives: 7843.0000 - cohen_kappa: 0.5424 - f1_score: 0.5549 - val_loss: 4.3657 - val_accuracy: 0.3278 - val_categorical_accuracy: 0.3278 - val_auc: 0.7351 - val_precision: 0.3592 - val_recall: 0.2910 - val_true_positives: 490.0000 - val_true_negatives: 41226.0000 - val_false_positives: 874.0000 - val_false_negatives: 1194.0000 - val_cohen_kappa: 0.3022 - val_f1_score: 0.2994 - lr: 0.0010\nEpoch 2/30\n832/831 [==============================] - ETA: 0s - loss: 0.7492 - accuracy: 0.7586 - categorical_accuracy: 0.7586 - auc: 0.9823 - precision: 0.8398 - recall: 0.6873 - true_positives: 9146.0000 - true_negatives: 330930.0000 - false_positives: 1745.0000 - false_negatives: 4161.0000 - cohen_kappa: 0.7484 - f1_score: 0.7576\nEpoch 2: val_accuracy improved from 0.32779 to 0.63777, saving model to Best_DenseNet201_v23.h5\n831/831 [==============================] - 220s 265ms/step - loss: 0.7492 - accuracy: 0.7586 - categorical_accuracy: 0.7586 - auc: 0.9823 - precision: 0.8398 - recall: 0.6873 - true_positives: 9146.0000 - true_negatives: 330930.0000 - false_positives: 1745.0000 - false_negatives: 4161.0000 - cohen_kappa: 0.7484 - f1_score: 0.7576 - val_loss: 1.1690 - val_accuracy: 0.6378 - val_categorical_accuracy: 0.6378 - val_auc: 0.9645 - val_precision: 0.7326 - val_recall: 0.5742 - val_true_positives: 967.0000 - val_true_negatives: 41747.0000 - val_false_positives: 353.0000 - val_false_negatives: 717.0000 - val_cohen_kappa: 0.6237 - val_f1_score: 0.6617 - lr: 0.0010\nEpoch 3/30\n832/831 [==============================] - ETA: 0s - loss: 0.5570 - accuracy: 0.8187 - categorical_accuracy: 0.8187 - auc: 0.9884 - precision: 0.8693 - recall: 0.7660 - true_positives: 10193.0000 - true_negatives: 331143.0000 - false_positives: 1532.0000 - false_negatives: 3114.0000 - cohen_kappa: 0.8110 - f1_score: 0.8176\nEpoch 3: val_accuracy improved from 0.63777 to 0.80819, saving model to Best_DenseNet201_v23.h5\n831/831 [==============================] - 227s 272ms/step - loss: 0.5570 - accuracy: 0.8187 - categorical_accuracy: 0.8187 - auc: 0.9884 - precision: 0.8693 - recall: 0.7660 - true_positives: 10193.0000 - true_negatives: 331143.0000 - false_positives: 1532.0000 - false_negatives: 3114.0000 - cohen_kappa: 0.8110 - f1_score: 0.8176 - val_loss: 0.6068 - val_accuracy: 0.8082 - val_categorical_accuracy: 0.8082 - val_auc: 0.9864 - val_precision: 0.8479 - val_recall: 0.7714 - val_true_positives: 1299.0000 - val_true_negatives: 41867.0000 - val_false_positives: 233.0000 - val_false_negatives: 385.0000 - val_cohen_kappa: 0.8003 - val_f1_score: 0.8073 - lr: 0.0010\nEpoch 4/30\n832/831 [==============================] - ETA: 0s - loss: 0.4618 - accuracy: 0.8420 - categorical_accuracy: 0.8420 - auc: 0.9917 - precision: 0.8833 - recall: 0.8074 - true_positives: 10744.0000 - true_negatives: 331255.0000 - false_positives: 1420.0000 - false_negatives: 2563.0000 - cohen_kappa: 0.8352 - f1_score: 0.8409\nEpoch 4: val_accuracy did not improve from 0.80819\n831/831 [==============================] - 220s 264ms/step - loss: 0.4618 - accuracy: 0.8420 - categorical_accuracy: 0.8420 - auc: 0.9917 - precision: 0.8833 - recall: 0.8074 - true_positives: 10744.0000 - true_negatives: 331255.0000 - false_positives: 1420.0000 - false_negatives: 2563.0000 - cohen_kappa: 0.8352 - f1_score: 0.8409 - val_loss: 0.8992 - val_accuracy: 0.7387 - val_categorical_accuracy: 0.7387 - val_auc: 0.9735 - val_precision: 0.8335 - val_recall: 0.6835 - val_true_positives: 1151.0000 - val_true_negatives: 41870.0000 - val_false_positives: 230.0000 - val_false_negatives: 533.0000 - val_cohen_kappa: 0.7277 - val_f1_score: 0.7248 - lr: 0.0010\nEpoch 5/30\n832/831 [==============================] - ETA: 0s - loss: 0.3945 - accuracy: 0.8623 - categorical_accuracy: 0.8623 - auc: 0.9936 - precision: 0.8948 - recall: 0.8342 - true_positives: 11101.0000 - true_negatives: 331370.0000 - false_positives: 1305.0000 - false_negatives: 2206.0000 - cohen_kappa: 0.8564 - f1_score: 0.8608\nEpoch 5: val_accuracy improved from 0.80819 to 0.85689, saving model to Best_DenseNet201_v23.h5\n831/831 [==============================] - 218s 263ms/step - loss: 0.3945 - accuracy: 0.8623 - categorical_accuracy: 0.8623 - auc: 0.9936 - precision: 0.8948 - recall: 0.8342 - true_positives: 11101.0000 - true_negatives: 331370.0000 - false_positives: 1305.0000 - false_negatives: 2206.0000 - cohen_kappa: 0.8564 - f1_score: 0.8608 - val_loss: 0.4265 - val_accuracy: 0.8569 - val_categorical_accuracy: 0.8569 - val_auc: 0.9922 - val_precision: 0.8828 - val_recall: 0.8361 - val_true_positives: 1408.0000 - val_true_negatives: 41913.0000 - val_false_positives: 187.0000 - val_false_negatives: 276.0000 - val_cohen_kappa: 0.8507 - val_f1_score: 0.8439 - lr: 0.0010\nEpoch 6/30\n832/831 [==============================] - ETA: 0s - loss: 0.3575 - accuracy: 0.8740 - categorical_accuracy: 0.8740 - auc: 0.9942 - precision: 0.9032 - recall: 0.8524 - true_positives: 11343.0000 - true_negatives: 331459.0000 - false_positives: 1216.0000 - false_negatives: 1964.0000 - cohen_kappa: 0.8686 - f1_score: 0.8727\nEpoch 6: val_accuracy did not improve from 0.85689\n831/831 [==============================] - 220s 264ms/step - loss: 0.3575 - accuracy: 0.8740 - categorical_accuracy: 0.8740 - auc: 0.9942 - precision: 0.9032 - recall: 0.8524 - true_positives: 11343.0000 - true_negatives: 331459.0000 - false_positives: 1216.0000 - false_negatives: 1964.0000 - cohen_kappa: 0.8686 - f1_score: 0.8727 - val_loss: 1.2203 - val_accuracy: 0.6728 - val_categorical_accuracy: 0.6728 - val_auc: 0.9555 - val_precision: 0.7377 - val_recall: 0.6164 - val_true_positives: 1038.0000 - val_true_negatives: 41731.0000 - val_false_positives: 369.0000 - val_false_negatives: 646.0000 - val_cohen_kappa: 0.6594 - val_f1_score: 0.6563 - lr: 0.0010\nEpoch 7/30\n832/831 [==============================] - ETA: 0s - loss: 0.3319 - accuracy: 0.8858 - categorical_accuracy: 0.8858 - auc: 0.9948 - precision: 0.9103 - recall: 0.8623 - true_positives: 11474.0000 - true_negatives: 331544.0000 - false_positives: 1131.0000 - false_negatives: 1833.0000 - cohen_kappa: 0.8809 - f1_score: 0.8842\nEpoch 7: val_accuracy did not improve from 0.85689\n831/831 [==============================] - 220s 264ms/step - loss: 0.3319 - accuracy: 0.8858 - categorical_accuracy: 0.8858 - auc: 0.9948 - precision: 0.9103 - recall: 0.8623 - true_positives: 11474.0000 - true_negatives: 331544.0000 - false_positives: 1131.0000 - false_negatives: 1833.0000 - cohen_kappa: 0.8809 - f1_score: 0.8842 - val_loss: 0.4914 - val_accuracy: 0.8444 - val_categorical_accuracy: 0.8444 - val_auc: 0.9903 - val_precision: 0.8825 - val_recall: 0.8254 - val_true_positives: 1390.0000 - val_true_negatives: 41915.0000 - val_false_positives: 185.0000 - val_false_negatives: 294.0000 - val_cohen_kappa: 0.8379 - val_f1_score: 0.8485 - lr: 0.0010\nEpoch 8/30\n832/831 [==============================] - ETA: 0s - loss: 0.2925 - accuracy: 0.9004 - categorical_accuracy: 0.9004 - auc: 0.9960 - precision: 0.9197 - recall: 0.8816 - true_positives: 11732.0000 - true_negatives: 331650.0000 - false_positives: 1025.0000 - false_negatives: 1575.0000 - cohen_kappa: 0.8961 - f1_score: 0.8983\nEpoch 8: val_accuracy did not improve from 0.85689\n831/831 [==============================] - 209s 252ms/step - loss: 0.2925 - accuracy: 0.9004 - categorical_accuracy: 0.9004 - auc: 0.9960 - precision: 0.9197 - recall: 0.8816 - true_positives: 11732.0000 - true_negatives: 331650.0000 - false_positives: 1025.0000 - false_negatives: 1575.0000 - cohen_kappa: 0.8961 - f1_score: 0.8983 - val_loss: 5.9983 - val_accuracy: 0.2963 - val_categorical_accuracy: 0.2963 - val_auc: 0.6973 - val_precision: 0.3008 - val_recall: 0.2856 - val_true_positives: 481.0000 - val_true_negatives: 40982.0000 - val_false_positives: 1118.0000 - val_false_negatives: 1203.0000 - val_cohen_kappa: 0.2694 - val_f1_score: 0.3088 - lr: 0.0010\nEpoch 9/30\n832/831 [==============================] - ETA: 0s - loss: 0.2788 - accuracy: 0.9004 - categorical_accuracy: 0.9004 - auc: 0.9959 - precision: 0.9188 - recall: 0.8849 - true_positives: 11775.0000 - true_negatives: 331635.0000 - false_positives: 1040.0000 - false_negatives: 1532.0000 - cohen_kappa: 0.8961 - f1_score: 0.9001\nEpoch 9: val_accuracy did not improve from 0.85689\n831/831 [==============================] - 217s 260ms/step - loss: 0.2788 - accuracy: 0.9004 - categorical_accuracy: 0.9004 - auc: 0.9959 - precision: 0.9188 - recall: 0.8849 - true_positives: 11775.0000 - true_negatives: 331635.0000 - false_positives: 1040.0000 - false_negatives: 1532.0000 - cohen_kappa: 0.8961 - f1_score: 0.9001 - val_loss: 0.6425 - val_accuracy: 0.8058 - val_categorical_accuracy: 0.8058 - val_auc: 0.9831 - val_precision: 0.8452 - val_recall: 0.7779 - val_true_positives: 1310.0000 - val_true_negatives: 41860.0000 - val_false_positives: 240.0000 - val_false_negatives: 374.0000 - val_cohen_kappa: 0.7979 - val_f1_score: 0.8093 - lr: 0.0010\nEpoch 10/30\n832/831 [==============================] - ETA: 0s - loss: 0.2382 - accuracy: 0.9155 - categorical_accuracy: 0.9155 - auc: 0.9969 - precision: 0.9318 - recall: 0.9016 - true_positives: 11998.0000 - true_negatives: 331797.0000 - false_positives: 878.0000 - false_negatives: 1309.0000 - cohen_kappa: 0.9119 - f1_score: 0.9137\nEpoch 10: val_accuracy did not improve from 0.85689\n831/831 [==============================] - 210s 252ms/step - loss: 0.2382 - accuracy: 0.9155 - categorical_accuracy: 0.9155 - auc: 0.9969 - precision: 0.9318 - recall: 0.9016 - true_positives: 11998.0000 - true_negatives: 331797.0000 - false_positives: 878.0000 - false_negatives: 1309.0000 - cohen_kappa: 0.9119 - f1_score: 0.9137 - val_loss: 2.2302 - val_accuracy: 0.5285 - val_categorical_accuracy: 0.5285 - val_auc: 0.8916 - val_precision: 0.5820 - val_recall: 0.4869 - val_true_positives: 820.0000 - val_true_negatives: 41511.0000 - val_false_positives: 589.0000 - val_false_negatives: 864.0000 - val_cohen_kappa: 0.5079 - val_f1_score: 0.5201 - lr: 0.0010\nEpoch 11/30\n832/831 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.9145 - categorical_accuracy: 0.9145 - auc: 0.9972 - precision: 0.9307 - recall: 0.9016 - true_positives: 11997.0000 - true_negatives: 331782.0000 - false_positives: 893.0000 - false_negatives: 1310.0000 - cohen_kappa: 0.9108 - f1_score: 0.9122\nEpoch 11: val_accuracy did not improve from 0.85689\n831/831 [==============================] - 218s 261ms/step - loss: 0.2407 - accuracy: 0.9145 - categorical_accuracy: 0.9145 - auc: 0.9972 - precision: 0.9307 - recall: 0.9016 - true_positives: 11997.0000 - true_negatives: 331782.0000 - false_positives: 893.0000 - false_negatives: 1310.0000 - cohen_kappa: 0.9108 - f1_score: 0.9122 - val_loss: 1.3159 - val_accuracy: 0.6342 - val_categorical_accuracy: 0.6342 - val_auc: 0.9522 - val_precision: 0.6732 - val_recall: 0.5885 - val_true_positives: 991.0000 - val_true_negatives: 41619.0000 - val_false_positives: 481.0000 - val_false_negatives: 693.0000 - val_cohen_kappa: 0.6191 - val_f1_score: 0.6524 - lr: 0.0010\nEpoch 12/30\n832/831 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.9129 - categorical_accuracy: 0.9129 - auc: 0.9966 - precision: 0.9285 - recall: 0.8998 - true_positives: 11974.0000 - true_negatives: 331753.0000 - false_positives: 922.0000 - false_negatives: 1333.0000 - cohen_kappa: 0.9092 - f1_score: 0.9113\nEpoch 12: val_accuracy did not improve from 0.85689\n831/831 [==============================] - 223s 268ms/step - loss: 0.2467 - accuracy: 0.9129 - categorical_accuracy: 0.9129 - auc: 0.9966 - precision: 0.9285 - recall: 0.8998 - true_positives: 11974.0000 - true_negatives: 331753.0000 - false_positives: 922.0000 - false_negatives: 1333.0000 - cohen_kappa: 0.9092 - f1_score: 0.9113 - val_loss: 1.3250 - val_accuracy: 0.6211 - val_categorical_accuracy: 0.6211 - val_auc: 0.9482 - val_precision: 0.7223 - val_recall: 0.5730 - val_true_positives: 965.0000 - val_true_negatives: 41729.0000 - val_false_positives: 371.0000 - val_false_negatives: 719.0000 - val_cohen_kappa: 0.6049 - val_f1_score: 0.6134 - lr: 0.0010\nEpoch 13/30\n832/831 [==============================] - ETA: 0s - loss: 0.2118 - accuracy: 0.9227 - categorical_accuracy: 0.9227 - auc: 0.9974 - precision: 0.9359 - recall: 0.9123 - true_positives: 12140.0000 - true_negatives: 331843.0000 - false_positives: 832.0000 - false_negatives: 1167.0000 - cohen_kappa: 0.9194 - f1_score: 0.9204\nEpoch 13: val_accuracy improved from 0.85689 to 0.94715, saving model to Best_DenseNet201_v23.h5\n831/831 [==============================] - 226s 272ms/step - loss: 0.2118 - accuracy: 0.9227 - categorical_accuracy: 0.9227 - auc: 0.9974 - precision: 0.9359 - recall: 0.9123 - true_positives: 12140.0000 - true_negatives: 331843.0000 - false_positives: 832.0000 - false_negatives: 1167.0000 - cohen_kappa: 0.9194 - f1_score: 0.9204 - val_loss: 0.1364 - val_accuracy: 0.9471 - val_categorical_accuracy: 0.9471 - val_auc: 0.9990 - val_precision: 0.9504 - val_recall: 0.9448 - val_true_positives: 1591.0000 - val_true_negatives: 42017.0000 - val_false_positives: 83.0000 - val_false_negatives: 93.0000 - val_cohen_kappa: 0.9449 - val_f1_score: 0.9413 - lr: 0.0010\nEpoch 14/30\n832/831 [==============================] - ETA: 0s - loss: 0.2117 - accuracy: 0.9255 - categorical_accuracy: 0.9255 - auc: 0.9971 - precision: 0.9393 - recall: 0.9145 - true_positives: 12169.0000 - true_negatives: 331888.0000 - false_positives: 787.0000 - false_negatives: 1138.0000 - cohen_kappa: 0.9223 - f1_score: 0.9236\nEpoch 14: val_accuracy did not improve from 0.94715\n831/831 [==============================] - 213s 255ms/step - loss: 0.2117 - accuracy: 0.9255 - categorical_accuracy: 0.9255 - auc: 0.9971 - precision: 0.9393 - recall: 0.9145 - true_positives: 12169.0000 - true_negatives: 331888.0000 - false_positives: 787.0000 - false_negatives: 1138.0000 - cohen_kappa: 0.9223 - f1_score: 0.9236 - val_loss: 0.2490 - val_accuracy: 0.9192 - val_categorical_accuracy: 0.9192 - val_auc: 0.9956 - val_precision: 0.9346 - val_recall: 0.9074 - val_true_positives: 1528.0000 - val_true_negatives: 41993.0000 - val_false_positives: 107.0000 - val_false_negatives: 156.0000 - val_cohen_kappa: 0.9158 - val_f1_score: 0.9180 - lr: 0.0010\nEpoch 15/30\n832/831 [==============================] - ETA: 0s - loss: 0.2057 - accuracy: 0.9276 - categorical_accuracy: 0.9276 - auc: 0.9973 - precision: 0.9393 - recall: 0.9158 - true_positives: 12186.0000 - true_negatives: 331888.0000 - false_positives: 787.0000 - false_negatives: 1121.0000 - cohen_kappa: 0.9244 - f1_score: 0.9258\nEpoch 15: val_accuracy did not improve from 0.94715\n831/831 [==============================] - 211s 254ms/step - loss: 0.2057 - accuracy: 0.9276 - categorical_accuracy: 0.9276 - auc: 0.9973 - precision: 0.9393 - recall: 0.9158 - true_positives: 12186.0000 - true_negatives: 331888.0000 - false_positives: 787.0000 - false_negatives: 1121.0000 - cohen_kappa: 0.9244 - f1_score: 0.9258 - val_loss: 0.1479 - val_accuracy: 0.9436 - val_categorical_accuracy: 0.9436 - val_auc: 0.9990 - val_precision: 0.9479 - val_recall: 0.9394 - val_true_positives: 1582.0000 - val_true_negatives: 42013.0000 - val_false_positives: 87.0000 - val_false_negatives: 102.0000 - val_cohen_kappa: 0.9412 - val_f1_score: 0.9397 - lr: 0.0010\nEpoch 16/30\n832/831 [==============================] - ETA: 0s - loss: 0.1854 - accuracy: 0.9320 - categorical_accuracy: 0.9320 - auc: 0.9977 - precision: 0.9429 - recall: 0.9237 - true_positives: 12292.0000 - true_negatives: 331930.0000 - false_positives: 745.0000 - false_negatives: 1015.0000 - cohen_kappa: 0.9291 - f1_score: 0.9303\nEpoch 16: val_accuracy improved from 0.94715 to 0.94774, saving model to Best_DenseNet201_v23.h5\n831/831 [==============================] - 217s 260ms/step - loss: 0.1854 - accuracy: 0.9320 - categorical_accuracy: 0.9320 - auc: 0.9977 - precision: 0.9429 - recall: 0.9237 - true_positives: 12292.0000 - true_negatives: 331930.0000 - false_positives: 745.0000 - false_negatives: 1015.0000 - cohen_kappa: 0.9291 - f1_score: 0.9303 - val_loss: 0.1398 - val_accuracy: 0.9477 - val_categorical_accuracy: 0.9477 - val_auc: 0.9990 - val_precision: 0.9521 - val_recall: 0.9442 - val_true_positives: 1590.0000 - val_true_negatives: 42020.0000 - val_false_positives: 80.0000 - val_false_negatives: 94.0000 - val_cohen_kappa: 0.9455 - val_f1_score: 0.9447 - lr: 0.0010\nEpoch 17/30\n832/831 [==============================] - ETA: 0s - loss: 0.1753 - accuracy: 0.9369 - categorical_accuracy: 0.9369 - auc: 0.9982 - precision: 0.9459 - recall: 0.9290 - true_positives: 12362.0000 - true_negatives: 331968.0000 - false_positives: 707.0000 - false_negatives: 945.0000 - cohen_kappa: 0.9342 - f1_score: 0.9346\nEpoch 17: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 17: val_accuracy did not improve from 0.94774\n831/831 [==============================] - 213s 256ms/step - loss: 0.1753 - accuracy: 0.9369 - categorical_accuracy: 0.9369 - auc: 0.9982 - precision: 0.9459 - recall: 0.9290 - true_positives: 12362.0000 - true_negatives: 331968.0000 - false_positives: 707.0000 - false_negatives: 945.0000 - cohen_kappa: 0.9342 - f1_score: 0.9346 - val_loss: 0.5996 - val_accuracy: 0.8201 - val_categorical_accuracy: 0.8201 - val_auc: 0.9825 - val_precision: 0.8471 - val_recall: 0.8029 - val_true_positives: 1352.0000 - val_true_negatives: 41856.0000 - val_false_positives: 244.0000 - val_false_negatives: 332.0000 - val_cohen_kappa: 0.8122 - val_f1_score: 0.8153 - lr: 0.0010\nEpoch 18/30\n832/831 [==============================] - ETA: 0s - loss: 0.0841 - accuracy: 0.9684 - categorical_accuracy: 0.9684 - auc: 0.9995 - precision: 0.9723 - recall: 0.9651 - true_positives: 12842.0000 - true_negatives: 332309.0000 - false_positives: 366.0000 - false_negatives: 465.0000 - cohen_kappa: 0.9670 - f1_score: 0.9658\nEpoch 18: val_accuracy improved from 0.94774 to 0.96675, saving model to Best_DenseNet201_v23.h5\n831/831 [==============================] - 214s 258ms/step - loss: 0.0841 - accuracy: 0.9684 - categorical_accuracy: 0.9684 - auc: 0.9995 - precision: 0.9723 - recall: 0.9651 - true_positives: 12842.0000 - true_negatives: 332309.0000 - false_positives: 366.0000 - false_negatives: 465.0000 - cohen_kappa: 0.9670 - f1_score: 0.9658 - val_loss: 0.0697 - val_accuracy: 0.9667 - val_categorical_accuracy: 0.9667 - val_auc: 0.9993 - val_precision: 0.9684 - val_recall: 0.9650 - val_true_positives: 1625.0000 - val_true_negatives: 42047.0000 - val_false_positives: 53.0000 - val_false_negatives: 59.0000 - val_cohen_kappa: 0.9653 - val_f1_score: 0.9618 - lr: 1.0000e-04\nEpoch 19/30\n832/831 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9755 - categorical_accuracy: 0.9755 - auc: 0.9999 - precision: 0.9770 - recall: 0.9743 - true_positives: 12965.0000 - true_negatives: 332370.0000 - false_positives: 305.0000 - false_negatives: 342.0000 - cohen_kappa: 0.9744 - f1_score: 0.9729\nEpoch 19: val_accuracy improved from 0.96675 to 0.97090, saving model to Best_DenseNet201_v23.h5\n831/831 [==============================] - 216s 259ms/step - loss: 0.0570 - accuracy: 0.9755 - categorical_accuracy: 0.9755 - auc: 0.9999 - precision: 0.9770 - recall: 0.9743 - true_positives: 12965.0000 - true_negatives: 332370.0000 - false_positives: 305.0000 - false_negatives: 342.0000 - cohen_kappa: 0.9744 - f1_score: 0.9729 - val_loss: 0.0544 - val_accuracy: 0.9709 - val_categorical_accuracy: 0.9709 - val_auc: 0.9999 - val_precision: 0.9726 - val_recall: 0.9709 - val_true_positives: 1635.0000 - val_true_negatives: 42054.0000 - val_false_positives: 46.0000 - val_false_negatives: 49.0000 - val_cohen_kappa: 0.9696 - val_f1_score: 0.9663 - lr: 1.0000e-04\nEpoch 20/30\n832/831 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9779 - categorical_accuracy: 0.9779 - auc: 0.9999 - precision: 0.9786 - recall: 0.9773 - true_positives: 13005.0000 - true_negatives: 332391.0000 - false_positives: 284.0000 - false_negatives: 302.0000 - cohen_kappa: 0.9769 - f1_score: 0.9754\nEpoch 20: val_accuracy improved from 0.97090 to 0.97328, saving model to Best_DenseNet201_v23.h5\n831/831 [==============================] - 213s 255ms/step - loss: 0.0491 - accuracy: 0.9779 - categorical_accuracy: 0.9779 - auc: 0.9999 - precision: 0.9786 - recall: 0.9773 - true_positives: 13005.0000 - true_negatives: 332391.0000 - false_positives: 284.0000 - false_negatives: 302.0000 - cohen_kappa: 0.9769 - f1_score: 0.9754 - val_loss: 0.0578 - val_accuracy: 0.9733 - val_categorical_accuracy: 0.9733 - val_auc: 0.9996 - val_precision: 0.9732 - val_recall: 0.9715 - val_true_positives: 1636.0000 - val_true_negatives: 42055.0000 - val_false_positives: 45.0000 - val_false_negatives: 48.0000 - val_cohen_kappa: 0.9721 - val_f1_score: 0.9691 - lr: 1.0000e-04\nEpoch 21/30\n832/831 [==============================] - ETA: 0s - loss: 0.0466 - accuracy: 0.9790 - categorical_accuracy: 0.9790 - auc: 0.9999 - precision: 0.9796 - recall: 0.9777 - true_positives: 13010.0000 - true_negatives: 332404.0000 - false_positives: 271.0000 - false_negatives: 297.0000 - cohen_kappa: 0.9780 - f1_score: 0.9763\nEpoch 21: val_accuracy did not improve from 0.97328\n831/831 [==============================] - 210s 252ms/step - loss: 0.0466 - accuracy: 0.9790 - categorical_accuracy: 0.9790 - auc: 0.9999 - precision: 0.9796 - recall: 0.9777 - true_positives: 13010.0000 - true_negatives: 332404.0000 - false_positives: 271.0000 - false_negatives: 297.0000 - cohen_kappa: 0.9780 - f1_score: 0.9763 - val_loss: 0.0551 - val_accuracy: 0.9709 - val_categorical_accuracy: 0.9709 - val_auc: 0.9996 - val_precision: 0.9721 - val_recall: 0.9709 - val_true_positives: 1635.0000 - val_true_negatives: 42053.0000 - val_false_positives: 47.0000 - val_false_negatives: 49.0000 - val_cohen_kappa: 0.9696 - val_f1_score: 0.9666 - lr: 1.0000e-04\nEpoch 22/30\n832/831 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9800 - categorical_accuracy: 0.9800 - auc: 0.9999 - precision: 0.9807 - recall: 0.9794 - true_positives: 13033.0000 - true_negatives: 332418.0000 - false_positives: 257.0000 - false_negatives: 274.0000 - cohen_kappa: 0.9791 - f1_score: 0.9773\nEpoch 22: val_accuracy did not improve from 0.97328\n831/831 [==============================] - 218s 261ms/step - loss: 0.0431 - accuracy: 0.9800 - categorical_accuracy: 0.9800 - auc: 0.9999 - precision: 0.9807 - recall: 0.9794 - true_positives: 13033.0000 - true_negatives: 332418.0000 - false_positives: 257.0000 - false_negatives: 274.0000 - cohen_kappa: 0.9791 - f1_score: 0.9773 - val_loss: 0.0683 - val_accuracy: 0.9709 - val_categorical_accuracy: 0.9709 - val_auc: 0.9990 - val_precision: 0.9709 - val_recall: 0.9709 - val_true_positives: 1635.0000 - val_true_negatives: 42051.0000 - val_false_positives: 49.0000 - val_false_negatives: 49.0000 - val_cohen_kappa: 0.9696 - val_f1_score: 0.9671 - lr: 1.0000e-04\nEpoch 23/30\n832/831 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9799 - categorical_accuracy: 0.9799 - auc: 0.9998 - precision: 0.9804 - recall: 0.9797 - true_positives: 13037.0000 - true_negatives: 332414.0000 - false_positives: 261.0000 - false_negatives: 270.0000 - cohen_kappa: 0.9790 - f1_score: 0.9773\nEpoch 23: val_accuracy did not improve from 0.97328\n831/831 [==============================] - 213s 256ms/step - loss: 0.0453 - accuracy: 0.9799 - categorical_accuracy: 0.9799 - auc: 0.9998 - precision: 0.9804 - recall: 0.9797 - true_positives: 13037.0000 - true_negatives: 332414.0000 - false_positives: 261.0000 - false_negatives: 270.0000 - cohen_kappa: 0.9790 - f1_score: 0.9773 - val_loss: 0.0593 - val_accuracy: 0.9673 - val_categorical_accuracy: 0.9673 - val_auc: 0.9993 - val_precision: 0.9679 - val_recall: 0.9667 - val_true_positives: 1628.0000 - val_true_negatives: 42046.0000 - val_false_positives: 54.0000 - val_false_negatives: 56.0000 - val_cohen_kappa: 0.9659 - val_f1_score: 0.9626 - lr: 1.0000e-04\nEpoch 24/30\n832/831 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9790 - categorical_accuracy: 0.9790 - auc: 0.9999 - precision: 0.9795 - recall: 0.9786 - true_positives: 13022.0000 - true_negatives: 332403.0000 - false_positives: 272.0000 - false_negatives: 285.0000 - cohen_kappa: 0.9780 - f1_score: 0.9762\nEpoch 24: val_accuracy improved from 0.97328 to 0.97387, saving model to Best_DenseNet201_v23.h5\n831/831 [==============================] - 217s 260ms/step - loss: 0.0442 - accuracy: 0.9790 - categorical_accuracy: 0.9790 - auc: 0.9999 - precision: 0.9795 - recall: 0.9786 - true_positives: 13022.0000 - true_negatives: 332403.0000 - false_positives: 272.0000 - false_negatives: 285.0000 - cohen_kappa: 0.9780 - f1_score: 0.9762 - val_loss: 0.0510 - val_accuracy: 0.9739 - val_categorical_accuracy: 0.9739 - val_auc: 0.9996 - val_precision: 0.9745 - val_recall: 0.9739 - val_true_positives: 1640.0000 - val_true_negatives: 42057.0000 - val_false_positives: 43.0000 - val_false_negatives: 44.0000 - val_cohen_kappa: 0.9727 - val_f1_score: 0.9677 - lr: 1.0000e-04\nEpoch 25/30\n832/831 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9808 - categorical_accuracy: 0.9808 - auc: 0.9999 - precision: 0.9809 - recall: 0.9803 - true_positives: 13045.0000 - true_negatives: 332421.0000 - false_positives: 254.0000 - false_negatives: 262.0000 - cohen_kappa: 0.9799 - f1_score: 0.9781\nEpoch 25: val_accuracy improved from 0.97387 to 0.97447, saving model to Best_DenseNet201_v23.h5\n831/831 [==============================] - 214s 257ms/step - loss: 0.0413 - accuracy: 0.9808 - categorical_accuracy: 0.9808 - auc: 0.9999 - precision: 0.9809 - recall: 0.9803 - true_positives: 13045.0000 - true_negatives: 332421.0000 - false_positives: 254.0000 - false_negatives: 262.0000 - cohen_kappa: 0.9799 - f1_score: 0.9781 - val_loss: 0.0554 - val_accuracy: 0.9745 - val_categorical_accuracy: 0.9745 - val_auc: 0.9993 - val_precision: 0.9745 - val_recall: 0.9745 - val_true_positives: 1641.0000 - val_true_negatives: 42057.0000 - val_false_positives: 43.0000 - val_false_negatives: 43.0000 - val_cohen_kappa: 0.9734 - val_f1_score: 0.9688 - lr: 1.0000e-04\nEpoch 26/30\n832/831 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9826 - categorical_accuracy: 0.9826 - auc: 1.0000 - precision: 0.9829 - recall: 0.9821 - true_positives: 13069.0000 - true_negatives: 332447.0000 - false_positives: 228.0000 - false_negatives: 238.0000 - cohen_kappa: 0.9818 - f1_score: 0.9800\nEpoch 26: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n\nEpoch 26: val_accuracy did not improve from 0.97447\n831/831 [==============================] - 210s 252ms/step - loss: 0.0368 - accuracy: 0.9826 - categorical_accuracy: 0.9826 - auc: 1.0000 - precision: 0.9829 - recall: 0.9821 - true_positives: 13069.0000 - true_negatives: 332447.0000 - false_positives: 228.0000 - false_negatives: 238.0000 - cohen_kappa: 0.9818 - f1_score: 0.9800 - val_loss: 0.0472 - val_accuracy: 0.9715 - val_categorical_accuracy: 0.9715 - val_auc: 0.9996 - val_precision: 0.9721 - val_recall: 0.9715 - val_true_positives: 1636.0000 - val_true_negatives: 42053.0000 - val_false_positives: 47.0000 - val_false_negatives: 48.0000 - val_cohen_kappa: 0.9703 - val_f1_score: 0.9661 - lr: 1.0000e-04\nEpoch 27/30\n832/831 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9817 - categorical_accuracy: 0.9817 - auc: 0.9999 - precision: 0.9820 - recall: 0.9814 - true_positives: 13060.0000 - true_negatives: 332435.0000 - false_positives: 240.0000 - false_negatives: 247.0000 - cohen_kappa: 0.9809 - f1_score: 0.9790\nEpoch 27: val_accuracy did not improve from 0.97447\n831/831 [==============================] - 215s 258ms/step - loss: 0.0407 - accuracy: 0.9817 - categorical_accuracy: 0.9817 - auc: 0.9999 - precision: 0.9820 - recall: 0.9814 - true_positives: 13060.0000 - true_negatives: 332435.0000 - false_positives: 240.0000 - false_negatives: 247.0000 - cohen_kappa: 0.9809 - f1_score: 0.9790 - val_loss: 0.0470 - val_accuracy: 0.9715 - val_categorical_accuracy: 0.9715 - val_auc: 0.9996 - val_precision: 0.9715 - val_recall: 0.9715 - val_true_positives: 1636.0000 - val_true_negatives: 42052.0000 - val_false_positives: 48.0000 - val_false_negatives: 48.0000 - val_cohen_kappa: 0.9703 - val_f1_score: 0.9667 - lr: 1.0000e-05\nEpoch 28/30\n832/831 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9844 - categorical_accuracy: 0.9844 - auc: 0.9999 - precision: 0.9847 - recall: 0.9842 - true_positives: 13097.0000 - true_negatives: 332472.0000 - false_positives: 203.0000 - false_negatives: 210.0000 - cohen_kappa: 0.9838 - f1_score: 0.9822\nEpoch 28: val_accuracy did not improve from 0.97447\n831/831 [==============================] - 217s 260ms/step - loss: 0.0354 - accuracy: 0.9844 - categorical_accuracy: 0.9844 - auc: 0.9999 - precision: 0.9847 - recall: 0.9842 - true_positives: 13097.0000 - true_negatives: 332472.0000 - false_positives: 203.0000 - false_negatives: 210.0000 - cohen_kappa: 0.9838 - f1_score: 0.9822 - val_loss: 0.0470 - val_accuracy: 0.9703 - val_categorical_accuracy: 0.9703 - val_auc: 0.9996 - val_precision: 0.9703 - val_recall: 0.9703 - val_true_positives: 1634.0000 - val_true_negatives: 42050.0000 - val_false_positives: 50.0000 - val_false_negatives: 50.0000 - val_cohen_kappa: 0.9690 - val_f1_score: 0.9653 - lr: 1.0000e-05\nEpoch 29/30\n832/831 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9835 - categorical_accuracy: 0.9835 - auc: 1.0000 - precision: 0.9836 - recall: 0.9832 - true_positives: 13084.0000 - true_negatives: 332457.0000 - false_positives: 218.0000 - false_negatives: 223.0000 - cohen_kappa: 0.9828 - f1_score: 0.9810\nEpoch 29: val_accuracy did not improve from 0.97447\n831/831 [==============================] - 213s 256ms/step - loss: 0.0347 - accuracy: 0.9835 - categorical_accuracy: 0.9835 - auc: 1.0000 - precision: 0.9836 - recall: 0.9832 - true_positives: 13084.0000 - true_negatives: 332457.0000 - false_positives: 218.0000 - false_negatives: 223.0000 - cohen_kappa: 0.9828 - f1_score: 0.9810 - val_loss: 0.0510 - val_accuracy: 0.9673 - val_categorical_accuracy: 0.9673 - val_auc: 0.9999 - val_precision: 0.9673 - val_recall: 0.9673 - val_true_positives: 1629.0000 - val_true_negatives: 42045.0000 - val_false_positives: 55.0000 - val_false_negatives: 55.0000 - val_cohen_kappa: 0.9659 - val_f1_score: 0.9625 - lr: 1.0000e-05\nEpoch 30/30\n832/831 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9835 - categorical_accuracy: 0.9835 - auc: 0.9999 - precision: 0.9839 - recall: 0.9835 - true_positives: 13087.0000 - true_negatives: 332461.0000 - false_positives: 214.0000 - false_negatives: 220.0000 - cohen_kappa: 0.9828 - f1_score: 0.9814\nEpoch 30: val_accuracy did not improve from 0.97447\n831/831 [==============================] - 217s 261ms/step - loss: 0.0346 - accuracy: 0.9835 - categorical_accuracy: 0.9835 - auc: 0.9999 - precision: 0.9839 - recall: 0.9835 - true_positives: 13087.0000 - true_negatives: 332461.0000 - false_positives: 214.0000 - false_negatives: 220.0000 - cohen_kappa: 0.9828 - f1_score: 0.9814 - val_loss: 0.0466 - val_accuracy: 0.9697 - val_categorical_accuracy: 0.9697 - val_auc: 0.9999 - val_precision: 0.9697 - val_recall: 0.9697 - val_true_positives: 1633.0000 - val_true_negatives: 42049.0000 - val_false_positives: 51.0000 - val_false_negatives: 51.0000 - val_cohen_kappa: 0.9684 - val_f1_score: 0.9650 - lr: 1.0000e-05\n","output_type":"stream"}]},{"cell_type":"code","source":"features = final_model.predict_generator(generator_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:57:06.238691Z","iopub.execute_input":"2024-01-03T08:57:06.239113Z","iopub.status.idle":"2024-01-03T08:57:24.560439Z","shell.execute_reply.started":"2024-01-03T08:57:06.239083Z","shell.execute_reply":"2024-01-03T08:57:24.559535Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/1249813240.py:1: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n  features = final_model.predict_generator(generator_test)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:57:27.291225Z","iopub.execute_input":"2024-01-03T08:57:27.291586Z","iopub.status.idle":"2024-01-03T08:57:27.296179Z","shell.execute_reply.started":"2024-01-03T08:57:27.291556Z","shell.execute_reply":"2024-01-03T08:57:27.295175Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Flatten the features\nfeatures_flat = features.reshape(features.shape[0], -1)\n\n# Assuming 'labels' is a list/array of class labels for each image in the test set\nlabels = generator_test.classes\n\n# Apply t-SNE\ntsne = TSNE(n_components=2, random_state=42)\ntsne_result = tsne.fit_transform(features_flat)\n\n# Create a scatter plot\nplt.figure(figsize=(10, 8))\nplt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=labels, cmap='viridis')\nplt.title('t-SNE Visualization of Model Features')\nplt.xlabel('t-SNE Component 1')\nplt.ylabel('t-SNE Component 2')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:57:29.989159Z","iopub.execute_input":"2024-01-03T08:57:29.989882Z","iopub.status.idle":"2024-01-03T08:57:35.648788Z","shell.execute_reply.started":"2024-01-03T08:57:29.989849Z","shell.execute_reply":"2024-01-03T08:57:35.647872Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x800 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import beit","metadata":{"execution":{"iopub.status.busy":"2023-12-11T20:06:19.440306Z","iopub.execute_input":"2023-12-11T20:06:19.441526Z","iopub.status.idle":"2023-12-11T20:06:19.511689Z","shell.execute_reply.started":"2023-12-11T20:06:19.441483Z","shell.execute_reply":"2023-12-11T20:06:19.510768Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import swin_transformer_v2","metadata":{"execution":{"iopub.status.busy":"2023-12-11T20:06:25.903555Z","iopub.execute_input":"2023-12-11T20:06:25.904378Z","iopub.status.idle":"2023-12-11T20:06:25.908399Z","shell.execute_reply.started":"2023-12-11T20:06:25.904341Z","shell.execute_reply":"2023-12-11T20:06:25.907410Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import swin_transformer_v2\nmm = swin_transformer_v2.SwinTransformerV2Tiny_window8(input_shape=(112, 112, 3))","metadata":{"execution":{"iopub.status.busy":"2023-12-11T20:06:32.451436Z","iopub.execute_input":"2023-12-11T20:06:32.452405Z","iopub.status.idle":"2023-12-11T20:06:37.010842Z","shell.execute_reply.started":"2023-12-11T20:06:32.452359Z","shell.execute_reply":"2023-12-11T20:06:37.009799Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":">>>> Load pretrained from: /root/.keras/models/swin_transformer_v2_tiny_window8_256_imagenet.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"# from keras_cv_attention_models import nat\n\nmm2 =beit.BeitBasePatch16(input_shape=(224, 224, 3))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T20:06:37.012747Z","iopub.execute_input":"2023-12-11T20:06:37.013074Z","iopub.status.idle":"2023-12-11T20:06:40.858770Z","shell.execute_reply.started":"2023-12-11T20:06:37.013047Z","shell.execute_reply":"2023-12-11T20:06:40.857894Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":">>>> Load pretrained from: /root/.keras/models/beit_base_patch16_224_imagenet21k-ft1k.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"mm_last_layer = custom_model .get_layer('avg_pool').output\n#out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\n#out = Dense(11, activation='softmax', name='prediction1')(out)\nmm_custom = Model(custom_model .input, mm_last_layer)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T08:31:34.207278Z","iopub.execute_input":"2023-12-30T08:31:34.208153Z","iopub.status.idle":"2023-12-30T08:31:34.233332Z","shell.execute_reply.started":"2023-12-30T08:31:34.208110Z","shell.execute_reply":"2023-12-30T08:31:34.232541Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"mm_last_layer = mm2 .get_layer('out_ln').output\n# out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\n# out = Dense(11, activation='softmax', name='prediction1')(out)\nmm2_custom = Model(mm2 .input,mm_last_layer )","metadata":{"execution":{"iopub.status.busy":"2023-12-11T20:06:42.647967Z","iopub.execute_input":"2023-12-11T20:06:42.648461Z","iopub.status.idle":"2023-12-11T20:06:42.680340Z","shell.execute_reply.started":"2023-12-11T20:06:42.648420Z","shell.execute_reply":"2023-12-11T20:06:42.679633Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"mm_last_layer = mm2 .get_layer('stack4_block5_2_output').output\n#out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\n#out = Dense(11, activation='softmax', name='prediction1')(out)\nmm_custom = Model(mm2 .input, mm_last_layer)\n\nfrom tensorflow.keras import layers\ninputs = keras.Input(shape=(112,112,3))\noutputs = layers.average([mm_custom(inputs)])\n","metadata":{"execution":{"iopub.status.busy":"2023-12-30T08:29:49.993592Z","iopub.execute_input":"2023-12-30T08:29:49.994543Z","iopub.status.idle":"2023-12-30T08:29:50.767100Z","shell.execute_reply.started":"2023-12-30T08:29:49.994505Z","shell.execute_reply":"2023-12-30T08:29:50.765635Z"},"trusted":true},"execution_count":15,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mm_last_layer \u001b[38;5;241m=\u001b[39m \u001b[43mmm2\u001b[49m \u001b[38;5;241m.\u001b[39mget_layer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstack4_block5_2_output\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moutput\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#out = Dense(11, activation='softmax', name='prediction1')(out)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m mm_custom \u001b[38;5;241m=\u001b[39m Model(mm2 \u001b[38;5;241m.\u001b[39minput, mm_last_layer)\n","\u001b[0;31mNameError\u001b[0m: name 'mm2' is not defined"],"ename":"NameError","evalue":"name 'mm2' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# from tensorflow.keras import layers\n\n# inputs = keras.Input(shape=(112, 112, 3))\n# mm_last_layer = mm2.get_layer('stack4_block5_2_output').output\n# outputs = layers.average([mm_last_layer])\n\n# avg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\ninputs = keras.Input(shape=(112,112,3))\noutputs = layers.average([mm_custom(inputs)])\n\navg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\navg_ensemble_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-30T08:31:38.844542Z","iopub.execute_input":"2023-12-30T08:31:38.844950Z","iopub.status.idle":"2023-12-30T08:31:39.958058Z","shell.execute_reply.started":"2023-12-30T08:31:38.844907Z","shell.execute_reply":"2023-12-30T08:31:39.956305Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Model: \"model_8\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_6 (InputLayer)        [(None, 112, 112, 3)]     0         \n                                                                 \n model_7 (Functional)        (None, 768)               22487658  \n                                                                 \n average_1 (Average)         (None, 768)               0         \n                                                                 \n=================================================================\nTotal params: 22,487,658\nTrainable params: 22,476,010\nNon-trainable params: 11,648\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras import layers\ninputs = keras.Input(shape=(224,224,3))\noutputs = layers.average([mm_custom(inputs)])\n\navg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\navg_ensemble_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install --upgrade keras_cv_attention_models\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 26\navg_ensemble_model_last_layer = avg_ensemble_model.get_layer('average').output\noutput_layer = Dense(num_classes, activation='softmax', name='output_1')(avg_ensemble_model_last_layer)\nfinal_model = Model(avg_ensemble_model.input, output_layer)\n\nfinal_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-29T13:20:49.721024Z","iopub.execute_input":"2023-12-29T13:20:49.721419Z","iopub.status.idle":"2023-12-29T13:20:49.782999Z","shell.execute_reply.started":"2023-12-29T13:20:49.721390Z","shell.execute_reply":"2023-12-29T13:20:49.782138Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Model: \"model_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 112, 112, 3)]     0         \n                                                                 \n model_1 (Functional)        (None, 512)               16704736  \n                                                                 \n average (Average)           (None, 512)               0         \n                                                                 \n output_1 (Dense)            (None, 26)                13338     \n                                                                 \n=================================================================\nTotal params: 16,718,074\nTrainable params: 16,693,498\nNon-trainable params: 24,576\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = Adam(lr=1e-5)\nloss = 'categorical_crossentropy'\n# metrics = ['categorical_accuracy']\nmetrics = ['accuracy', 'categorical_accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), \n           tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), \n           tf.keras.metrics.FalseNegatives(), tfa.metrics.CohenKappa(num_classes = num_classes), \n           tfa.metrics.F1Score(num_classes = num_classes)]\n\nfinal_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)","metadata":{"execution":{"iopub.status.busy":"2023-12-29T13:20:59.180813Z","iopub.execute_input":"2023-12-29T13:20:59.181220Z","iopub.status.idle":"2023-12-29T13:20:59.244358Z","shell.execute_reply.started":"2023-12-29T13:20:59.181187Z","shell.execute_reply":"2023-12-29T13:20:59.243429Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nlr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1,\n    patience=9, mode=\"max\", min_delta=0.0001, min_lr=0.00001, verbose=1)\ncheckpoint = ModelCheckpoint(filepath='Best_DenseNet2013_v23.h5', save_best_only=True, monitor = 'val_accuracy', verbose=1)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, restore_best_weights=True)\n\ncallbacks = [lr, checkpoint, early_stopping]","metadata":{"execution":{"iopub.status.busy":"2023-12-29T13:21:46.150220Z","iopub.execute_input":"2023-12-29T13:21:46.150596Z","iopub.status.idle":"2023-12-29T13:21:46.157221Z","shell.execute_reply.started":"2023-12-29T13:21:46.150567Z","shell.execute_reply":"2023-12-29T13:21:46.156146Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"epochs = 30\n\nsteps_per_epoch = generator_train.n / batch_size\nsteps_test = generator_test.n / batch_size\n\nhistory = final_model.fit_generator(generator=generator_train,\n                                  epochs=epochs,\n                                  steps_per_epoch=steps_per_epoch,\n                                  validation_data=generator_test,\n                                  validation_steps=steps_test,\n                                   callbacks=callbacks, class_weight =class_weights)","metadata":{"execution":{"iopub.status.busy":"2023-12-29T13:21:50.979832Z","iopub.execute_input":"2023-12-29T13:21:50.980247Z","iopub.status.idle":"2023-12-29T15:13:16.827340Z","shell.execute_reply.started":"2023-12-29T13:21:50.980213Z","shell.execute_reply":"2023-12-29T15:13:16.826338Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/2027776823.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n  history = final_model.fit_generator(generator=generator_train,\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30\n832/831 [==============================] - ETA: 0s - loss: 1.5334 - accuracy: 0.5354 - categorical_accuracy: 0.5354 - auc: 0.9359 - precision: 0.7516 - recall: 0.3906 - true_positives: 5198.0000 - true_negatives: 330957.0000 - false_positives: 1718.0000 - false_negatives: 8109.0000 - cohen_kappa: 0.5163 - f1_score: 0.5287\nEpoch 1: val_accuracy improved from -inf to 0.37173, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 358s 346ms/step - loss: 1.5334 - accuracy: 0.5354 - categorical_accuracy: 0.5354 - auc: 0.9359 - precision: 0.7516 - recall: 0.3906 - true_positives: 5198.0000 - true_negatives: 330957.0000 - false_positives: 1718.0000 - false_negatives: 8109.0000 - cohen_kappa: 0.5163 - f1_score: 0.5287 - val_loss: 2.4273 - val_accuracy: 0.3717 - val_categorical_accuracy: 0.3717 - val_auc: 0.8711 - val_precision: 0.4443 - val_recall: 0.3171 - val_true_positives: 534.0000 - val_true_negatives: 41432.0000 - val_false_positives: 668.0000 - val_false_negatives: 1150.0000 - val_cohen_kappa: 0.3465 - val_f1_score: 0.3623 - lr: 0.0010\nEpoch 2/30\n832/831 [==============================] - ETA: 0s - loss: 0.7923 - accuracy: 0.7449 - categorical_accuracy: 0.7449 - auc: 0.9799 - precision: 0.8302 - recall: 0.6703 - true_positives: 8920.0000 - true_negatives: 330850.0000 - false_positives: 1825.0000 - false_negatives: 4387.0000 - cohen_kappa: 0.7341 - f1_score: 0.7434\nEpoch 2: val_accuracy improved from 0.37173 to 0.60629, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 224s 269ms/step - loss: 0.7923 - accuracy: 0.7449 - categorical_accuracy: 0.7449 - auc: 0.9799 - precision: 0.8302 - recall: 0.6703 - true_positives: 8920.0000 - true_negatives: 330850.0000 - false_positives: 1825.0000 - false_negatives: 4387.0000 - cohen_kappa: 0.7341 - f1_score: 0.7434 - val_loss: 1.4716 - val_accuracy: 0.6063 - val_categorical_accuracy: 0.6063 - val_auc: 0.9406 - val_precision: 0.6627 - val_recall: 0.5635 - val_true_positives: 949.0000 - val_true_negatives: 41617.0000 - val_false_positives: 483.0000 - val_false_negatives: 735.0000 - val_cohen_kappa: 0.5912 - val_f1_score: 0.6386 - lr: 0.0010\nEpoch 3/30\n832/831 [==============================] - ETA: 0s - loss: 0.5858 - accuracy: 0.8096 - categorical_accuracy: 0.8096 - auc: 0.9881 - precision: 0.8631 - recall: 0.7568 - true_positives: 10071.0000 - true_negatives: 331078.0000 - false_positives: 1597.0000 - false_negatives: 3236.0000 - cohen_kappa: 0.8015 - f1_score: 0.8082\nEpoch 3: val_accuracy improved from 0.60629 to 0.61164, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 218s 262ms/step - loss: 0.5858 - accuracy: 0.8096 - categorical_accuracy: 0.8096 - auc: 0.9881 - precision: 0.8631 - recall: 0.7568 - true_positives: 10071.0000 - true_negatives: 331078.0000 - false_positives: 1597.0000 - false_negatives: 3236.0000 - cohen_kappa: 0.8015 - f1_score: 0.8082 - val_loss: 1.5807 - val_accuracy: 0.6116 - val_categorical_accuracy: 0.6116 - val_auc: 0.9330 - val_precision: 0.6503 - val_recall: 0.5730 - val_true_positives: 965.0000 - val_true_negatives: 41581.0000 - val_false_positives: 519.0000 - val_false_negatives: 719.0000 - val_cohen_kappa: 0.5972 - val_f1_score: 0.6356 - lr: 0.0010\nEpoch 4/30\n832/831 [==============================] - ETA: 0s - loss: 0.4635 - accuracy: 0.8461 - categorical_accuracy: 0.8461 - auc: 0.9918 - precision: 0.8892 - recall: 0.8091 - true_positives: 10767.0000 - true_negatives: 331334.0000 - false_positives: 1341.0000 - false_negatives: 2540.0000 - cohen_kappa: 0.8395 - f1_score: 0.8446\nEpoch 4: val_accuracy improved from 0.61164 to 0.82007, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 213s 256ms/step - loss: 0.4635 - accuracy: 0.8461 - categorical_accuracy: 0.8461 - auc: 0.9918 - precision: 0.8892 - recall: 0.8091 - true_positives: 10767.0000 - true_negatives: 331334.0000 - false_positives: 1341.0000 - false_negatives: 2540.0000 - cohen_kappa: 0.8395 - f1_score: 0.8446 - val_loss: 0.5349 - val_accuracy: 0.8201 - val_categorical_accuracy: 0.8201 - val_auc: 0.9894 - val_precision: 0.8590 - val_recall: 0.7779 - val_true_positives: 1310.0000 - val_true_negatives: 41885.0000 - val_false_positives: 215.0000 - val_false_negatives: 374.0000 - val_cohen_kappa: 0.8124 - val_f1_score: 0.8123 - lr: 0.0010\nEpoch 5/30\n832/831 [==============================] - ETA: 0s - loss: 0.4024 - accuracy: 0.8605 - categorical_accuracy: 0.8605 - auc: 0.9932 - precision: 0.8961 - recall: 0.8308 - true_positives: 11056.0000 - true_negatives: 331393.0000 - false_positives: 1282.0000 - false_negatives: 2251.0000 - cohen_kappa: 0.8545 - f1_score: 0.8599\nEpoch 5: val_accuracy did not improve from 0.82007\n831/831 [==============================] - 215s 258ms/step - loss: 0.4024 - accuracy: 0.8605 - categorical_accuracy: 0.8605 - auc: 0.9932 - precision: 0.8961 - recall: 0.8308 - true_positives: 11056.0000 - true_negatives: 331393.0000 - false_positives: 1282.0000 - false_negatives: 2251.0000 - cohen_kappa: 0.8545 - f1_score: 0.8599 - val_loss: 0.6772 - val_accuracy: 0.7815 - val_categorical_accuracy: 0.7815 - val_auc: 0.9828 - val_precision: 0.8326 - val_recall: 0.7381 - val_true_positives: 1243.0000 - val_true_negatives: 41850.0000 - val_false_positives: 250.0000 - val_false_negatives: 441.0000 - val_cohen_kappa: 0.7724 - val_f1_score: 0.7879 - lr: 0.0010\nEpoch 6/30\n832/831 [==============================] - ETA: 0s - loss: 0.3548 - accuracy: 0.8765 - categorical_accuracy: 0.8765 - auc: 0.9943 - precision: 0.9053 - recall: 0.8511 - true_positives: 11325.0000 - true_negatives: 331490.0000 - false_positives: 1185.0000 - false_negatives: 1982.0000 - cohen_kappa: 0.8712 - f1_score: 0.8754\nEpoch 6: val_accuracy did not improve from 0.82007\n831/831 [==============================] - 210s 253ms/step - loss: 0.3548 - accuracy: 0.8765 - categorical_accuracy: 0.8765 - auc: 0.9943 - precision: 0.9053 - recall: 0.8511 - true_positives: 11325.0000 - true_negatives: 331490.0000 - false_positives: 1185.0000 - false_negatives: 1982.0000 - cohen_kappa: 0.8712 - f1_score: 0.8754 - val_loss: 3.4096 - val_accuracy: 0.3741 - val_categorical_accuracy: 0.3741 - val_auc: 0.8094 - val_precision: 0.4187 - val_recall: 0.3456 - val_true_positives: 582.0000 - val_true_negatives: 41292.0000 - val_false_positives: 808.0000 - val_false_negatives: 1102.0000 - val_cohen_kappa: 0.3484 - val_f1_score: 0.3431 - lr: 0.0010\nEpoch 7/30\n832/831 [==============================] - ETA: 0s - loss: 0.3266 - accuracy: 0.8886 - categorical_accuracy: 0.8886 - auc: 0.9953 - precision: 0.9128 - recall: 0.8668 - true_positives: 11534.0000 - true_negatives: 331573.0000 - false_positives: 1102.0000 - false_negatives: 1773.0000 - cohen_kappa: 0.8838 - f1_score: 0.8877\nEpoch 7: val_accuracy improved from 0.82007 to 0.86045, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 221s 266ms/step - loss: 0.3266 - accuracy: 0.8886 - categorical_accuracy: 0.8886 - auc: 0.9953 - precision: 0.9128 - recall: 0.8668 - true_positives: 11534.0000 - true_negatives: 331573.0000 - false_positives: 1102.0000 - false_negatives: 1773.0000 - cohen_kappa: 0.8838 - f1_score: 0.8877 - val_loss: 0.4074 - val_accuracy: 0.8605 - val_categorical_accuracy: 0.8605 - val_auc: 0.9929 - val_precision: 0.8850 - val_recall: 0.8314 - val_true_positives: 1400.0000 - val_true_negatives: 41918.0000 - val_false_positives: 182.0000 - val_false_negatives: 284.0000 - val_cohen_kappa: 0.8546 - val_f1_score: 0.8634 - lr: 0.0010\nEpoch 8/30\n832/831 [==============================] - ETA: 0s - loss: 0.2961 - accuracy: 0.9010 - categorical_accuracy: 0.9010 - auc: 0.9955 - precision: 0.9225 - recall: 0.8825 - true_positives: 11743.0000 - true_negatives: 331689.0000 - false_positives: 986.0000 - false_negatives: 1564.0000 - cohen_kappa: 0.8967 - f1_score: 0.8986\nEpoch 8: val_accuracy did not improve from 0.86045\n831/831 [==============================] - 212s 255ms/step - loss: 0.2961 - accuracy: 0.9010 - categorical_accuracy: 0.9010 - auc: 0.9955 - precision: 0.9225 - recall: 0.8825 - true_positives: 11743.0000 - true_negatives: 331689.0000 - false_positives: 986.0000 - false_negatives: 1564.0000 - cohen_kappa: 0.8967 - f1_score: 0.8986 - val_loss: 1.2833 - val_accuracy: 0.6188 - val_categorical_accuracy: 0.6188 - val_auc: 0.9546 - val_precision: 0.6841 - val_recall: 0.5903 - val_true_positives: 994.0000 - val_true_negatives: 41641.0000 - val_false_positives: 459.0000 - val_false_negatives: 690.0000 - val_cohen_kappa: 0.6037 - val_f1_score: 0.6390 - lr: 0.0010\nEpoch 9/30\n832/831 [==============================] - ETA: 0s - loss: 0.2414 - accuracy: 0.9116 - categorical_accuracy: 0.9116 - auc: 0.9971 - precision: 0.9276 - recall: 0.8979 - true_positives: 11949.0000 - true_negatives: 331743.0000 - false_positives: 932.0000 - false_negatives: 1358.0000 - cohen_kappa: 0.9078 - f1_score: 0.9110\nEpoch 9: val_accuracy improved from 0.86045 to 0.88717, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 213s 256ms/step - loss: 0.2414 - accuracy: 0.9116 - categorical_accuracy: 0.9116 - auc: 0.9971 - precision: 0.9276 - recall: 0.8979 - true_positives: 11949.0000 - true_negatives: 331743.0000 - false_positives: 932.0000 - false_negatives: 1358.0000 - cohen_kappa: 0.9078 - f1_score: 0.9110 - val_loss: 0.3221 - val_accuracy: 0.8872 - val_categorical_accuracy: 0.8872 - val_auc: 0.9933 - val_precision: 0.9020 - val_recall: 0.8747 - val_true_positives: 1473.0000 - val_true_negatives: 41940.0000 - val_false_positives: 160.0000 - val_false_negatives: 211.0000 - val_cohen_kappa: 0.8824 - val_f1_score: 0.8907 - lr: 0.0010\nEpoch 10/30\n832/831 [==============================] - ETA: 0s - loss: 0.2595 - accuracy: 0.9110 - categorical_accuracy: 0.9110 - auc: 0.9965 - precision: 0.9283 - recall: 0.8960 - true_positives: 11923.0000 - true_negatives: 331754.0000 - false_positives: 921.0000 - false_negatives: 1384.0000 - cohen_kappa: 0.9072 - f1_score: 0.9086\nEpoch 10: val_accuracy did not improve from 0.88717\n831/831 [==============================] - 220s 264ms/step - loss: 0.2595 - accuracy: 0.9110 - categorical_accuracy: 0.9110 - auc: 0.9965 - precision: 0.9283 - recall: 0.8960 - true_positives: 11923.0000 - true_negatives: 331754.0000 - false_positives: 921.0000 - false_negatives: 1384.0000 - cohen_kappa: 0.9072 - f1_score: 0.9086 - val_loss: 0.9451 - val_accuracy: 0.7310 - val_categorical_accuracy: 0.7310 - val_auc: 0.9685 - val_precision: 0.7874 - val_recall: 0.6686 - val_true_positives: 1126.0000 - val_true_negatives: 41796.0000 - val_false_positives: 304.0000 - val_false_negatives: 558.0000 - val_cohen_kappa: 0.7195 - val_f1_score: 0.7176 - lr: 0.0010\nEpoch 11/30\n832/831 [==============================] - ETA: 0s - loss: 0.2345 - accuracy: 0.9151 - categorical_accuracy: 0.9151 - auc: 0.9972 - precision: 0.9304 - recall: 0.9018 - true_positives: 12000.0000 - true_negatives: 331778.0000 - false_positives: 897.0000 - false_negatives: 1307.0000 - cohen_kappa: 0.9114 - f1_score: 0.9138\nEpoch 11: val_accuracy improved from 0.88717 to 0.91568, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 216s 259ms/step - loss: 0.2345 - accuracy: 0.9151 - categorical_accuracy: 0.9151 - auc: 0.9972 - precision: 0.9304 - recall: 0.9018 - true_positives: 12000.0000 - true_negatives: 331778.0000 - false_positives: 897.0000 - false_negatives: 1307.0000 - cohen_kappa: 0.9114 - f1_score: 0.9138 - val_loss: 0.2194 - val_accuracy: 0.9157 - val_categorical_accuracy: 0.9157 - val_auc: 0.9970 - val_precision: 0.9294 - val_recall: 0.9068 - val_true_positives: 1527.0000 - val_true_negatives: 41984.0000 - val_false_positives: 116.0000 - val_false_negatives: 157.0000 - val_cohen_kappa: 0.9120 - val_f1_score: 0.9117 - lr: 0.0010\nEpoch 12/30\n832/831 [==============================] - ETA: 0s - loss: 0.2331 - accuracy: 0.9158 - categorical_accuracy: 0.9158 - auc: 0.9975 - precision: 0.9311 - recall: 0.9045 - true_positives: 12036.0000 - true_negatives: 331785.0000 - false_positives: 890.0000 - false_negatives: 1271.0000 - cohen_kappa: 0.9121 - f1_score: 0.9138\nEpoch 12: val_accuracy improved from 0.91568 to 0.92755, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 215s 259ms/step - loss: 0.2331 - accuracy: 0.9158 - categorical_accuracy: 0.9158 - auc: 0.9975 - precision: 0.9311 - recall: 0.9045 - true_positives: 12036.0000 - true_negatives: 331785.0000 - false_positives: 890.0000 - false_negatives: 1271.0000 - cohen_kappa: 0.9121 - f1_score: 0.9138 - val_loss: 0.2142 - val_accuracy: 0.9276 - val_categorical_accuracy: 0.9276 - val_auc: 0.9953 - val_precision: 0.9305 - val_recall: 0.9216 - val_true_positives: 1552.0000 - val_true_negatives: 41984.0000 - val_false_positives: 116.0000 - val_false_negatives: 132.0000 - val_cohen_kappa: 0.9244 - val_f1_score: 0.9245 - lr: 0.0010\nEpoch 13/30\n832/831 [==============================] - ETA: 0s - loss: 0.2118 - accuracy: 0.9258 - categorical_accuracy: 0.9258 - auc: 0.9974 - precision: 0.9392 - recall: 0.9137 - true_positives: 12158.0000 - true_negatives: 331888.0000 - false_positives: 787.0000 - false_negatives: 1149.0000 - cohen_kappa: 0.9226 - f1_score: 0.9240\nEpoch 13: val_accuracy did not improve from 0.92755\n831/831 [==============================] - 221s 265ms/step - loss: 0.2118 - accuracy: 0.9258 - categorical_accuracy: 0.9258 - auc: 0.9974 - precision: 0.9392 - recall: 0.9137 - true_positives: 12158.0000 - true_negatives: 331888.0000 - false_positives: 787.0000 - false_negatives: 1149.0000 - cohen_kappa: 0.9226 - f1_score: 0.9240 - val_loss: 0.2280 - val_accuracy: 0.9204 - val_categorical_accuracy: 0.9204 - val_auc: 0.9970 - val_precision: 0.9325 - val_recall: 0.9103 - val_true_positives: 1533.0000 - val_true_negatives: 41989.0000 - val_false_positives: 111.0000 - val_false_negatives: 151.0000 - val_cohen_kappa: 0.9170 - val_f1_score: 0.9159 - lr: 0.0010\nEpoch 14/30\n832/831 [==============================] - ETA: 0s - loss: 0.2007 - accuracy: 0.9266 - categorical_accuracy: 0.9266 - auc: 0.9976 - precision: 0.9388 - recall: 0.9170 - true_positives: 12203.0000 - true_negatives: 331879.0000 - false_positives: 796.0000 - false_negatives: 1104.0000 - cohen_kappa: 0.9234 - f1_score: 0.9244\nEpoch 14: val_accuracy improved from 0.92755 to 0.93646, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 212s 255ms/step - loss: 0.2007 - accuracy: 0.9266 - categorical_accuracy: 0.9266 - auc: 0.9976 - precision: 0.9388 - recall: 0.9170 - true_positives: 12203.0000 - true_negatives: 331879.0000 - false_positives: 796.0000 - false_negatives: 1104.0000 - cohen_kappa: 0.9234 - f1_score: 0.9244 - val_loss: 0.1699 - val_accuracy: 0.9365 - val_categorical_accuracy: 0.9365 - val_auc: 0.9979 - val_precision: 0.9440 - val_recall: 0.9317 - val_true_positives: 1569.0000 - val_true_negatives: 42007.0000 - val_false_positives: 93.0000 - val_false_negatives: 115.0000 - val_cohen_kappa: 0.9337 - val_f1_score: 0.9294 - lr: 0.0010\nEpoch 15/30\n832/831 [==============================] - ETA: 0s - loss: 0.2057 - accuracy: 0.9248 - categorical_accuracy: 0.9248 - auc: 0.9976 - precision: 0.9371 - recall: 0.9150 - true_positives: 12176.0000 - true_negatives: 331858.0000 - false_positives: 817.0000 - false_negatives: 1131.0000 - cohen_kappa: 0.9215 - f1_score: 0.9228\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 15: val_accuracy did not improve from 0.93646\n831/831 [==============================] - 213s 256ms/step - loss: 0.2057 - accuracy: 0.9248 - categorical_accuracy: 0.9248 - auc: 0.9976 - precision: 0.9371 - recall: 0.9150 - true_positives: 12176.0000 - true_negatives: 331858.0000 - false_positives: 817.0000 - false_negatives: 1131.0000 - cohen_kappa: 0.9215 - f1_score: 0.9228 - val_loss: 0.3294 - val_accuracy: 0.8842 - val_categorical_accuracy: 0.8842 - val_auc: 0.9940 - val_precision: 0.8983 - val_recall: 0.8658 - val_true_positives: 1458.0000 - val_true_negatives: 41935.0000 - val_false_positives: 165.0000 - val_false_negatives: 226.0000 - val_cohen_kappa: 0.8792 - val_f1_score: 0.8826 - lr: 0.0010\nEpoch 16/30\n832/831 [==============================] - ETA: 0s - loss: 0.0864 - accuracy: 0.9654 - categorical_accuracy: 0.9654 - auc: 0.9995 - precision: 0.9690 - recall: 0.9618 - true_positives: 12799.0000 - true_negatives: 332266.0000 - false_positives: 409.0000 - false_negatives: 508.0000 - cohen_kappa: 0.9639 - f1_score: 0.9626\nEpoch 16: val_accuracy improved from 0.93646 to 0.96734, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 220s 265ms/step - loss: 0.0864 - accuracy: 0.9654 - categorical_accuracy: 0.9654 - auc: 0.9995 - precision: 0.9690 - recall: 0.9618 - true_positives: 12799.0000 - true_negatives: 332266.0000 - false_positives: 409.0000 - false_negatives: 508.0000 - cohen_kappa: 0.9639 - f1_score: 0.9626 - val_loss: 0.0618 - val_accuracy: 0.9673 - val_categorical_accuracy: 0.9673 - val_auc: 0.9999 - val_precision: 0.9696 - val_recall: 0.9662 - val_true_positives: 1627.0000 - val_true_negatives: 42049.0000 - val_false_positives: 51.0000 - val_false_negatives: 57.0000 - val_cohen_kappa: 0.9659 - val_f1_score: 0.9624 - lr: 1.0000e-04\nEpoch 17/30\n832/831 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9748 - categorical_accuracy: 0.9748 - auc: 0.9999 - precision: 0.9766 - recall: 0.9735 - true_positives: 12955.0000 - true_negatives: 332365.0000 - false_positives: 310.0000 - false_negatives: 352.0000 - cohen_kappa: 0.9737 - f1_score: 0.9724\nEpoch 17: val_accuracy did not improve from 0.96734\n831/831 [==============================] - 215s 258ms/step - loss: 0.0571 - accuracy: 0.9748 - categorical_accuracy: 0.9748 - auc: 0.9999 - precision: 0.9766 - recall: 0.9735 - true_positives: 12955.0000 - true_negatives: 332365.0000 - false_positives: 310.0000 - false_negatives: 352.0000 - cohen_kappa: 0.9737 - f1_score: 0.9724 - val_loss: 0.0575 - val_accuracy: 0.9673 - val_categorical_accuracy: 0.9673 - val_auc: 0.9999 - val_precision: 0.9685 - val_recall: 0.9662 - val_true_positives: 1627.0000 - val_true_negatives: 42047.0000 - val_false_positives: 53.0000 - val_false_negatives: 57.0000 - val_cohen_kappa: 0.9659 - val_f1_score: 0.9634 - lr: 1.0000e-04\nEpoch 18/30\n832/831 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.9766 - categorical_accuracy: 0.9766 - auc: 0.9999 - precision: 0.9776 - recall: 0.9758 - true_positives: 12985.0000 - true_negatives: 332377.0000 - false_positives: 298.0000 - false_negatives: 322.0000 - cohen_kappa: 0.9755 - f1_score: 0.9740\nEpoch 18: val_accuracy improved from 0.96734 to 0.96912, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 224s 270ms/step - loss: 0.0511 - accuracy: 0.9766 - categorical_accuracy: 0.9766 - auc: 0.9999 - precision: 0.9776 - recall: 0.9758 - true_positives: 12985.0000 - true_negatives: 332377.0000 - false_positives: 298.0000 - false_negatives: 322.0000 - cohen_kappa: 0.9755 - f1_score: 0.9740 - val_loss: 0.0573 - val_accuracy: 0.9691 - val_categorical_accuracy: 0.9691 - val_auc: 0.9996 - val_precision: 0.9697 - val_recall: 0.9691 - val_true_positives: 1632.0000 - val_true_negatives: 42049.0000 - val_false_positives: 51.0000 - val_false_negatives: 52.0000 - val_cohen_kappa: 0.9678 - val_f1_score: 0.9652 - lr: 1.0000e-04\nEpoch 19/30\n832/831 [==============================] - ETA: 0s - loss: 0.0466 - accuracy: 0.9783 - categorical_accuracy: 0.9783 - auc: 0.9999 - precision: 0.9789 - recall: 0.9778 - true_positives: 13012.0000 - true_negatives: 332394.0000 - false_positives: 281.0000 - false_negatives: 295.0000 - cohen_kappa: 0.9773 - f1_score: 0.9756\nEpoch 19: val_accuracy improved from 0.96912 to 0.96971, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 213s 255ms/step - loss: 0.0466 - accuracy: 0.9783 - categorical_accuracy: 0.9783 - auc: 0.9999 - precision: 0.9789 - recall: 0.9778 - true_positives: 13012.0000 - true_negatives: 332394.0000 - false_positives: 281.0000 - false_negatives: 295.0000 - cohen_kappa: 0.9773 - f1_score: 0.9756 - val_loss: 0.0553 - val_accuracy: 0.9697 - val_categorical_accuracy: 0.9697 - val_auc: 0.9996 - val_precision: 0.9709 - val_recall: 0.9691 - val_true_positives: 1632.0000 - val_true_negatives: 42051.0000 - val_false_positives: 49.0000 - val_false_negatives: 52.0000 - val_cohen_kappa: 0.9684 - val_f1_score: 0.9641 - lr: 1.0000e-04\nEpoch 20/30\n832/831 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9784 - categorical_accuracy: 0.9784 - auc: 0.9999 - precision: 0.9790 - recall: 0.9778 - true_positives: 13011.0000 - true_negatives: 332396.0000 - false_positives: 279.0000 - false_negatives: 296.0000 - cohen_kappa: 0.9774 - f1_score: 0.9757\nEpoch 20: val_accuracy did not improve from 0.96971\n831/831 [==============================] - 217s 260ms/step - loss: 0.0471 - accuracy: 0.9784 - categorical_accuracy: 0.9784 - auc: 0.9999 - precision: 0.9790 - recall: 0.9778 - true_positives: 13011.0000 - true_negatives: 332396.0000 - false_positives: 279.0000 - false_negatives: 296.0000 - cohen_kappa: 0.9774 - f1_score: 0.9757 - val_loss: 0.0510 - val_accuracy: 0.9697 - val_categorical_accuracy: 0.9697 - val_auc: 0.9999 - val_precision: 0.9703 - val_recall: 0.9697 - val_true_positives: 1633.0000 - val_true_negatives: 42050.0000 - val_false_positives: 50.0000 - val_false_negatives: 51.0000 - val_cohen_kappa: 0.9684 - val_f1_score: 0.9654 - lr: 1.0000e-04\nEpoch 21/30\n832/831 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9791 - categorical_accuracy: 0.9791 - auc: 0.9999 - precision: 0.9795 - recall: 0.9785 - true_positives: 13021.0000 - true_negatives: 332402.0000 - false_positives: 273.0000 - false_negatives: 286.0000 - cohen_kappa: 0.9782 - f1_score: 0.9765\nEpoch 21: val_accuracy improved from 0.96971 to 0.97209, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 212s 254ms/step - loss: 0.0433 - accuracy: 0.9791 - categorical_accuracy: 0.9791 - auc: 0.9999 - precision: 0.9795 - recall: 0.9785 - true_positives: 13021.0000 - true_negatives: 332402.0000 - false_positives: 273.0000 - false_negatives: 286.0000 - cohen_kappa: 0.9782 - f1_score: 0.9765 - val_loss: 0.0593 - val_accuracy: 0.9721 - val_categorical_accuracy: 0.9721 - val_auc: 0.9993 - val_precision: 0.9721 - val_recall: 0.9721 - val_true_positives: 1637.0000 - val_true_negatives: 42053.0000 - val_false_positives: 47.0000 - val_false_negatives: 47.0000 - val_cohen_kappa: 0.9709 - val_f1_score: 0.9670 - lr: 1.0000e-04\nEpoch 22/30\n832/831 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9810 - categorical_accuracy: 0.9810 - auc: 1.0000 - precision: 0.9812 - recall: 0.9807 - true_positives: 13050.0000 - true_negatives: 332425.0000 - false_positives: 250.0000 - false_negatives: 257.0000 - cohen_kappa: 0.9802 - f1_score: 0.9786\nEpoch 22: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 213s 256ms/step - loss: 0.0412 - accuracy: 0.9810 - categorical_accuracy: 0.9810 - auc: 1.0000 - precision: 0.9812 - recall: 0.9807 - true_positives: 13050.0000 - true_negatives: 332425.0000 - false_positives: 250.0000 - false_negatives: 257.0000 - cohen_kappa: 0.9802 - f1_score: 0.9786 - val_loss: 0.0583 - val_accuracy: 0.9709 - val_categorical_accuracy: 0.9709 - val_auc: 0.9993 - val_precision: 0.9709 - val_recall: 0.9703 - val_true_positives: 1634.0000 - val_true_negatives: 42051.0000 - val_false_positives: 49.0000 - val_false_negatives: 50.0000 - val_cohen_kappa: 0.9696 - val_f1_score: 0.9665 - lr: 1.0000e-04\nEpoch 23/30\n832/831 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9799 - categorical_accuracy: 0.9799 - auc: 1.0000 - precision: 0.9801 - recall: 0.9795 - true_positives: 13034.0000 - true_negatives: 332410.0000 - false_positives: 265.0000 - false_negatives: 273.0000 - cohen_kappa: 0.9790 - f1_score: 0.9770\nEpoch 23: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 213s 256ms/step - loss: 0.0411 - accuracy: 0.9799 - categorical_accuracy: 0.9799 - auc: 1.0000 - precision: 0.9801 - recall: 0.9795 - true_positives: 13034.0000 - true_negatives: 332410.0000 - false_positives: 265.0000 - false_negatives: 273.0000 - cohen_kappa: 0.9790 - f1_score: 0.9770 - val_loss: 0.0627 - val_accuracy: 0.9691 - val_categorical_accuracy: 0.9691 - val_auc: 0.9993 - val_precision: 0.9697 - val_recall: 0.9685 - val_true_positives: 1631.0000 - val_true_negatives: 42049.0000 - val_false_positives: 51.0000 - val_false_negatives: 53.0000 - val_cohen_kappa: 0.9678 - val_f1_score: 0.9646 - lr: 1.0000e-04\nEpoch 24/30\n832/831 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9804 - categorical_accuracy: 0.9804 - auc: 0.9999 - precision: 0.9807 - recall: 0.9803 - true_positives: 13045.0000 - true_negatives: 332418.0000 - false_positives: 257.0000 - false_negatives: 262.0000 - cohen_kappa: 0.9795 - f1_score: 0.9777\nEpoch 24: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n\nEpoch 24: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 225s 270ms/step - loss: 0.0408 - accuracy: 0.9804 - categorical_accuracy: 0.9804 - auc: 0.9999 - precision: 0.9807 - recall: 0.9803 - true_positives: 13045.0000 - true_negatives: 332418.0000 - false_positives: 257.0000 - false_negatives: 262.0000 - cohen_kappa: 0.9795 - f1_score: 0.9777 - val_loss: 0.0573 - val_accuracy: 0.9662 - val_categorical_accuracy: 0.9662 - val_auc: 0.9996 - val_precision: 0.9673 - val_recall: 0.9662 - val_true_positives: 1627.0000 - val_true_negatives: 42045.0000 - val_false_positives: 55.0000 - val_false_negatives: 57.0000 - val_cohen_kappa: 0.9647 - val_f1_score: 0.9607 - lr: 1.0000e-04\nEpoch 25/30\n832/831 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9817 - categorical_accuracy: 0.9817 - auc: 0.9999 - precision: 0.9820 - recall: 0.9813 - true_positives: 13058.0000 - true_negatives: 332436.0000 - false_positives: 239.0000 - false_negatives: 249.0000 - cohen_kappa: 0.9809 - f1_score: 0.9791\nEpoch 25: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 221s 265ms/step - loss: 0.0362 - accuracy: 0.9817 - categorical_accuracy: 0.9817 - auc: 0.9999 - precision: 0.9820 - recall: 0.9813 - true_positives: 13058.0000 - true_negatives: 332436.0000 - false_positives: 239.0000 - false_negatives: 249.0000 - cohen_kappa: 0.9809 - f1_score: 0.9791 - val_loss: 0.0511 - val_accuracy: 0.9721 - val_categorical_accuracy: 0.9721 - val_auc: 0.9996 - val_precision: 0.9721 - val_recall: 0.9721 - val_true_positives: 1637.0000 - val_true_negatives: 42053.0000 - val_false_positives: 47.0000 - val_false_negatives: 47.0000 - val_cohen_kappa: 0.9709 - val_f1_score: 0.9669 - lr: 1.0000e-05\nEpoch 26/30\n832/831 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9828 - categorical_accuracy: 0.9828 - auc: 0.9999 - precision: 0.9832 - recall: 0.9826 - true_positives: 13075.0000 - true_negatives: 332452.0000 - false_positives: 223.0000 - false_negatives: 232.0000 - cohen_kappa: 0.9820 - f1_score: 0.9803\nEpoch 26: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 219s 263ms/step - loss: 0.0357 - accuracy: 0.9828 - categorical_accuracy: 0.9828 - auc: 0.9999 - precision: 0.9832 - recall: 0.9826 - true_positives: 13075.0000 - true_negatives: 332452.0000 - false_positives: 223.0000 - false_negatives: 232.0000 - cohen_kappa: 0.9820 - f1_score: 0.9803 - val_loss: 0.0501 - val_accuracy: 0.9721 - val_categorical_accuracy: 0.9721 - val_auc: 0.9993 - val_precision: 0.9721 - val_recall: 0.9721 - val_true_positives: 1637.0000 - val_true_negatives: 42053.0000 - val_false_positives: 47.0000 - val_false_negatives: 47.0000 - val_cohen_kappa: 0.9709 - val_f1_score: 0.9669 - lr: 1.0000e-05\nEpoch 27/30\n832/831 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9825 - categorical_accuracy: 0.9825 - auc: 0.9999 - precision: 0.9829 - recall: 0.9823 - true_positives: 13071.0000 - true_negatives: 332448.0000 - false_positives: 227.0000 - false_negatives: 236.0000 - cohen_kappa: 0.9817 - f1_score: 0.9801\nEpoch 27: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 223s 267ms/step - loss: 0.0372 - accuracy: 0.9825 - categorical_accuracy: 0.9825 - auc: 0.9999 - precision: 0.9829 - recall: 0.9823 - true_positives: 13071.0000 - true_negatives: 332448.0000 - false_positives: 227.0000 - false_negatives: 236.0000 - cohen_kappa: 0.9817 - f1_score: 0.9801 - val_loss: 0.0478 - val_accuracy: 0.9697 - val_categorical_accuracy: 0.9697 - val_auc: 0.9996 - val_precision: 0.9697 - val_recall: 0.9697 - val_true_positives: 1633.0000 - val_true_negatives: 42049.0000 - val_false_positives: 51.0000 - val_false_negatives: 51.0000 - val_cohen_kappa: 0.9684 - val_f1_score: 0.9645 - lr: 1.0000e-05\nEpoch 28/30\n832/831 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9829 - categorical_accuracy: 0.9829 - auc: 0.9999 - precision: 0.9829 - recall: 0.9829 - true_positives: 13079.0000 - true_negatives: 332448.0000 - false_positives: 227.0000 - false_negatives: 228.0000 - cohen_kappa: 0.9821 - f1_score: 0.9802\nEpoch 28: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 212s 255ms/step - loss: 0.0335 - accuracy: 0.9829 - categorical_accuracy: 0.9829 - auc: 0.9999 - precision: 0.9829 - recall: 0.9829 - true_positives: 13079.0000 - true_negatives: 332448.0000 - false_positives: 227.0000 - false_negatives: 228.0000 - cohen_kappa: 0.9821 - f1_score: 0.9802 - val_loss: 0.0484 - val_accuracy: 0.9697 - val_categorical_accuracy: 0.9697 - val_auc: 0.9996 - val_precision: 0.9697 - val_recall: 0.9697 - val_true_positives: 1633.0000 - val_true_negatives: 42049.0000 - val_false_positives: 51.0000 - val_false_negatives: 51.0000 - val_cohen_kappa: 0.9684 - val_f1_score: 0.9646 - lr: 1.0000e-05\nEpoch 29/30\n832/831 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9858 - categorical_accuracy: 0.9858 - auc: 0.9999 - precision: 0.9860 - recall: 0.9856 - true_positives: 13115.0000 - true_negatives: 332489.0000 - false_positives: 186.0000 - false_negatives: 192.0000 - cohen_kappa: 0.9852 - f1_score: 0.9836\nEpoch 29: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 214s 257ms/step - loss: 0.0317 - accuracy: 0.9858 - categorical_accuracy: 0.9858 - auc: 0.9999 - precision: 0.9860 - recall: 0.9856 - true_positives: 13115.0000 - true_negatives: 332489.0000 - false_positives: 186.0000 - false_negatives: 192.0000 - cohen_kappa: 0.9852 - f1_score: 0.9836 - val_loss: 0.0456 - val_accuracy: 0.9703 - val_categorical_accuracy: 0.9703 - val_auc: 0.9996 - val_precision: 0.9703 - val_recall: 0.9697 - val_true_positives: 1633.0000 - val_true_negatives: 42050.0000 - val_false_positives: 50.0000 - val_false_negatives: 51.0000 - val_cohen_kappa: 0.9690 - val_f1_score: 0.9655 - lr: 1.0000e-05\nEpoch 30/30\n832/831 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9856 - categorical_accuracy: 0.9856 - auc: 1.0000 - precision: 0.9857 - recall: 0.9854 - true_positives: 13113.0000 - true_negatives: 332485.0000 - false_positives: 190.0000 - false_negatives: 194.0000 - cohen_kappa: 0.9849 - f1_score: 0.9834\nEpoch 30: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 224s 268ms/step - loss: 0.0319 - accuracy: 0.9856 - categorical_accuracy: 0.9856 - auc: 1.0000 - precision: 0.9857 - recall: 0.9854 - true_positives: 13113.0000 - true_negatives: 332485.0000 - false_positives: 190.0000 - false_negatives: 194.0000 - cohen_kappa: 0.9849 - f1_score: 0.9834 - val_loss: 0.0464 - val_accuracy: 0.9703 - val_categorical_accuracy: 0.9703 - val_auc: 0.9999 - val_precision: 0.9703 - val_recall: 0.9703 - val_true_positives: 1634.0000 - val_true_negatives: 42050.0000 - val_false_positives: 50.0000 - val_false_negatives: 50.0000 - val_cohen_kappa: 0.9690 - val_f1_score: 0.9650 - lr: 1.0000e-05\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load model\nfrom tensorflow.keras.models import load_model\nmodel1 = load_model(\"/kaggle/working/Best_DenseNet2013_v23.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-12-29T16:39:15.919513Z","iopub.execute_input":"2023-12-29T16:39:15.919750Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Conv2D, Add, Concatenate\n\ndef mhsa_block(x, num_heads=4, ff_dim=None):\n    if ff_dim is None:\n        ff_dim = x.shape[-1]\n\n    # MHSA (Multi-Head Self Attention)\n    mhsa = MultiHeadSelfAttention(num_heads=num_heads, key_dim=ff_dim // num_heads)(x)\n    mhsa = Dropout(0.3)(mhsa)\n    mhsa = LayerNormalization(epsilon=1e-6)(mhsa)\n    mhsa = Add()([x, mhsa])  # Residual connection\n\n    # Feed Forward\n    ff = Conv2D(ff_dim, kernel_size=1, activation='relu')(mhsa)\n    ff = Dropout(0.3)(ff)\n    ff = Conv2D(x.shape[-1], kernel_size=1)(ff)\n    ff = Dropout(0.3)(ff)\n    ff = Add()([mhsa, ff])  # Residual connection\n\n    return ff\n\ndef lvit_block(x, num_heads=4, ff_dim=None):\n    # Clone the input\n    clone = Conv2D(x.shape[-1], kernel_size=1, strides=2)(x)\n\n    # Apply MHSA to one branch\n    mhsa_branch = mhsa_block(x, num_heads=num_heads, ff_dim=ff_dim)\n\n    # Concatenate the MHSA branch output with the cloned input\n    out = Concatenate()([clone, mhsa_branch])\n\n    # 1x1 convolution to compress dimension and maintain the block's dimension\n    out = Conv2D(x.shape[-1], kernel_size=1, strides=2)(out)\n\n    return out\n\ndef modify_wave_mlp_with_lvit(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Apply LViT block\n    lvit_output = lvit_block(mm_headless.output, num_heads=4, ff_dim=512)\n\n    # Add your custom head\n    head_output = custom_head(lvit_output, num_classes)\n\n    # Create the custom model by combining the base model, LViT block, and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model_with_lvit = modify_wave_mlp_with_lvit(input_shape, num_classes)\ncustom_model_with_lvit.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import GlobalAveragePooling2D\n\ndef modify_wave_mlp_with_lvit(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Apply Global Average Pooling to reduce dimensions\n    gap_output = GlobalAveragePooling2D()(mm_headless.output)\n\n    # Reshape output to match the original shape\n    reshaped_output = Reshape((1, 1, mm_headless.output.shape[-1]))(gap_output)\n\n    # Apply LViT block\n    lvit_output = lvit_block(reshaped_output, num_heads=4, ff_dim=512)\n\n    # Add your custom head\n    head_output = custom_head(lvit_output, num_classes)\n\n    # Create the custom model by combining the base model, LViT block, and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model_with_lvit = modify_wave_mlp_with_lvit(input_shape, num_classes)\ncustom_model_with_lvit.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Conv2D, DepthwiseConv2D, BatchNormalization, Activation, MultiHeadAttention, Dropout, Concatenate, GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\ndef custom_head(input_tensor, num_classes):\n    x = GlobalAveragePooling2D()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef lvit_block(x, mhsa_channels, strides=1, dropout_rate=0.1):\n    # Clone the input\n    input_clone = x\n\n    # Apply DW Conv to reduce computations\n    x = DepthwiseConv2D(3, strides=strides, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    # Apply 1x1 Conv to expand or decrease dimension\n    x = Conv2D(mhsa_channels, 1, padding='same')(x)\n\n    # Pass one branch through MHSA module\n    x = MultiHeadAttention(num_heads=4, key_dim=mhsa_channels // 4)(x, x)\n    x = Dropout(dropout_rate)(x)\n\n    # Concatenate the output of MHSA with the cloned input\n    x = Concatenate()([input_clone, x])\n\n    # Apply 1x1 Conv to compress dimension with a stride of 2\n    x = Conv2D(mhsa_channels, 1, strides=2, padding='same')(x)\n\n    return x\n\ndef custom_lvit(input_shape, num_classes):\n    inputs = Input(shape=input_shape)\n\n    # Initial convolutional layer\n    x = Conv2D(64, 3, strides=2, padding='same', activation='relu')(inputs)\n\n    # LViT block\n    x = lvit_block(x, mhsa_channels=256, strides=1, dropout_rate=0.3)\n\n    # Add your custom head\n    head_output = custom_head(x, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=inputs, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in custom_model.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = custom_lvit(input_shape, num_classes)\ncustom_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom keras_cv_attention_models import wave_mlp, nat\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless_wave_mlp = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head for WaveMLP\n    head_output_wave_mlp = custom_head(mm_headless_wave_mlp.output, num_classes)\n\n    # Create the custom model for WaveMLP by combining the base model and the custom head\n    custom_model_wave_mlp = Model(inputs=mm_headless_wave_mlp.input, outputs=head_output_wave_mlp)\n\n    # Fine-tune the last few layers of the base model for WaveMLP\n    for layer in mm_headless_wave_mlp.layers[:-5]:\n        layer.trainable = True\n\n    return custom_model_wave_mlp\n\n# Example usage for WaveMLP:\ninput_shape_wave_mlp = (112, 112, 3)\nnum_classes_wave_mlp = 26  # Adjust based on your task\n\ncustom_model_wave_mlp = modify_wave_mlp(input_shape_wave_mlp, num_classes_wave_mlp)\ncustom_model_wave_mlp.summary()\n\n# Load DiNAT_Mini model\nmm_nat = nat.DiNAT_Mini(input_shape=(374, 269, 3), pretrained=\"imagenet\")\n\n# Combine the models\ncombined_input = Concatenate()([custom_model_wave_mlp.output, mm_nat.output])\n\n# Add a fusion layer, you can customize this layer based on your needs\nfusion_layer = Dense(256, activation='relu')(combined_input)\nfusion_layer = Dropout(0.3)(fusion_layer)\n\n# Add the final output layer\nfinal_output = Dense(num_classes_wave_mlp, activation='softmax')(fusion_layer)\n\n# Create the combined model\ncombined_model = Model(inputs=[custom_model_wave_mlp.input, mm_nat.input], outputs=final_output)\n\n# Compile the combined model\ncombined_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n# Example usage for the combined model:\ncombined_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom keras_cv_attention_models import wave_mlp, nat\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu', name='custom_dense_1')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu', name='custom_dense_2')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax', name='custom_output')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless_wave_mlp = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head for WaveMLP\n    head_output_wave_mlp = custom_head(mm_headless_wave_mlp.output, num_classes)\n\n    # Create the custom model for WaveMLP by combining the base model and the custom head\n    custom_model_wave_mlp = Model(inputs=mm_headless_wave_mlp.input, outputs=head_output_wave_mlp)\n\n    # Fine-tune the last few layers of the base model for WaveMLP\n    for layer in mm_headless_wave_mlp.layers[:-5]:\n        layer.trainable = True\n\n    return custom_model_wave_mlp\n\n# Example usage for WaveMLP:\ninput_shape_wave_mlp = (112, 112, 3)\nnum_classes_wave_mlp = 26  # Adjust based on your task\n\ncustom_model_wave_mlp = modify_wave_mlp(input_shape_wave_mlp, num_classes_wave_mlp)\ncustom_model_wave_mlp.summary()\n\n# Load DiNAT_Mini model\nmm_nat = nat.DiNAT_Mini(input_shape=(374, 269, 3), pretrained=\"imagenet\")\n\n# Combine the models\ncombined_input = Concatenate(name='combined_input')([custom_model_wave_mlp.output, mm_nat.output])\n\n# Add a fusion layer, you can customize this layer based on your needs\nfusion_layer = Dense(256, activation='relu', name='fusion_dense_1')(combined_input)\nfusion_layer = Dropout(0.3)(fusion_layer)\n\n# Add the final output layer\nfinal_output = Dense(num_classes_wave_mlp, activation='softmax', name='final_output')(fusion_layer)\n\n# Create the combined model\ncombined_model = Model(inputs=[custom_model_wave_mlp.input, mm_nat.input], outputs=final_output)\n\n# Compile the combined model\ncombined_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n# Example usage for the combined model:\ncombined_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import wave_mlp\nmm = wave_mlp.WaveMLP_T(input_shape=(112, 112, 3), pretrained=\"imagenet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import nat\n\nmm2 = nat.NAT_Mini(input_shape=(112, 112, 3))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm_last_layer = custom_model .get_layer('dense_2').output\n#out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\n#out = Dense(11, activation='softmax', name='prediction1')(out)\nmm_custom = Model(custom_model .input, mm_last_layer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\ninputs = keras.Input(shape=(112,112,3))\noutputs = layers.average([mm_custom(inputs)])\n\navg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\navg_ensemble_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 26\navg_ensemble_model_last_layer = avg_ensemble_model.get_layer('average').output\noutput_layer = Dense(num_classes, activation='softmax', name='output_1')(avg_ensemble_model_last_layer)\nfinal_model = Model(avg_ensemble_model.input, output_layer)\n\nfinal_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(lr=1e-5)\nloss = 'categorical_crossentropy'\n# metrics = ['categorical_accuracy']\nmetrics = ['accuracy', 'categorical_accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), \n           tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), \n           tf.keras.metrics.FalseNegatives(), tfa.metrics.CohenKappa(num_classes = num_classes), \n           tfa.metrics.F1Score(num_classes = num_classes)]\n\nfinal_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nlr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1,\n    patience=9, mode=\"max\", min_delta=0.0001, min_lr=0.00001, verbose=1)\ncheckpoint = ModelCheckpoint(filepath='Best_DenseNet201_v23.h5', save_best_only=True, monitor = 'val_accuracy', verbose=1)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, restore_best_weights=True)\n\ncallbacks = [lr, checkpoint, early_stopping]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\n\nsteps_per_epoch = generator_train.n / batch_size\nsteps_test = generator_test.n / batch_size\n\nhistory = final_model.fit_generator(generator=generator_train,\n                                  epochs=epochs,\n                                  steps_per_epoch=steps_per_epoch,\n                                  validation_data=generator_test,\n                                  validation_steps=steps_test,\n                                   callbacks=callbacks, class_weight =class_weights)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import wave_mlp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow-addons\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow_addons.layers import MultiHeadAttention\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU, Dropout, Concatenate\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\ndef evaluate_(model, generator_test):\n    model.evaluate(generator_test)\n    \n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n    \n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()\n\ntrain_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\ndatagen_train = ImageDataGenerator(rescale=1./255,\n                                  width_shift_range=0.1,\n                                  height_shift_range=0.1,\n                                  horizontal_flip=True,\n                                  vertical_flip=False)\n\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\nbatch_size = 16\ngenerator_train = datagen_train.flow_from_directory(directory=train_dir,\n                                                    target_size=(112, 112),\n                                                    batch_size=batch_size,\n                                                    shuffle=True)\n\ngenerator_test = datagen_test.flow_from_directory(directory=test_dir,\n                                                  target_size=(112, 112),\n                                                  batch_size=batch_size,\n                                                  shuffle=False)\n\n# Calculate class weights\nlabels = generator_train.classes\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\nclass_weights = dict(zip(np.unique(labels), class_weights))\nprint(class_weights)\n\nfrom tensorflow_addons.layers import MultiHeadSelfAttention\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\nfrom tensorflow_addons.layers import MultiHeadAttention\n\nfrom tensorflow_addons.layers import MultiHeadAttention\n\ndef add_attention_layers(base_model_output):\n    # Add self-attention for global analysis\n    attention_global = MultiHeadAttention(\n        num_heads=8, key_dim=512, dropout=0.3\n    )(base_model_output, base_model_output, return_attention_scores=False)\n    x = GlobalAveragePooling2D()(attention_global)\n\n    # Add self-attention for local analysis\n    attention_local = MultiHeadAttention(\n        num_heads=8, key_dim=512, dropout=0.3\n    )(base_model_output, base_model_output, return_attention_scores=False)\n    x_local = Flatten()(attention_local)\n\n    # Concatenate global and local attention features\n    x = Concatenate()([x, x_local])\n\n    return x\n\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add attention layers for local and global analysis\n    attention_output = add_attention_layers(mm_headless.output)\n\n    # Add your custom head\n    head_output = custom_head(attention_output, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras-self-attention","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Reshape, Concatenate, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom keras_self_attention import SeqSelfAttention\nfrom keras_cv_attention_models import wave_mlp\n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\ndef evaluate_(model, generator_test):\n    model.evaluate(generator_test)\n    \n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n    \n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()\n\ntrain_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\ndatagen_train = ImageDataGenerator(rescale=1./255, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True, vertical_flip=False)\n\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\nbatch_size = 16\ngenerator_train = datagen_train.flow_from_directory(directory=train_dir, target_size=(112, 112), batch_size=batch_size, shuffle=True)\n\ngenerator_test = datagen_test.flow_from_directory(directory=test_dir, target_size=(112, 112), batch_size=batch_size, shuffle=False)\n\n# Calculate class weights\nlabels = generator_train.classes\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\nclass_weights = dict(zip(np.unique(labels), class_weights))\nprint(class_weights)\n\nfrom keras_self_attention import SeqSelfAttention\n\ndef create_local_global_layer(input_tensor):\n    # Local Attention\n    local_attention = SeqSelfAttention(\n        attention_width=15,\n        attention_activation='sigmoid',\n        name='LocalAttention',\n    )(input_tensor)\n\n    # Multiplicative Attention\n    multiplicative_attention = SeqSelfAttention(\n        attention_width=15,\n        attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n        attention_activation=None,\n        kernel_regularizer=keras.regularizers.l2(1e-6),\n        use_attention_bias=False,\n        name='MultiplicativeAttention',\n    )(input_tensor)\n\n    return local_attention, multiplicative_attention\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Global Average Pooling on the output\n    x = GlobalAveragePooling2D()(mm_headless.output)\n\n    # Reshape to (batch_size, 1, 1, channels)\n    x = Reshape((1, 1, -1))(x)\n\n    # Add the local and multiplicative attention layers\n    local_attention, multiplicative_attention = create_local_global_layer(x)\n\n    # Reshape attention outputs to match the shape of mm_headless.output\n    local_attention = Reshape((-1,))(local_attention)\n    multiplicative_attention = Reshape((-1,))(multiplicative_attention)\n\n    # Concatenate the attention outputs with mm_headless.output\n    concatenated_attention = Concatenate()([mm_headless.output, local_attention, multiplicative_attention])\n\n    # Add your custom head on top of the concatenated attention output\n    head_output = custom_head(concatenated_attention, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras-multi-head","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom keras_multi_head import MultiHead\nfrom keras_cv_attention_models import wave_mlp\nfrom tensorflow.keras.layers import Embedding \nfrom tensorflow.keras.layers import Embedding, concatenate \n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\ndef evaluate_(model, generator_test):\n    model.evaluate(generator_test)\n    \n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n    \n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()\n\ntrain_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\ndatagen_train = ImageDataGenerator(rescale=1./255,\n                                  width_shift_range=0.1,\n                                  height_shift_range=0.1,\n                                  horizontal_flip=True,\n                                  vertical_flip=False)\n\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\nbatch_size = 16\ngenerator_train = datagen_train.flow_from_directory(directory=train_dir,\n                                                    target_size=(112, 112),\n                                                    batch_size=batch_size,\n                                                    shuffle=True)\n\ngenerator_test = datagen_test.flow_from_directory(directory=test_dir,\n                                                  target_size=(112, 112),\n                                                  batch_size=batch_size,\n                                                  shuffle=False)\n\n# Calculate class weights\nlabels = generator_train.classes\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\nclass_weights = dict(zip(np.unique(labels), class_weights))\nprint(class_weights)\n\n!pip install keras_cv_attention_models \n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp_with_multihead(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n    head_output = custom_head(mm_headless.output, num_classes)\n\n    # Create the MultiHead model\n    multihead_model = Sequential()  # Make sure to import Sequential from keras.models\n    multihead_model.add(Embedding(input_dim=5, output_dim=3, name='Embed'))  # Add the Embedding layer\n    multihead_model.add(MultiHead(\n        layer=keras.layers.Bidirectional(keras.layers.LSTM(units=16), name='LSTM'),\n        layer_num=5,\n        reg_index=[1, 4],\n        reg_slice=(slice(None, None), slice(32, 48)),\n        reg_factor=0.1,\n        name='Multi-Head-Attention',\n    ))\n    multihead_model.add(Flatten(name='Flatten'))\n    multihead_model.add(Dense(units=num_classes, activation='softmax', name='Dense'))\n\n    # Combine the base model, custom head, and MultiHead model\n    combined_output = concatenate([head_output, multihead_model.output])  # Connect the outputs\n    combined_model = Model(inputs=mm_headless.input, outputs=combined_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    combined_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return combined_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncombined_model = modify_wave_mlp_with_multihead(input_shape, num_classes)\ncombined_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\n\n\n\ndef evaluate_(model, generator_test):\n    model.evaluate(generator_test)\n    \n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n    \n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()\n\n\ntrain_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\ndatagen_train = ImageDataGenerator(rescale=1./255,\n                                  width_shift_range=0.1,\n                                  height_shift_range=0.1,\n                                  horizontal_flip=True,\n                                  vertical_flip=False)\n\n\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\n\nbatch_size = 16\ngenerator_train = datagen_train.flow_from_directory(directory=train_dir,\n                                                    target_size=(112, 112),\n                                                    batch_size=batch_size,\n                                                    shuffle=True)\n\ngenerator_test = datagen_test.flow_from_directory(directory=test_dir,\n                                                  target_size=(112, 112),\n                                                  batch_size=batch_size,\n                                                  shuffle=False)\n# Calculate class weights\nlabels = generator_train.classes\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\nclass_weights = dict(zip(np.unique(labels), class_weights))\nprint(class_weights)\n\n\n\n!pip install keras_cv_attention_models \n\n\nfrom keras_cv_attention_models import wave_mlp\n\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n    head_output = custom_head(mm_headless.output, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tf_keras_vis.gradcam import Gradcam\nfrom tf_keras_vis.utils import normalize\n\ndef generate_gradcam(model, img_path, layer_name, num_classes):\n    img = cv2.imread(img_path)\n    img = cv2.resize(img, (112, 112))\n    img = img / 255.0\n    img_array = np.expand_dims(img, axis=0)\n\n    # Create Gradcam object\n    gradcam = Gradcam(model, model_modifier=None, clone=False)\n\n    # Generate heatmap\n    cam = gradcam(img_array, penultimate_layer=layer_name, backprop_modifier=None, grad_modifier=None)\n\n    # Render heatmap\n    heatmap = np.uint8(255 * normalize(cam))\n\n    # Resize heatmap to the original image size\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n\n    # Apply colormap (jet) to the heatmap\n    jet = plt.get_cmap(\"jet\")\n    heatmap = jet(heatmap)\n\n    # Superimpose heatmap on the original image\n    superimposed_img = (heatmap[:, :, :3] * 0.4 + img * 0.6).astype(np.uint8)\n\n    # Plot the original image, heatmap, and superimposed image\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 3, 1)\n    plt.imshow(img)\n    plt.title(\"Original Image\")\n    plt.subplot(1, 3, 2)\n    plt.imshow(heatmap)\n    plt.title(\"Grad-CAM Heatmap\")\n    plt.subplot(1, 3, 3)\n    plt.imshow(superimposed_img)\n    plt.title(\"Superimposed Image\")\n    plt.show()\n\n# Example usage:\n# Choose an image from the test set\nsample_img_path = \"/kaggle/input/mango-leaf/mango-prepo/train/Aprupali/Amrupali (163).jpg\"\nimg_class = generator_test.classes[0]  # Replace with the actual class of the image\nimg_class_name = list(generator_test.class_indices.keys())[img_class]\n\n# Generate Grad-CAM visualization for the chosen image\ngenerate_gradcam(custom_model, sample_img_path, 'your_target_layer_name', num_classes)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_grad_cam(model, img_path, class_index, preprocess_input_fn):\n    # Load the image and preprocess it\n    img = cv2.imread(img_path)\n    img = cv2.resize(img, (112, 112))\n    img = preprocess_input_fn(img)\n    img_array = np.expand_dims(img, axis=0)\n\n    # Get the last convolutional layer and the model's output\n    last_conv_layer = model.get_layer('dense_11')\n    classifier_layer_names = [\n        layer.name for layer in model.layers if layer.name.endswith('dense_layer_name')]\n\n    # Create a model that maps the input image to the activations of the last conv layer and output\n    grad_model = Model([model.inputs], [last_conv_layer.output, model.get_layer(classifier_layer_names[0]).output])\n\n    # Get the gradients of the predicted class with respect to the output feature map of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_output, preds = grad_model(img_array)\n        class_channel = preds[:, class_index]\n\n    grads = tape.gradient(class_channel, last_conv_output)\n\n    # Compute the guided gradients\n    guided_grads = tf.cast(last_conv_output > 0, 'float32') * tf.cast(grads > 0, 'float32') * grads\n\n    # Compute the average of the gradients over each feature map\n    weights = tf.reduce_mean(guided_grads, axis=(1, 2))\n\n    # Build a weighted sum of the activations of the last conv layer\n    cam = tf.reduce_sum(tf.multiply(weights, last_conv_output), axis=-1)\n\n    # Normalize the CAM\n    cam = np.maximum(cam, 0)\n    cam = cam / np.max(cam)\n\n    # Resize the CAM to match the input image size\n    cam = cv2.resize(cam[0], (112, 112))\n\n    # Convert the CAM to the range [0, 255]\n    cam = np.uint8(255 * cam)\n\n    # Apply a colormap to the CAM\n    heatmap = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n\n    # Combine the heatmap with the original image\n    img = cv2.cvtColor(img.astype('uint8'), cv2.COLOR_BGR2RGB)\n    superimposed_img = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n\n    # Display the original image, heatmap, and superimposed image\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 3, 1)\n    plt.imshow(img)\n    plt.title('Original Image')\n\n    plt.subplot(1, 3, 2)\n    plt.imshow(heatmap)\n    plt.title('Grad-CAM Heatmap')\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(superimposed_img)\n    plt.title('Superimposed Image')\n    plt.show()\n\n\n# Example usage:\ndef preprocess_input(img):\n    img = img.astype('float32') / 255.0\n    return img\n\n# Assuming you have a test image path\ntest_image_path = '/kaggle/input/mango-leaf/mango-prepo/train/Aprupali/Amrupali (163) flip.png'\nimg_class_index = 0  # Change this based on the class you want to visualize\ngenerate_grad_cam(custom_model, test_image_path, img_class_index, preprocess_input)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom keras_cv_attention_models import wave_mlp\nfrom tensorflow.keras.preprocessing import image\n\n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\n\ndef get_grad_cam(model, img_path, layer_name):\n    img = image.load_img(img_path, target_size=(112, 112))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = img_array / 255.0  # Rescale to [0,1]\n\n    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.output])\n\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        class_index = np.argmax(predictions[0])\n        loss = predictions[:, class_index]\n\n    grads = tape.gradient(loss, conv_outputs)[0]\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    conv_outputs = conv_outputs[0]\n\n    heatmap = tf.reduce_mean(tf.multiply(conv_outputs, pooled_grads), axis=-1)\n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= np.max(heatmap)\n\n    return heatmap\n\ndef evaluate_with_grad_cam(model, generator_test, img_path):\n    model.evaluate(generator_test)\n\n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n\n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()\n\n    # Get Grad-CAM for a sample image\n    img_heatmap = get_grad_cam(model, img_path, layer_name=\"multiply_1230\")\n\n    # Load the original image\n    original_img = cv2.imread(img_path)\n\n    # Resize heatmap to the size of the original image\n    heatmap_resized = cv2.resize(img_heatmap, (original_img.shape[1], original_img.shape[0]))\n\n    # Convert heatmap to RGB\n    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_resized), cv2.COLORMAP_JET)\n\n    # Superimpose the heatmap on the original image\n    superimposed_img = heatmap_colored * 0.4 + original_img * 0.6\n\n    # Display the original image, heatmap, and superimposed image\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 3, 1)\n    plt.imshow(original_img)\n    plt.title('Original Image')\n\n    plt.subplot(1, 3, 2)\n    plt.imshow(heatmap_colored)\n    plt.title('Grad-CAM Heatmap')\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(superimposed_img.astype(int))\n    plt.title('Superimposed Image')\n\n    plt.show()\n\n\ntrain_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\ndatagen_train = ImageDataGenerator(rescale=1./255,\n                                  width_shift_range=0.1,\n                                  height_shift_range=0.1,\n                                  horizontal_flip=True,\n                                  vertical_flip=False)\n\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\nbatch_size = 16\ngenerator_train = datagen_train.flow_from_directory(directory=train_dir,\n                                                    target_size=(112, 112),\n                                                    batch_size=batch_size,\n                                                    shuffle=True)\n\ngenerator_test = datagen_test.flow_from_directory(directory=test_dir,\n                                                  target_size=(112, 112),\n                                                  batch_size=batch_size,\n                                                  shuffle=False)\n\n# Calculate class weights\nlabels = generator_train.classes\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\nclass_weights = dict(zip(np.unique(labels), class_weights))\nprint(class_weights)\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n    head_output = custom_head(mm_headless.output, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tfa.metrics.F1Score(num_classes=num_classes, average='weighted')])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\n# custom_model.summary()\n\n# Example usage of evaluate_with_grad_cam\nevaluate_with_grad_cam(custom_model, generator_test, img_path=\"/kaggle/input/mango-leaf/mango-prepo/train/Aprupali/Amrupali (163) random_gaussian_noise.png\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom sklearn.utils import compute_class_weight\nfrom tensorflow.keras.layers import UpSampling2D\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\n\ndef build_autoencoder(input_shape):\n    # Build a simple convolutional autoencoder\n    # You can customize this architecture based on your requirements\n    autoencoder_input = Input(shape=input_shape)\n    \n    # Encoder\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(autoencoder_input)\n    x = MaxPool2D((2, 2), padding='same')(x)\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n    encoded = MaxPool2D((2, 2), padding='same')(x)\n\n    # Decoder\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = UpSampling2D((2, 2))(x)\n    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n\n    autoencoder = Model(autoencoder_input, decoded)\n    autoencoder.compile(optimizer='adam', loss='mse')  # Use Mean Squared Error as loss for reconstruction\n    \n    return autoencoder\n\n\ndef train_autoencoder(autoencoder, generator_train):\n    # Train the autoencoder on your training data\n    autoencoder.fit(generator_train, epochs=10, steps_per_epoch=len(generator_train), verbose=1)\n\n\ndef extract_encoder_from_autoencoder(autoencoder):\n    # Extract encoder part from the trained autoencoder\n    encoder = Model(autoencoder.input, autoencoder.layers[4].output)  # Adjust layer index based on your autoencoder architecture\n    return encoder\n\n\n# Create and train autoencoder\ninput_shape = (112, 112, 3)  # Adjust based on your image dimensions\nautoencoder = build_autoencoder(input_shape)\ntrain_autoencoder(autoencoder, generator_train)\n\n# Extract encoder from autoencoder\nencoder = extract_encoder_from_autoencoder(autoencoder)\n\n# Build classification model with encoder\nclassification_model = Sequential([\n    encoder,\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(generator_train.num_classes, activation='softmax')\n])\n\n# Compile the model\nclassification_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train classification model\nhistory = classification_model.fit(generator_train, epochs=10, validation_data=generator_test)\n\n# Evaluate the model\nevaluate_(classification_model, generator_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU, UpSampling2D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\n\ndef build_autoencoder(input_shape):\n    # Build a simple convolutional autoencoder\n    # You can customize this architecture based on your requirements\n    autoencoder_input = Input(shape=input_shape)\n    \n    # Encoder\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(autoencoder_input)\n    x = MaxPool2D((2, 2), padding='same')(x)\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n    encoded = MaxPool2D((2, 2), padding='same')(x)\n\n    # Decoder\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = UpSampling2D((2, 2))(x)\n    decoded = Conv2D(3, (3, 3), activation='linear', padding='same')(x)  # Use 'linear' activation instead of 'sigmoid'\n    decoded_reshaped = Reshape(input_shape)(decoded)  # Reshape to match input shape\n\n    autoencoder = Model(autoencoder_input, decoded_reshaped)\n    autoencoder.compile(optimizer='adam', loss='mse')  # Use Mean Squared Error as loss for reconstruction\n    \n    return autoencoder\n\n\ndef train_autoencoder(autoencoder, generator_train):\n    # Train the autoencoder on your training data\n    autoencoder.fit(generator_train, epochs=10, steps_per_epoch=len(generator_train), verbose=1)\n\n\ndef extract_encoder_from_autoencoder(autoencoder):\n    # Extract encoder part from the trained autoencoder\n    encoder = Model(autoencoder.input, autoencoder.layers[4].output)  # Adjust layer index based on your autoencoder architecture\n    return encoder\n\n\ndef evaluate_(model, generator_test):\n    model.evaluate(generator_test)\n    \n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n    \n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()\n\n\n# Paths\ntrain_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\n\n# Data Generators\ndatagen_train = ImageDataGenerator(rescale=1./255,\n                                  width_shift_range=0.1,\n                                  height_shift_range=0.1,\n                                  horizontal_flip=True,\n                                  vertical_flip=False)\n\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\nbatch_size = 16\ngenerator_train = datagen_train.flow_from_directory(directory=train_dir,\n                                                    target_size=(112, 112),\n                                                    batch_size=batch_size,\n                                                    shuffle=True)\n\ngenerator_test = datagen_test.flow_from_directory(directory=test_dir,\n                                                  target_size=(112, 112),\n                                                  batch_size=batch_size,\n                                                  shuffle=False)\n\n# Calculate class weights\nlabels = generator_train.classes\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\nclass_weights = dict(zip(np.unique(labels), class_weights))\nprint(class_weights)\n\n# Create and train autoencoder\ninput_shape = (112, 112, 3)  # Adjust based on your image dimensions\nautoencoder = build_autoencoder(input_shape)\ntrain_autoencoder(autoencoder, generator_train)\n\n# Extract encoder from autoencoder\nencoder = extract_encoder_from_autoencoder(autoencoder)\n\n# Build classification model with encoder\nclassification_model = Sequential([\n    encoder,\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(generator_train.num_classes, activation='softmax')\n])\n\n# Compile the model\nclassification_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train classification model\nhistory = classification_model.fit(generator_train, epochs=10, validation_data=generator_test)\n\n# Evaluate the model\nevaluate_(classification_model, generator_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D, UpSampling2D\nfrom tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\n\n# Autoencoder Model\ndef build_autoencoder(input_shape):\n    model = Sequential()\n\n    # Encoder\n    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n    model.add(MaxPool2D((2, 2), padding='same'))\n    model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n    model.add(MaxPool2D((2, 2), padding='same'))\n\n    # Decoder\n    model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n    model.add(UpSampling2D((2, 2)))\n    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n    model.add(UpSampling2D((2, 2)))\n    model.add(Conv2D(3, (3, 3), activation='sigmoid', padding='same'))\n\n    return model\n\n\ndef evaluate_(model, generator_test):\n    model.evaluate(generator_test)\n\n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n\n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()\n\n\ntrain_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\ndatagen_train = ImageDataGenerator(rescale=1./255,\n                                  width_shift_range=0.1,\n                                  height_shift_range=0.1,\n                                  horizontal_flip=True,\n                                  vertical_flip=False)\n\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\nbatch_size = 16\ngenerator_train = datagen_train.flow_from_directory(directory=train_dir,\n                                                    target_size=(112, 112),\n                                                    batch_size=batch_size,\n                                                    shuffle=True)\n\ngenerator_test = datagen_test.flow_from_directory(directory=test_dir,\n                                                  target_size=(112, 112),\n                                                  batch_size=batch_size,\n                                                  shuffle=False)\n\n# Build and compile the autoencoder\nautoencoder_input_shape = (112, 112, 3)\nautoencoder = build_autoencoder(autoencoder_input_shape)\nautoencoder.compile(optimizer='adam', loss='mse')  # Use Mean Squared Error for loss\n\n# Train the autoencoder on your training data\nautoencoder.fit(generator_train.next(), generator_train.next(), epochs=10, batch_size=batch_size)\n\n# Use the encoder part of the autoencoder to obtain encoded representations\nencoded_train_data = autoencoder.layers[0].predict(generator_train.next())\n\n# Classification Model using the encoded data\nclassification_model = Sequential()\nclassification_model.add(Flatten(input_shape=(encoded_train_data.shape[1:])))\nclassification_model.add(Dense(256, activation='relu'))\nclassification_model.add(Dropout(0.5))\nclassification_model.add(Dense(generator_train.num_classes, activation='softmax'))\n\n# Compile the classification model\nclassification_model.compile(optimizer='adam',\n                              loss='categorical_crossentropy',\n                              metrics=['accuracy'])\n\n# Train the classification model using the encoded data\nclassification_model.fit(encoded_train_data, keras.utils.to_categorical(generator_train.classes),\n                          epochs=10, batch_size=batch_size, class_weight=class_weights)\n\n# Evaluate the model\nevaluate_(classification_model, generator_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_grad_cam(model, img_path, class_index, preprocess_input_fn):\n    # Load the image and preprocess it\n    img = cv2.imread(img_path)\n    img = cv2.resize(img, (112, 112))\n    img = preprocess_input_fn(img)\n    img_array = np.expand_dims(img, axis=0)\n\n    # Get the last convolutional layer and the model's output\n    last_conv_layer = model.get_layer('dense_11')\n    classifier_layer_name = 'dense_2'  # Replace with the actual name of your dense layer\n\n    # Create a model that maps the input image to the activations of the last conv layer and output\n    grad_model = Model([model.inputs], [last_conv_layer.output, model.get_layer(classifier_layer_name).output])\n\n    # Get the gradients of the predicted class with respect to the output feature map of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_output, preds = grad_model(img_array)\n        class_channel = preds[:, class_index]\n\n    grads = tape.gradient(class_channel, last_conv_output)\n\n    # Compute the guided gradients\n    guided_grads = tf.cast(last_conv_output > 0, 'float32') * tf.cast(grads > 0, 'float32') * grads\n\n    # Compute the average of the gradients over each feature map\n    weights = tf.reduce_mean(guided_grads, axis=(1, 2))\n\n    # Build a weighted sum of the activations of the last conv layer\n    cam = tf.reduce_sum(tf.multiply(weights, last_conv_output), axis=-1)\n\n    # Normalize the CAM\n    cam = np.maximum(cam, 0)\n    cam = cam / np.max(cam)\n\n    # Resize the CAM to match the input image size\n    cam = cv2.resize(cam[0], (112, 112))\n\n    # Convert the CAM to the range [0, 255]\n    cam = np.uint8(255 * cam)\n\n    # Apply a colormap to the CAM\n    heatmap = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n\n    # Combine the heatmap with the original image\n    img = cv2.cvtColor(img.astype('uint8'), cv2.COLOR_BGR2RGB)\n    superimposed_img = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n\n    # Display the original image, heatmap, and superimposed image\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 3, 1)\n    plt.imshow(img)\n    plt.title('Original Image')\n\n    plt.subplot(1, 3, 2)\n    plt.imshow(heatmap)\n    plt.title('Grad-CAM Heatmap')\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(superimposed_img)\n    plt.title('Superimposed Image')\n    plt.show()\n\n\n# Example usage:\ndef preprocess_input(img):\n    img = img.astype('float32') / 255.0\n    return img\n\n# Assuming you have a test image path\ntest_image_path = '/kaggle/input/mango-leaf/mango-prepo/train/Aprupali/Amrupali (163) flip.png'\nimg_class_index = 0  # Change this based on the class you want to visualize\ngenerate_grad_cam(custom_model, test_image_path, img_class_index, preprocess_input)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tf-keras-vis\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import WaveMLP, DPTAttention\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef add_attention_layers(base_model_output):\n    # Add self-attention for global analysis\n    attention_global = DPTAttention(output_dim=512, num_heads=8, name='attention_global')(base_model_output)\n    x = GlobalAveragePooling2D()(attention_global)\n\n    # Add self-attention for local analysis\n    attention_local = DPTAttention(output_dim=512, num_heads=8, name='attention_local')(base_model_output)\n    x_local = Flatten()(attention_local)\n\n    # Concatenate global and local attention features\n    x = Concatenate()([x, x_local])\n\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = WaveMLP.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add attention layers for local and global analysis\n    attention_output = add_attention_layers(mm_headless.output)\n\n    # Add your custom head\n    head_output = custom_head(attention_output, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(256, activation='relu')(x)  # Add your own dense layers\n    x = Dropout(0.5)(x)  # Add dropout for regularization\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\n\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n#     head_output = custom_head(mm_headless.output, num_classes)\n    print(\"mm_headless.output shape:\", mm_headless.output_shape)\n    head_output = custom_head(mm_headless.output, num_classes)\n\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\n# custom_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n    head_output = custom_head(mm_headless.output, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n\n# def custom_head(input_tensor, num_classes):\n#     x = Flatten()(input_tensor)\n#     x = Dense(512, activation='relu')(x)\n#     x = Dropout(0.3)(x)\n#     x = Dense(256, activation='relu')(x)\n#     x = Dropout(0.3)(x)\n#     x = Dense(num_classes, activation='softmax')(x)\n#     return x\n\n# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Dropout\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.optimizers import Adam\n\n\n# # Define your custom_head function\n# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Dropout\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.optimizers import Adam\n\n# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Dropout\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.optimizers import Adam\n\n# # Define your custom_head function\n\n# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Dropout\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.optimizers import Adam\n\n# # Define your custom_head function\n\n# def autoencoder(base_model_output):\n#     # Extract the shape of the output tensor from the base model\n#     output_shape = base_model_output.shape[1:]\n\n#     # Check if output_shape is valid (at least 3 dimensions)\n#     if len(output_shape) < 3:\n#         raise ValueError(\"Invalid output_shape. It should have at least 3 dimensions.\")\n\n#     # Extract the relevant information from output_shape\n#     height, width, channels = output_shape[0], output_shape[1], output_shape[2]\n\n#     # Calculate the input shape based on the output shape\n#     input_shape = (height // 4, width // 4, channels)\n\n#     # Encoder\n#     encoder_input = Input(shape=input_shape)\n#     x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoder_input)\n#     x = MaxPooling2D((2, 2), padding='same')(x)\n#     x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n#     encoded = MaxPooling2D((2, 2), padding='same')(x)\n\n#     # Decoder\n#     x = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n#     x = UpSampling2D((2, 2))(x)\n#     x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n#     x = UpSampling2D((2, 2))(x)\n#     decoded = Conv2D(channels, (3, 3), activation='sigmoid', padding='same')(x)\n\n#     autoencoder_model = Model(encoder_input, decoded)\n#     autoencoder_model.compile(optimizer='adam', loss='mse')  # Adjust the loss function as needed\n\n#     return autoencoder_model\n\n# def modify_wave_mlp(input_shape, num_classes):\n#     # Load the WaveMLP model without the top layers (head)\n#     mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n#     # Obtain the output tensor from the last layer in the base model\n#     base_model_output = mm_headless.layers[-1].output\n\n#     # Create an autoencoder with input shape matching the last layer shape of the base model\n#     autoencoder_model = autoencoder(base_model_output)\n\n#     # Connect the autoencoder to the base model\n#     autoencoder_input = Input(shape=base_model_output.shape[1:])\n#     autoencoder_output = autoencoder_model(autoencoder_input)\n\n#     # Add your custom head for classification\n#     classification_output = custom_head(autoencoder_output, num_classes)\n\n#     # Create the custom model by combining the base model, autoencoder, and the custom head\n#     custom_model = Model(inputs=[mm_headless.input, autoencoder_input], outputs=[classification_output, autoencoder_output])\n\n#     # Fine-tune the last few layers of the base model\n#     for layer in mm_headless.layers[:-15]:\n#         layer.trainable = True\n\n#     # Compile the model with appropriate loss functions and metrics\n#     custom_model.compile(optimizer=Adam(lr=1e-4), \n#                          loss=['categorical_crossentropy', 'mse'], \n#                          loss_weights=[1.0, 0.5],  # Adjust the weight for the autoencoder loss\n#                          metrics={'dense_3': 'accuracy', 'model_4': 'mse'})\n\n#     return custom_model\n\n# # Example usage:\n# input_shape = (112, 112, 3)\n# num_classes = 26  # Adjust based on your task\n\n# custom_model = modify_wave_mlp(input_shape, num_classes)\n# custom_model.summary()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import imagenet_utils\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras import backend as K\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_gradcam(model, img_path, class_index, layer_name='your_last_conv_layer'):\n    # Load the image with target size\n    img = image.load_img(img_path, target_size=(input_shape[0], input_shape[1]))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = imagenet_utils.preprocess_input(img_array)\n\n    # Get the class predictions and the target layer's output\n    preds, last_conv_output = model.predict(img_array)\n    class_output = preds[0, class_index]\n\n    # Compute the gradient of the class output with respect to the feature map\n    grads = K.gradients(class_output, last_conv_output)[0]\n\n    # Pool the gradients over all the axes leaving out the channel dimension\n    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n\n    # Access the value of the target layer output\n    iterate = K.function([model.input], [pooled_grads, last_conv_output[0]])\n    pooled_grads_value, conv_layer_output_value = iterate([img_array])\n\n    # Weigh the output feature map with the computed gradient values\n    for i in range(pooled_grads.shape[0]):\n        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n\n    # Average the weighted feature map along the channel dimension to obtain the heatmap\n    heatmap = np.mean(conv_layer_output_value, axis=-1)\n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= np.max(heatmap)\n\n    # Resize the heatmap to be the same as the input image\n    img = cv2.imread(img_path)\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n\n    # Apply the heatmap to the original image\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    superimposed_img = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n\n    return superimposed_img\n\n# Specify the path to an image for visualization\nimg_path = '/kaggle/input/mango-leaf/mango-prepo/val/Baper Bari/Bbari (1002) flip.png'\n\n# Choose the index of the class you want to visualize\nclass_index = 0\n\n# Visualize Grad-CAM\ngradcam_img = get_gradcam(custom_model, img_path, class_index)\nplt.imshow(gradcam_img)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import imagenet_utils\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras import backend as K\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n# ... (rest of your code)\n\ndef get_gradcam(model, img_path, class_index, layer_name='your_last_conv_layer'):\n    # Load the image with target size\n    img = image.load_img(img_path, target_size=(input_shape[0], input_shape[1]))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = imagenet_utils.preprocess_input(img_array)\n\n    # Get the class predictions and the target layer's output\n    last_conv_output = model.predict(img_array)\n    class_output = last_conv_output[0, class_index]\n\n    # Compute the gradient of the class output with respect to the feature map\n    grads = K.gradients(class_output, last_conv_output)[0]\n\n    # Pool the gradients over all the axes leaving out the channel dimension\n    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n\n    # Access the value of the target layer output\n    iterate = K.function([model.input], [pooled_grads, last_conv_output[0]])\n    pooled_grads_value, conv_layer_output_value = iterate([img_array])\n\n    # Weigh the output feature map with the computed gradient values\n    for i in range(pooled_grads.shape[0]):\n        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n\n    # Average the weighted feature map along the channel dimension to obtain the heatmap\n    heatmap = np.mean(conv_layer_output_value, axis=-1)\n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= np.max(heatmap)\n\n    # Resize the heatmap to be the same as the input image\n    img = cv2.imread(img_path)\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n\n    # Apply the heatmap to the original image\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    superimposed_img = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n\n    return superimposed_img\n\n# ... (rest of your code)\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()\n\n# Specify the path to an image for visualization\nimg_path = '/kaggle/input/mango-leaf/mango-prepo/val/Baper Bari/Bbari (1002) flip.png'\n\n# Choose the index of the class you want to visualize\nclass_index = 0\n\n# Visualize Grad-CAM\ngradcam_img = get_gradcam(custom_model, img_path, class_index)\nplt.imshow(gradcam_img)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import imagenet_utils\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef autoencoder(input_shape):\n    # Encoder\n    encoder_input = Input(shape=input_shape)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoder_input)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n    encoded = MaxPooling2D((2, 2), padding='same')(x)\n\n    # Decoder\n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = UpSampling2D((2, 2))(x)\n    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n\n    autoencoder_model = Model(encoder_input, decoded)\n    autoencoder_model.compile(optimizer='adam', loss='mse')  # Adjust the loss function as needed\n\n    return autoencoder_model\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Get the shape of the last layer in the base model\n    base_model_output_shape = mm_headless.layers[-1].output_shape\n\n    # Check if output_shape is valid (at least 3 dimensions)\n    if len(base_model_output_shape) < 3:\n        raise ValueError(\"Invalid output_shape. It should have at least 3 dimensions.\")\n\n    # Create an autoencoder with input shape matching the last layer shape of the base model\n    autoencoder_model = autoencoder(base_model_output_shape[1:])\n\n    # Connect the autoencoder to the base model\n    autoencoder_input = Input(shape=base_model_output_shape[1:])\n    autoencoder_output = autoencoder_model(autoencoder_input)\n\n    # Add your custom head for classification\n    classification_output = custom_head(autoencoder_output, num_classes)\n\n    # Create the custom model by combining the base model, autoencoder, and the custom head\n    custom_model = Model(inputs=[mm_headless.input, autoencoder_input], outputs=[classification_output, autoencoder_output])\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with appropriate loss functions and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), \n                         loss=['categorical_crossentropy', 'mse'], \n                         loss_weights=[1.0, 0.5],  # Adjust the weight for the autoencoder loss\n                         metrics={'dense_3': 'accuracy', 'model_1': 'mse'})\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()\n\n# ... (rest of your code)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the path to an image for visualization\nimg_path = '/kaggle/input/mango-leaf/mango-prepo/val/Baper Bari/Bbari (1002) flip.png'\n\n# Choose the index of the class you want to visualize\nclass_index = 0\n\n# Visualize Grad-CAM\ngradcam_img = get_gradcam(custom_model, img_path, class_index)\nplt.imshow(gradcam_img)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm_last_layer = custom_model .get_layer('avg_pool').output\n#out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\n#out = Dense(11, activation='softmax', name='prediction1')(out)\nmm_custom = Model(custom_model .input, mm_last_layer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"mm_headless.output shape:\", mm_headless.output_shape)\nhead_output = custom_head(mm_headless.output, num_classes)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import coatnet\nmm = coatnet.CoAtNet0(input_shape=(112, 112, 3), pretrained=\"imagenet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import coatnet\nmm = coatnet.CoAtNet0(input_shape=(112, 112, 3), pretrained=\"imagenet\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import res_mlp\n# mm = res_mlp.ResMLP12()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm = res_mlp.ResMLP12(input_shape=(112, 112, 3), pretrained=\"imagenet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import wave_mlp\nmm = wave_mlp.WaveMLP_T(input_shape=(112, 112, 3), pretrained=\"imagenet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import mobilevit\nmm = mobilevit.MobileViTBasePatch16(input_shape=(112, 112, 3))\nmm2 = mobilevit.MobileViTBasePatch16(input_shape=(112, 112, 3))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mm.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import swin_transformer_v2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm2 = swin_transformer_v2.SwinTransformerV2Tiny_window8(input_shape=(112, 112, 3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mm2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ntransfer_layer = mm.get_layer('avg_pool')\nconv_model = Model(inputs=mm.input, outputs=transfer_layer.output)\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n#for layer in conv_model.layers:\n#    layer.trainable = False\n    \n# Start a new Keras Sequential model.\nnew_model = Sequential()\n\n# Add the convolutional part of the VGG16 model from above.\nnew_model.add(conv_model)\n\n\n# Add the final layer for the actual classification.\nnew_model.add(Dense(2, activation='softmax'))\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import beit\nmm2 = beit.BeitBasePatch16(input_shape=(112, 112, 3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm_last_layer = mm.get_layer('avg_pool').output\n#out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\n#out = Dense(11, activation='softmax', name='prediction1')(out)\nmm_custom = Model(mm.input, mm_last_layer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm2_last_layer = mm2.get_layer('avg_pool').output\n#out2 = Dense(256, activation='relu', name='dense_1')(mm2_last_layer)\n#out2 = Dense(11, activation='softmax', name='prediction1')(out2)\nmm2_custom = Model(mm2.input, mm2_last_layer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a custom linear attention layer\nclass LinearAttentionLayer(keras.layers.Layer):\n    def __init__(self, units, **kwargs):\n        super(LinearAttentionLayer, self).__init__(**kwargs)\n        self.units = units\n\n    def build(self, input_shape):\n        self.W_q = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n        self.W_k = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n        self.W_v = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n\n    def call(self, inputs):\n        Q = tf.matmul(inputs, self.W_q)\n        K = tf.matmul(inputs, self.W_k)\n        V = tf.matmul(inputs, self.W_v)\n\n        attn_scores = tf.matmul(Q, K, transpose_b=True)\n        attn_scores = tf.nn.softmax(attn_scores / tf.math.sqrt(tf.cast(self.units, tf.float32)), axis=-1)\n        output = tf.matmul(attn_scores, V)\n\n        return output\n\n# ... Continue with your code ...\n\n# Add the attention layer where needed in your model\nnum_classes = 2\navg_ensemble_model_last_layer = avg_ensemble_model.get_layer('average').output\n\n# Add Linear Attention Layer here (for example, just before the output layer)\nattention_output = LinearAttentionLayer(64)(avg_ensemble_model_last_layer)\n\noutput_layer = Dense(num_classes, activation='softmax', name='output_1')(attention_output)\nfinal_model = Model(avg_ensemble_model.input, output_layer)\n\nfinal_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\ninputs = keras.Input(shape=(112,112,3))\noutputs = layers.average([ mm2_custom(inputs)])\n\navg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\navg_ensemble_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\ninputs = keras.Input(shape=(112,112,3))\noutputs = layers.average([mm2_custom(inputs)])\n\navg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\navg_ensemble_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 26\navg_ensemble_model_last_layer = avg_ensemble_model.get_layer('average_3').output\noutput_layer = Dense(num_classes, activation='softmax', name='output_1')(avg_ensemble_model_last_layer)\nfinal_model = Model(avg_ensemble_model.input, output_layer)\n\nfinal_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(lr=1e-5)\nloss = 'categorical_crossentropy'\n# metrics = ['categorical_accuracy']\nmetrics = ['accuracy', 'categorical_accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), \n           tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), \n           tf.keras.metrics.FalseNegatives(), tfa.metrics.CohenKappa(num_classes = num_classes), \n           tfa.metrics.F1Score(num_classes = num_classes)]\n\nfinal_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Delete the existing HDF5 file if it exists\nif os.path.exists('Best_DenseNet201.h5'):\n    os.remove('Best_DenseNet201.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nlr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1,\n    patience=9, mode=\"max\", min_delta=0.0001, min_lr=0.00001, verbose=1)\ncheckpoint = ModelCheckpoint(filepath='Best_DenseNet201_v23.h5', save_best_only=True, monitor = 'val_accuracy', verbose=1)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, restore_best_weights=True)\n\ncallbacks = [lr, checkpoint, early_stopping]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 10\n\nsteps_per_epoch = generator_train.n / batch_size\nsteps_test = generator_test.n / batch_size\n\nhistory = final_model.fit_generator(generator=generator_train,\n                                  epochs=epochs,\n                                  steps_per_epoch=steps_per_epoch,\n                                  validation_data=generator_test,\n                                  validation_steps=steps_test,\n                                   callbacks=callbacks, class_weight =class_weights)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_(final_model, generator_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['categorical_accuracy'])\nplt.plot(history.history['val_categorical_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade scipy scikit-image\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_size = 128\nbatch_size = 8\n\ntrain_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\nval_dir = r\"/kaggle/input/mango-leaf/mango-prepo/val\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\n\n\ndatagen_train = ImageDataGenerator(rescale=1./255, width_shift_range=0.1, height_shift_range=0.1,\n                                  horizontal_flip=True,  vertical_flip=False)\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = datagen_train.flow_from_directory(directory=train_dir, target_size=(image_size, image_size),\n                                                    batch_size=batch_size, shuffle=True)\nval_generator = datagen_test.flow_from_directory(directory=val_dir, target_size=(image_size, image_size),\n                                                  batch_size=batch_size, shuffle=False)\ntest_generator = datagen_test.flow_from_directory(directory=test_dir, target_size=(image_size, image_size),\n                                                  batch_size=batch_size, shuffle=False)\n\n#Define the number of classes in your dataset\nnum_classes = train_generator.num_classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer, Attention\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LinearAttention(Layer):\n    def __init__(self, units):\n        super(LinearAttention, self).__init__()\n        self.units = units\n\n    def build(self, input_shape):\n        self.W_q = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n        self.W_k = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n        self.W_v = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n\n    def call(self, inputs):\n        Q = tf.matmul(inputs, self.W_q)\n        K = tf.matmul(inputs, self.W_k)\n        V = tf.matmul(inputs, self.W_v)\n\n        attn_scores = tf.matmul(Q, K, transpose_b=True)\n        attn_scores = tf.nn.softmax(attn_scores / tf.math.sqrt(tf.cast(self.units, tf.float32)), axis=-1)\n        output = tf.matmul(attn_scores, V)\n\n        return output\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define your model function with attention\ndef modelfunction_with_attention(base):\n    x = base.output\n\n    # Add Self-Attention Layer\n    att_output = LinearAttention(128)(x)\n\n    # Add more layers if needed\n    x = tf.keras.layers.GlobalAveragePooling2D()(att_output)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    predictions = tf.keras.layers.Dense(units=num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.02, l2=0.02))(x)\n    model = Model(inputs=base.input, outputs=predictions)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def modelfunction(base):\n    x = base.output\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    predictions = tf.keras.layers.Dense(units=num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.02, l2=0.02))(x)\n    model = Model(inputs=base.input, outputs=predictions)\n    return model\n\ndef get_callbacks(weight):\n    checkpoint = ModelCheckpoint(weight, monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n    learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=5, verbose=1, factor=0.2, min_lr=0.0002)\n    callbacks = [checkpoint, learning_rate_reduction]\n    return callbacks\n\ndef evaluate(model, generator_test):\n    model.evaluate(generator_test)\n\n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n\n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()\n\ndef model_training(base, weight, epochs):\n    model = modelfunction(base)\n    print(\"\\n\\n\\n-------------------- Model Initialized --------------------\")\n\n    callbacks = get_callbacks(weight)\n    metrics = ['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(),\n               tfa.metrics.CohenKappa(num_classes=num_classes), tfa.metrics.F1Score(num_classes=num_classes)]\n    model.compile(tf.keras.optimizers.Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=metrics)\n\n    history = model.fit(train_generator, steps_per_epoch=366 // batch_size,\n                        validation_data=val_generator,  # Add this line\n                        epochs=epochs, callbacks=callbacks)\n    # Plotting accuracy and loss curves\n    plt.figure(figsize=(12, 4))\n\n    # Plot accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='accuracy')\n    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Accuracy Over Epochs')\n    plt.legend()\n\n    # Plot loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Loss Over Epochs')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n\\n\\n-------------------- Evaluation --------------------\")\n    evaluate(model, val_generator)\n\n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create and train the model with attention\nVGG19 = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_tensor=None, input_shape=None)\nVGG19_model_with_attention = model_training(VGG19, 'VGG19_with_attention.h5', 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lime\nfrom lime import lime_image\nfrom skimage.segmentation import mark_boundaries\nimport matplotlib.pyplot as plt\nimport random\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lime import lime_image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skimage.segmentation import mark_boundaries","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
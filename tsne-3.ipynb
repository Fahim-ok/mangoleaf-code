{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6762217,"sourceType":"datasetVersion","datasetId":3891934}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))","metadata":{"execution":{"iopub.status.busy":"2024-01-03T16:01:57.461283Z","iopub.execute_input":"2024-01-03T16:01:57.462101Z","iopub.status.idle":"2024-01-03T16:01:57.477957Z","shell.execute_reply.started":"2024-01-03T16:01:57.462067Z","shell.execute_reply":"2024-01-03T16:01:57.476988Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Device mapping:\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def evaluate_(model, generator_test):\n    model.evaluate(generator_test)\n    \n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n    \n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T16:02:03.081242Z","iopub.execute_input":"2024-01-03T16:02:03.082042Z","iopub.status.idle":"2024-01-03T16:02:03.088568Z","shell.execute_reply.started":"2024-01-03T16:02:03.082007Z","shell.execute_reply":"2024-01-03T16:02:03.087577Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef evaluate_(model, generator_test):\n    model.evaluate(generator_test)\n    \n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n    \n    # Plotting the confusion matrix\n    plt.figure(figsize=(12, 4))\n    \n    plt.subplot(1, 2, 1)\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.title('Confusion Matrix')\n    \n    # ROC curve\n    plt.subplot(1, 2, 2)\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n\n    for i in range(len(class_labels)):\n        fpr[i], tpr[i], _ = roc_curve(y_true == i, y_pred[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    for i in range(len(class_labels)):\n        plt.plot(fpr[i], tpr[i], label=f'{class_labels[i]} (AUC = {roc_auc[i]:.2f})')\n\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc=\"lower right\")\n\n    plt.tight_layout()\n    plt.show()\n\n# Call the function with your model and test generator\n","metadata":{"execution":{"iopub.status.busy":"2024-01-03T16:02:08.910103Z","iopub.execute_input":"2024-01-03T16:02:08.910446Z","iopub.status.idle":"2024-01-03T16:02:08.922551Z","shell.execute_reply.started":"2024-01-03T16:02:08.910419Z","shell.execute_reply":"2024-01-03T16:02:08.921456Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\ndatagen_train = ImageDataGenerator(rescale=1./255,\n                                  width_shift_range=0.1,\n                                  height_shift_range=0.1,\n                                  horizontal_flip=True,\n                                  vertical_flip=False)\n\n\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\n\nbatch_size = 16\ngenerator_train = datagen_train.flow_from_directory(directory=train_dir,\n                                                    target_size=(128, 128),\n                                                    batch_size=batch_size,\n                                                    shuffle=True)\n\ngenerator_test = datagen_test.flow_from_directory(directory=test_dir,\n                                                  target_size=(128, 128),\n                                                  batch_size=batch_size,\n                                                  shuffle=False)\n# Calculate class weights\nlabels = generator_train.classes\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\nclass_weights = dict(zip(np.unique(labels), class_weights))\nprint(class_weights)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T16:02:55.745732Z","iopub.execute_input":"2024-01-03T16:02:55.746664Z","iopub.status.idle":"2024-01-03T16:02:56.953614Z","shell.execute_reply.started":"2024-01-03T16:02:55.746627Z","shell.execute_reply":"2024-01-03T16:02:56.952580Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Found 13307 images belonging to 26 classes.\nFound 1684 images belonging to 26 classes.\n{0: 1.0640492563569486, 1: 1.0662660256410257, 2: 1.0662660256410257, 3: 1.0402595372107568, 4: 1.2273565762774397, 5: 1.0381494772975504, 6: 1.053102247546692, 7: 1.2795192307692307, 8: 1.0487862547288778, 9: 1.053102247546692, 10: 0.6486789509603198, 11: 1.0662660256410257, 12: 1.4623076923076923, 13: 0.8809082483781279, 14: 0.7571119708693673, 15: 1.0466414975617429, 16: 1.7527660695468914, 17: 0.5415954415954416, 18: 1.0662660256410257, 19: 0.9042538733351454, 20: 0.789826685660019, 21: 1.5462468045549616, 22: 1.0256667180514876, 23: 1.8085077466702908, 24: 1.0445054945054946, 25: 0.7259683578832515}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras_cv_attention_models ","metadata":{"execution":{"iopub.status.busy":"2024-01-03T16:03:01.360764Z","iopub.execute_input":"2024-01-03T16:03:01.361619Z","iopub.status.idle":"2024-01-03T16:03:13.732846Z","shell.execute_reply.started":"2024-01-03T16:03:01.361581Z","shell.execute_reply":"2024-01-03T16:03:13.731496Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Requirement already satisfied: keras_cv_attention_models in /opt/conda/lib/python3.10/site-packages (1.3.24)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (9.5.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (4.66.1)\nRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (6.1.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (2023.6.3)\nRequirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (4.9.2)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (from keras_cv_attention_models) (2.12.0)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->keras_cv_attention_models) (0.2.12)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (3.9.0)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (0.4.13)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (16.0.0)\nRequirement already satisfied: numpy<1.24,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.23.5)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (68.0.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.16.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (4.6.3)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras_cv_attention_models) (0.32.0)\nRequirement already satisfied: array-record in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (0.4.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (8.1.7)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (0.1.8)\nRequirement already satisfied: etils[enp,epath]>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (1.3.0)\nRequirement already satisfied: promise in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (2.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (5.9.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (2.31.0)\nRequirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (0.14.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras_cv_attention_models) (0.10.2)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow->keras_cv_attention_models) (0.40.0)\nRequirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->keras_cv_attention_models) (5.12.0)\nRequirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->keras_cv_attention_models) (3.15.0)\nRequirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow->keras_cv_attention_models) (0.2.0)\nRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow->keras_cv_attention_models) (1.11.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (2023.7.22)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (3.4.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (2.3.7)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow->keras_cv_attention_models) (3.0.9)\nRequirement already satisfied: googleapis-common-protos in /opt/conda/lib/python3.10/site-packages (from tensorflow-metadata->tensorflow-datasets->keras_cv_attention_models) (1.59.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras_cv_attention_models) (3.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras_cv_attention_models import wave_mlp","metadata":{"execution":{"iopub.status.busy":"2024-01-03T12:30:50.445883Z","iopub.execute_input":"2024-01-03T12:30:50.446247Z","iopub.status.idle":"2024-01-03T12:30:50.450779Z","shell.execute_reply.started":"2024-01-03T12:30:50.446212Z","shell.execute_reply":"2024-01-03T12:30:50.449625Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import coatnet","metadata":{"execution":{"iopub.status.busy":"2024-01-02T22:13:31.350341Z","iopub.execute_input":"2024-01-02T22:13:31.350718Z","iopub.status.idle":"2024-01-02T22:13:31.355220Z","shell.execute_reply.started":"2024-01-02T22:13:31.350686Z","shell.execute_reply":"2024-01-02T22:13:31.354213Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(256, activation='relu')(x)  # Add your own dense layers\n    x = Dropout(0.5)(x)  # Add dropout for regularization\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\n\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = coatnet.CoAtNet0(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n#     head_output = custom_head(mm_headless.output, num_classes)\n    print(\"mm_headless.output shape:\", mm_headless.output_shape)\n    head_output = custom_head(mm_headless.output, num_classes)\n\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\n# custom_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-02T22:11:59.489705Z","iopub.execute_input":"2024-01-02T22:11:59.490526Z","iopub.status.idle":"2024-01-02T22:12:00.307998Z","shell.execute_reply.started":"2024-01-02T22:11:59.490462Z","shell.execute_reply":"2024-01-02T22:12:00.306522Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     27\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m26\u001b[39m  \u001b[38;5;66;03m# Adjust based on your task\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m custom_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodify_wave_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# custom_model.summary()\u001b[39;00m\n","Cell \u001b[0;32mIn[7], line 12\u001b[0m, in \u001b[0;36mmodify_wave_mlp\u001b[0;34m(input_shape, num_classes)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodify_wave_mlp\u001b[39m(input_shape, num_classes):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Load the WaveMLP model without the top layers (head)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     mm_headless \u001b[38;5;241m=\u001b[39m \u001b[43mcoatnet\u001b[49m\u001b[38;5;241m.\u001b[39mCoAtNet0(input_shape\u001b[38;5;241m=\u001b[39minput_shape, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Add your custom head\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     head_output = custom_head(mm_headless.output, num_classes)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmm_headless.output shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mm_headless\u001b[38;5;241m.\u001b[39moutput_shape)\n","\u001b[0;31mNameError\u001b[0m: name 'coatnet' is not defined"],"ename":"NameError","evalue":"name 'coatnet' is not defined","output_type":"error"}]},{"cell_type":"code","source":"def custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = coatnet.CoAtNet0(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n    head_output = custom_head(mm_headless.output, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-100]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:41:55.877170Z","iopub.execute_input":"2024-01-03T06:41:55.877510Z","iopub.status.idle":"2024-01-03T06:41:56.666981Z","shell.execute_reply.started":"2024-01-03T06:41:55.877483Z","shell.execute_reply":"2024-01-03T06:41:56.665758Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     31\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m26\u001b[39m  \u001b[38;5;66;03m# Adjust based on your task\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m custom_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodify_wave_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m custom_model\u001b[38;5;241m.\u001b[39msummary()\n","Cell \u001b[0;32mIn[7], line 12\u001b[0m, in \u001b[0;36mmodify_wave_mlp\u001b[0;34m(input_shape, num_classes)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodify_wave_mlp\u001b[39m(input_shape, num_classes):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Load the WaveMLP model without the top layers (head)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     mm_headless \u001b[38;5;241m=\u001b[39m \u001b[43mcoatnet\u001b[49m\u001b[38;5;241m.\u001b[39mCoAtNet0(input_shape\u001b[38;5;241m=\u001b[39minput_shape, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Add your custom head\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     head_output \u001b[38;5;241m=\u001b[39m custom_head(mm_headless\u001b[38;5;241m.\u001b[39moutput, num_classes)\n","\u001b[0;31mNameError\u001b[0m: name 'coatnet' is not defined"],"ename":"NameError","evalue":"name 'coatnet' is not defined","output_type":"error"}]},{"cell_type":"code","source":"def custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n    head_output = custom_head(mm_headless.output, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\n# custom_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T16:03:21.219136Z","iopub.execute_input":"2024-01-03T16:03:21.219993Z","iopub.status.idle":"2024-01-03T16:03:24.650878Z","shell.execute_reply.started":"2024-01-03T16:03:21.219956Z","shell.execute_reply":"2024-01-03T16:03:24.649841Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":">>>> Load pretrained from: /root/.keras/models/wavemlp_t_imagenet.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"mm_last_layer = custom_model .get_layer('avg_pool').output\n#out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\n#out = Dense(11, activation='softmax', name='prediction1')(out)\nmm_custom = Model(custom_model .input, mm_last_layer)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T16:03:29.617181Z","iopub.execute_input":"2024-01-03T16:03:29.617593Z","iopub.status.idle":"2024-01-03T16:03:29.657698Z","shell.execute_reply.started":"2024-01-03T16:03:29.617558Z","shell.execute_reply":"2024-01-03T16:03:29.656855Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\ninputs = keras.Input(shape=(128,128,3))\noutputs = layers.average([mm_custom(inputs)])\n\navg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\navg_ensemble_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T16:03:52.028662Z","iopub.execute_input":"2024-01-03T16:03:52.029285Z","iopub.status.idle":"2024-01-03T16:03:53.141838Z","shell.execute_reply.started":"2024-01-03T16:03:52.029252Z","shell.execute_reply":"2024-01-03T16:03:53.140938Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Model: \"model_6\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 128, 128, 3)]     0         \n                                                                 \n model_5 (Functional)        (None, 512)               16704736  \n                                                                 \n average_1 (Average)         (None, 512)               0         \n                                                                 \n=================================================================\nTotal params: 16,704,736\nTrainable params: 16,680,160\nNon-trainable params: 24,576\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 26\navg_ensemble_model_last_layer = avg_ensemble_model.get_layer('average_1').output\noutput_layer = Dense(num_classes, activation='softmax', name='output_1')(avg_ensemble_model_last_layer)\nfinal_model = Model(avg_ensemble_model.input, output_layer)\n\nfinal_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T16:04:05.908080Z","iopub.execute_input":"2024-01-03T16:04:05.908443Z","iopub.status.idle":"2024-01-03T16:04:05.971672Z","shell.execute_reply.started":"2024-01-03T16:04:05.908413Z","shell.execute_reply":"2024-01-03T16:04:05.970803Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Model: \"model_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 128, 128, 3)]     0         \n                                                                 \n model_5 (Functional)        (None, 512)               16704736  \n                                                                 \n average_1 (Average)         (None, 512)               0         \n                                                                 \n output_1 (Dense)            (None, 26)                13338     \n                                                                 \n=================================================================\nTotal params: 16,718,074\nTrainable params: 16,693,498\nNon-trainable params: 24,576\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = Adam(lr=1e-5)\nloss = 'categorical_crossentropy'\n# metrics = ['categorical_accuracy']\nmetrics = ['accuracy', 'categorical_accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), \n           tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), \n           tf.keras.metrics.FalseNegatives(), tfa.metrics.CohenKappa(num_classes = num_classes), \n           tfa.metrics.F1Score(num_classes = num_classes)]\n\nfinal_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T16:04:10.557596Z","iopub.execute_input":"2024-01-03T16:04:10.557990Z","iopub.status.idle":"2024-01-03T16:04:10.610556Z","shell.execute_reply.started":"2024-01-03T16:04:10.557961Z","shell.execute_reply":"2024-01-03T16:04:10.609639Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nlr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1,\n    patience=9, mode=\"max\", min_delta=0.0001, min_lr=0.00001, verbose=1)\ncheckpoint = ModelCheckpoint(filepath='Best_DenseNet201_v23.h5', save_best_only=True, monitor = 'val_accuracy', verbose=1)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, restore_best_weights=True)\n\ncallbacks = [lr, checkpoint, early_stopping]","metadata":{"execution":{"iopub.status.busy":"2024-01-03T16:04:15.275359Z","iopub.execute_input":"2024-01-03T16:04:15.276218Z","iopub.status.idle":"2024-01-03T16:04:15.289123Z","shell.execute_reply.started":"2024-01-03T16:04:15.276185Z","shell.execute_reply":"2024-01-03T16:04:15.288032Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"epochs = 30\n\nsteps_per_epoch = generator_train.n / batch_size\nsteps_test = generator_test.n / batch_size\n\nhistory = final_model.fit_generator(generator=generator_train,\n                                  epochs=epochs,\n                                  steps_per_epoch=steps_per_epoch,\n                                  validation_data=generator_test,\n                                  validation_steps=steps_test,\n                                   callbacks=callbacks, class_weight =class_weights)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T16:05:17.927335Z","iopub.execute_input":"2024-01-03T16:05:17.927693Z","iopub.status.idle":"2024-01-03T17:59:09.397313Z","shell.execute_reply.started":"2024-01-03T16:05:17.927665Z","shell.execute_reply":"2024-01-03T17:59:09.396530Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/2027776823.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n  history = final_model.fit_generator(generator=generator_train,\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30\n832/831 [==============================] - ETA: 0s - loss: 1.4661 - accuracy: 0.5553 - categorical_accuracy: 0.5553 - auc_1: 0.9419 - precision_1: 0.7560 - recall_1: 0.4006 - true_positives_1: 5331.0000 - true_negatives_1: 330954.0000 - false_positives_1: 1721.0000 - false_negatives_1: 7976.0000 - cohen_kappa: 0.5370 - f1_score: 0.5499\nEpoch 1: val_accuracy improved from -inf to 0.68290, saving model to Best_DenseNet201_v23.h5\n831/831 [==============================] - 330s 323ms/step - loss: 1.4661 - accuracy: 0.5553 - categorical_accuracy: 0.5553 - auc_1: 0.9419 - precision_1: 0.7560 - recall_1: 0.4006 - true_positives_1: 5331.0000 - true_negatives_1: 330954.0000 - false_positives_1: 1721.0000 - false_negatives_1: 7976.0000 - cohen_kappa: 0.5370 - f1_score: 0.5499 - val_loss: 1.0109 - val_accuracy: 0.6829 - val_categorical_accuracy: 0.6829 - val_auc_1: 0.9701 - val_precision_1: 0.8068 - val_recall_1: 0.6075 - val_true_positives_1: 1023.0000 - val_true_negatives_1: 41855.0000 - val_false_positives_1: 245.0000 - val_false_negatives_1: 661.0000 - val_cohen_kappa: 0.6695 - val_f1_score: 0.6781 - lr: 0.0010\nEpoch 2/30\n832/831 [==============================] - ETA: 0s - loss: 0.7351 - accuracy: 0.7601 - categorical_accuracy: 0.7601 - auc_1: 0.9832 - precision_1: 0.8412 - recall_1: 0.6860 - true_positives_1: 9129.0000 - true_negatives_1: 330952.0000 - false_positives_1: 1723.0000 - false_negatives_1: 4178.0000 - cohen_kappa: 0.7499 - f1_score: 0.7587\nEpoch 2: val_accuracy did not improve from 0.68290\n831/831 [==============================] - 250s 301ms/step - loss: 0.7351 - accuracy: 0.7601 - categorical_accuracy: 0.7601 - auc_1: 0.9832 - precision_1: 0.8412 - recall_1: 0.6860 - true_positives_1: 9129.0000 - true_negatives_1: 330952.0000 - false_positives_1: 1723.0000 - false_negatives_1: 4178.0000 - cohen_kappa: 0.7499 - f1_score: 0.7587 - val_loss: 2.2549 - val_accuracy: 0.4768 - val_categorical_accuracy: 0.4768 - val_auc_1: 0.8883 - val_precision_1: 0.5136 - val_recall_1: 0.4382 - val_true_positives_1: 738.0000 - val_true_negatives_1: 41401.0000 - val_false_positives_1: 699.0000 - val_false_negatives_1: 946.0000 - val_cohen_kappa: 0.4558 - val_f1_score: 0.5104 - lr: 0.0010\nEpoch 3/30\n832/831 [==============================] - ETA: 0s - loss: 0.5238 - accuracy: 0.8269 - categorical_accuracy: 0.8269 - auc_1: 0.9901 - precision_1: 0.8749 - recall_1: 0.7794 - true_positives_1: 10371.0000 - true_negatives_1: 331192.0000 - false_positives_1: 1483.0000 - false_negatives_1: 2936.0000 - cohen_kappa: 0.8195 - f1_score: 0.8264\nEpoch 3: val_accuracy improved from 0.68290 to 0.84561, saving model to Best_DenseNet201_v23.h5\n831/831 [==============================] - 249s 299ms/step - loss: 0.5238 - accuracy: 0.8269 - categorical_accuracy: 0.8269 - auc_1: 0.9901 - precision_1: 0.8749 - recall_1: 0.7794 - true_positives_1: 10371.0000 - true_negatives_1: 331192.0000 - false_positives_1: 1483.0000 - false_negatives_1: 2936.0000 - cohen_kappa: 0.8195 - f1_score: 0.8264 - val_loss: 0.4733 - val_accuracy: 0.8456 - val_categorical_accuracy: 0.8456 - val_auc_1: 0.9917 - val_precision_1: 0.8900 - val_recall_1: 0.8118 - val_true_positives_1: 1367.0000 - val_true_negatives_1: 41931.0000 - val_false_positives_1: 169.0000 - val_false_negatives_1: 317.0000 - val_cohen_kappa: 0.8391 - val_f1_score: 0.8453 - lr: 0.0010\nEpoch 4/30\n832/831 [==============================] - ETA: 0s - loss: 0.4394 - accuracy: 0.8527 - categorical_accuracy: 0.8527 - auc_1: 0.9922 - precision_1: 0.8911 - recall_1: 0.8195 - true_positives_1: 10905.0000 - true_negatives_1: 331343.0000 - false_positives_1: 1332.0000 - false_negatives_1: 2402.0000 - cohen_kappa: 0.8464 - f1_score: 0.8511\nEpoch 4: val_accuracy did not improve from 0.84561\n831/831 [==============================] - 254s 305ms/step - loss: 0.4394 - accuracy: 0.8527 - categorical_accuracy: 0.8527 - auc_1: 0.9922 - precision_1: 0.8911 - recall_1: 0.8195 - true_positives_1: 10905.0000 - true_negatives_1: 331343.0000 - false_positives_1: 1332.0000 - false_negatives_1: 2402.0000 - cohen_kappa: 0.8464 - f1_score: 0.8511 - val_loss: 2.4044 - val_accuracy: 0.5042 - val_categorical_accuracy: 0.5042 - val_auc_1: 0.8746 - val_precision_1: 0.5622 - val_recall_1: 0.4697 - val_true_positives_1: 791.0000 - val_true_negatives_1: 41484.0000 - val_false_positives_1: 616.0000 - val_false_negatives_1: 893.0000 - val_cohen_kappa: 0.4843 - val_f1_score: 0.5011 - lr: 0.0010\nEpoch 5/30\n832/831 [==============================] - ETA: 0s - loss: 0.3826 - accuracy: 0.8698 - categorical_accuracy: 0.8698 - auc_1: 0.9931 - precision_1: 0.9012 - recall_1: 0.8432 - true_positives_1: 11221.0000 - true_negatives_1: 331445.0000 - false_positives_1: 1230.0000 - false_negatives_1: 2086.0000 - cohen_kappa: 0.8642 - f1_score: 0.8682\nEpoch 5: val_accuracy did not improve from 0.84561\n831/831 [==============================] - 245s 294ms/step - loss: 0.3826 - accuracy: 0.8698 - categorical_accuracy: 0.8698 - auc_1: 0.9931 - precision_1: 0.9012 - recall_1: 0.8432 - true_positives_1: 11221.0000 - true_negatives_1: 331445.0000 - false_positives_1: 1230.0000 - false_negatives_1: 2086.0000 - cohen_kappa: 0.8642 - f1_score: 0.8682 - val_loss: 1.6175 - val_accuracy: 0.5707 - val_categorical_accuracy: 0.5707 - val_auc_1: 0.9309 - val_precision_1: 0.6558 - val_recall_1: 0.5160 - val_true_positives_1: 869.0000 - val_true_negatives_1: 41644.0000 - val_false_positives_1: 456.0000 - val_false_negatives_1: 815.0000 - val_cohen_kappa: 0.5532 - val_f1_score: 0.5308 - lr: 0.0010\nEpoch 6/30\n832/831 [==============================] - ETA: 0s - loss: 0.3424 - accuracy: 0.8834 - categorical_accuracy: 0.8834 - auc_1: 0.9946 - precision_1: 0.9092 - recall_1: 0.8587 - true_positives_1: 11427.0000 - true_negatives_1: 331534.0000 - false_positives_1: 1141.0000 - false_negatives_1: 1880.0000 - cohen_kappa: 0.8784 - f1_score: 0.8815\nEpoch 6: val_accuracy did not improve from 0.84561\n831/831 [==============================] - 251s 302ms/step - loss: 0.3424 - accuracy: 0.8834 - categorical_accuracy: 0.8834 - auc_1: 0.9946 - precision_1: 0.9092 - recall_1: 0.8587 - true_positives_1: 11427.0000 - true_negatives_1: 331534.0000 - false_positives_1: 1141.0000 - false_negatives_1: 1880.0000 - cohen_kappa: 0.8784 - f1_score: 0.8815 - val_loss: 0.4954 - val_accuracy: 0.8438 - val_categorical_accuracy: 0.8438 - val_auc_1: 0.9882 - val_precision_1: 0.8937 - val_recall_1: 0.8189 - val_true_positives_1: 1379.0000 - val_true_negatives_1: 41936.0000 - val_false_positives_1: 164.0000 - val_false_negatives_1: 305.0000 - val_cohen_kappa: 0.8373 - val_f1_score: 0.8457 - lr: 0.0010\nEpoch 7/30\n832/831 [==============================] - ETA: 0s - loss: 0.3114 - accuracy: 0.8923 - categorical_accuracy: 0.8923 - auc_1: 0.9953 - precision_1: 0.9168 - recall_1: 0.8729 - true_positives_1: 11616.0000 - true_negatives_1: 331621.0000 - false_positives_1: 1054.0000 - false_negatives_1: 1691.0000 - cohen_kappa: 0.8877 - f1_score: 0.8902\nEpoch 7: val_accuracy did not improve from 0.84561\n831/831 [==============================] - 244s 294ms/step - loss: 0.3114 - accuracy: 0.8923 - categorical_accuracy: 0.8923 - auc_1: 0.9953 - precision_1: 0.9168 - recall_1: 0.8729 - true_positives_1: 11616.0000 - true_negatives_1: 331621.0000 - false_positives_1: 1054.0000 - false_negatives_1: 1691.0000 - cohen_kappa: 0.8877 - f1_score: 0.8902 - val_loss: 0.7370 - val_accuracy: 0.7785 - val_categorical_accuracy: 0.7785 - val_auc_1: 0.9797 - val_precision_1: 0.8111 - val_recall_1: 0.7393 - val_true_positives_1: 1245.0000 - val_true_negatives_1: 41810.0000 - val_false_positives_1: 290.0000 - val_false_negatives_1: 439.0000 - val_cohen_kappa: 0.7690 - val_f1_score: 0.7894 - lr: 0.0010\nEpoch 8/30\n832/831 [==============================] - ETA: 0s - loss: 0.2622 - accuracy: 0.9079 - categorical_accuracy: 0.9079 - auc_1: 0.9964 - precision_1: 0.9263 - recall_1: 0.8914 - true_positives_1: 11862.0000 - true_negatives_1: 331731.0000 - false_positives_1: 944.0000 - false_negatives_1: 1445.0000 - cohen_kappa: 0.9040 - f1_score: 0.9058\nEpoch 8: val_accuracy did not improve from 0.84561\n831/831 [==============================] - 246s 296ms/step - loss: 0.2622 - accuracy: 0.9079 - categorical_accuracy: 0.9079 - auc_1: 0.9964 - precision_1: 0.9263 - recall_1: 0.8914 - true_positives_1: 11862.0000 - true_negatives_1: 331731.0000 - false_positives_1: 944.0000 - false_negatives_1: 1445.0000 - cohen_kappa: 0.9040 - f1_score: 0.9058 - val_loss: 2.2953 - val_accuracy: 0.5439 - val_categorical_accuracy: 0.5439 - val_auc_1: 0.8850 - val_precision_1: 0.6096 - val_recall_1: 0.5036 - val_true_positives_1: 848.0000 - val_true_negatives_1: 41557.0000 - val_false_positives_1: 543.0000 - val_false_negatives_1: 836.0000 - val_cohen_kappa: 0.5249 - val_f1_score: 0.5027 - lr: 0.0010\nEpoch 9/30\n832/831 [==============================] - ETA: 0s - loss: 0.2661 - accuracy: 0.9073 - categorical_accuracy: 0.9073 - auc_1: 0.9961 - precision_1: 0.9246 - recall_1: 0.8913 - true_positives_1: 11860.0000 - true_negatives_1: 331708.0000 - false_positives_1: 967.0000 - false_negatives_1: 1447.0000 - cohen_kappa: 0.9033 - f1_score: 0.9052\nEpoch 9: val_accuracy improved from 0.84561 to 0.93171, saving model to Best_DenseNet201_v23.h5\n831/831 [==============================] - 249s 300ms/step - loss: 0.2661 - accuracy: 0.9073 - categorical_accuracy: 0.9073 - auc_1: 0.9961 - precision_1: 0.9246 - recall_1: 0.8913 - true_positives_1: 11860.0000 - true_negatives_1: 331708.0000 - false_positives_1: 967.0000 - false_negatives_1: 1447.0000 - cohen_kappa: 0.9033 - f1_score: 0.9052 - val_loss: 0.1790 - val_accuracy: 0.9317 - val_categorical_accuracy: 0.9317 - val_auc_1: 0.9979 - val_precision_1: 0.9441 - val_recall_1: 0.9234 - val_true_positives_1: 1555.0000 - val_true_negatives_1: 42008.0000 - val_false_positives_1: 92.0000 - val_false_negatives_1: 129.0000 - val_cohen_kappa: 0.9288 - val_f1_score: 0.9296 - lr: 0.0010\nEpoch 10/30\n832/831 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.9140 - categorical_accuracy: 0.9140 - auc_1: 0.9965 - precision_1: 0.9303 - recall_1: 0.8999 - true_positives_1: 11975.0000 - true_negatives_1: 331778.0000 - false_positives_1: 897.0000 - false_negatives_1: 1332.0000 - cohen_kappa: 0.9103 - f1_score: 0.9125\nEpoch 10: val_accuracy did not improve from 0.93171\n831/831 [==============================] - 250s 300ms/step - loss: 0.2433 - accuracy: 0.9140 - categorical_accuracy: 0.9140 - auc_1: 0.9965 - precision_1: 0.9303 - recall_1: 0.8999 - true_positives_1: 11975.0000 - true_negatives_1: 331778.0000 - false_positives_1: 897.0000 - false_negatives_1: 1332.0000 - cohen_kappa: 0.9103 - f1_score: 0.9125 - val_loss: 0.2900 - val_accuracy: 0.8996 - val_categorical_accuracy: 0.8996 - val_auc_1: 0.9952 - val_precision_1: 0.9255 - val_recall_1: 0.8854 - val_true_positives_1: 1491.0000 - val_true_negatives_1: 41980.0000 - val_false_positives_1: 120.0000 - val_false_negatives_1: 193.0000 - val_cohen_kappa: 0.8954 - val_f1_score: 0.8952 - lr: 0.0010\nEpoch 11/30\n832/831 [==============================] - ETA: 0s - loss: 0.2303 - accuracy: 0.9164 - categorical_accuracy: 0.9164 - auc_1: 0.9972 - precision_1: 0.9328 - recall_1: 0.9039 - true_positives_1: 12028.0000 - true_negatives_1: 331808.0000 - false_positives_1: 867.0000 - false_negatives_1: 1279.0000 - cohen_kappa: 0.9128 - f1_score: 0.9141\nEpoch 11: val_accuracy did not improve from 0.93171\n831/831 [==============================] - 247s 297ms/step - loss: 0.2303 - accuracy: 0.9164 - categorical_accuracy: 0.9164 - auc_1: 0.9972 - precision_1: 0.9328 - recall_1: 0.9039 - true_positives_1: 12028.0000 - true_negatives_1: 331808.0000 - false_positives_1: 867.0000 - false_negatives_1: 1279.0000 - cohen_kappa: 0.9128 - f1_score: 0.9141 - val_loss: 0.2638 - val_accuracy: 0.9086 - val_categorical_accuracy: 0.9086 - val_auc_1: 0.9958 - val_precision_1: 0.9216 - val_recall_1: 0.9002 - val_true_positives_1: 1516.0000 - val_true_negatives_1: 41971.0000 - val_false_positives_1: 129.0000 - val_false_negatives_1: 168.0000 - val_cohen_kappa: 0.9046 - val_f1_score: 0.9021 - lr: 0.0010\nEpoch 12/30\n832/831 [==============================] - ETA: 0s - loss: 0.2218 - accuracy: 0.9168 - categorical_accuracy: 0.9168 - auc_1: 0.9975 - precision_1: 0.9325 - recall_1: 0.9054 - true_positives_1: 12048.0000 - true_negatives_1: 331803.0000 - false_positives_1: 872.0000 - false_negatives_1: 1259.0000 - cohen_kappa: 0.9132 - f1_score: 0.9145\nEpoch 12: val_accuracy did not improve from 0.93171\n831/831 [==============================] - 240s 288ms/step - loss: 0.2218 - accuracy: 0.9168 - categorical_accuracy: 0.9168 - auc_1: 0.9975 - precision_1: 0.9325 - recall_1: 0.9054 - true_positives_1: 12048.0000 - true_negatives_1: 331803.0000 - false_positives_1: 872.0000 - false_negatives_1: 1259.0000 - cohen_kappa: 0.9132 - f1_score: 0.9145 - val_loss: 0.2980 - val_accuracy: 0.8985 - val_categorical_accuracy: 0.8985 - val_auc_1: 0.9960 - val_precision_1: 0.9126 - val_recall_1: 0.8872 - val_true_positives_1: 1494.0000 - val_true_negatives_1: 41957.0000 - val_false_positives_1: 143.0000 - val_false_negatives_1: 190.0000 - val_cohen_kappa: 0.8940 - val_f1_score: 0.8995 - lr: 0.0010\nEpoch 13/30\n832/831 [==============================] - ETA: 0s - loss: 0.2094 - accuracy: 0.9253 - categorical_accuracy: 0.9253 - auc_1: 0.9974 - precision_1: 0.9394 - recall_1: 0.9140 - true_positives_1: 12162.0000 - true_negatives_1: 331891.0000 - false_positives_1: 784.0000 - false_negatives_1: 1145.0000 - cohen_kappa: 0.9221 - f1_score: 0.9227\nEpoch 13: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 13: val_accuracy did not improve from 0.93171\n831/831 [==============================] - 246s 295ms/step - loss: 0.2094 - accuracy: 0.9253 - categorical_accuracy: 0.9253 - auc_1: 0.9974 - precision_1: 0.9394 - recall_1: 0.9140 - true_positives_1: 12162.0000 - true_negatives_1: 331891.0000 - false_positives_1: 784.0000 - false_negatives_1: 1145.0000 - cohen_kappa: 0.9221 - f1_score: 0.9227 - val_loss: 0.2087 - val_accuracy: 0.9317 - val_categorical_accuracy: 0.9317 - val_auc_1: 0.9968 - val_precision_1: 0.9392 - val_recall_1: 0.9264 - val_true_positives_1: 1560.0000 - val_true_negatives_1: 41999.0000 - val_false_positives_1: 101.0000 - val_false_negatives_1: 124.0000 - val_cohen_kappa: 0.9288 - val_f1_score: 0.9264 - lr: 0.0010\nEpoch 14/30\n832/831 [==============================] - ETA: 0s - loss: 0.0891 - accuracy: 0.9653 - categorical_accuracy: 0.9653 - auc_1: 0.9996 - precision_1: 0.9698 - recall_1: 0.9617 - true_positives_1: 12798.0000 - true_negatives_1: 332276.0000 - false_positives_1: 399.0000 - false_negatives_1: 509.0000 - cohen_kappa: 0.9638 - f1_score: 0.9627\nEpoch 14: val_accuracy improved from 0.93171 to 0.96971, saving model to Best_DenseNet201_v23.h5\n831/831 [==============================] - 251s 302ms/step - loss: 0.0891 - accuracy: 0.9653 - categorical_accuracy: 0.9653 - auc_1: 0.9996 - precision_1: 0.9698 - recall_1: 0.9617 - true_positives_1: 12798.0000 - true_negatives_1: 332276.0000 - false_positives_1: 399.0000 - false_negatives_1: 509.0000 - cohen_kappa: 0.9638 - f1_score: 0.9627 - val_loss: 0.0606 - val_accuracy: 0.9697 - val_categorical_accuracy: 0.9697 - val_auc_1: 0.9996 - val_precision_1: 0.9703 - val_recall_1: 0.9685 - val_true_positives_1: 1631.0000 - val_true_negatives_1: 42050.0000 - val_false_positives_1: 50.0000 - val_false_negatives_1: 53.0000 - val_cohen_kappa: 0.9684 - val_f1_score: 0.9650 - lr: 1.0000e-04\nEpoch 15/30\n832/831 [==============================] - ETA: 0s - loss: 0.0623 - accuracy: 0.9728 - categorical_accuracy: 0.9728 - auc_1: 0.9996 - precision_1: 0.9747 - recall_1: 0.9715 - true_positives_1: 12928.0000 - true_negatives_1: 332339.0000 - false_positives_1: 336.0000 - false_negatives_1: 379.0000 - cohen_kappa: 0.9716 - f1_score: 0.9707\nEpoch 15: val_accuracy improved from 0.96971 to 0.97090, saving model to Best_DenseNet201_v23.h5\n831/831 [==============================] - 253s 304ms/step - loss: 0.0623 - accuracy: 0.9728 - categorical_accuracy: 0.9728 - auc_1: 0.9996 - precision_1: 0.9747 - recall_1: 0.9715 - true_positives_1: 12928.0000 - true_negatives_1: 332339.0000 - false_positives_1: 336.0000 - false_negatives_1: 379.0000 - cohen_kappa: 0.9716 - f1_score: 0.9707 - val_loss: 0.0508 - val_accuracy: 0.9709 - val_categorical_accuracy: 0.9709 - val_auc_1: 0.9999 - val_precision_1: 0.9715 - val_recall_1: 0.9703 - val_true_positives_1: 1634.0000 - val_true_negatives_1: 42052.0000 - val_false_positives_1: 48.0000 - val_false_negatives_1: 50.0000 - val_cohen_kappa: 0.9696 - val_f1_score: 0.9664 - lr: 1.0000e-04\nEpoch 16/30\n832/831 [==============================] - ETA: 0s - loss: 0.0569 - accuracy: 0.9751 - categorical_accuracy: 0.9751 - auc_1: 0.9997 - precision_1: 0.9766 - recall_1: 0.9740 - true_positives_1: 12961.0000 - true_negatives_1: 332364.0000 - false_positives_1: 311.0000 - false_negatives_1: 346.0000 - cohen_kappa: 0.9740 - f1_score: 0.9723\nEpoch 16: val_accuracy did not improve from 0.97090\n831/831 [==============================] - 250s 301ms/step - loss: 0.0569 - accuracy: 0.9751 - categorical_accuracy: 0.9751 - auc_1: 0.9997 - precision_1: 0.9766 - recall_1: 0.9740 - true_positives_1: 12961.0000 - true_negatives_1: 332364.0000 - false_positives_1: 311.0000 - false_negatives_1: 346.0000 - cohen_kappa: 0.9740 - f1_score: 0.9723 - val_loss: 0.0516 - val_accuracy: 0.9697 - val_categorical_accuracy: 0.9697 - val_auc_1: 0.9999 - val_precision_1: 0.9697 - val_recall_1: 0.9691 - val_true_positives_1: 1632.0000 - val_true_negatives_1: 42049.0000 - val_false_positives_1: 51.0000 - val_false_negatives_1: 52.0000 - val_cohen_kappa: 0.9684 - val_f1_score: 0.9645 - lr: 1.0000e-04\nEpoch 17/30\n832/831 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9768 - categorical_accuracy: 0.9768 - auc_1: 0.9999 - precision_1: 0.9776 - recall_1: 0.9762 - true_positives_1: 12990.0000 - true_negatives_1: 332377.0000 - false_positives_1: 298.0000 - false_negatives_1: 317.0000 - cohen_kappa: 0.9758 - f1_score: 0.9742\nEpoch 17: val_accuracy did not improve from 0.97090\n831/831 [==============================] - 252s 303ms/step - loss: 0.0492 - accuracy: 0.9768 - categorical_accuracy: 0.9768 - auc_1: 0.9999 - precision_1: 0.9776 - recall_1: 0.9762 - true_positives_1: 12990.0000 - true_negatives_1: 332377.0000 - false_positives_1: 298.0000 - false_negatives_1: 317.0000 - cohen_kappa: 0.9758 - f1_score: 0.9742 - val_loss: 0.0463 - val_accuracy: 0.9691 - val_categorical_accuracy: 0.9691 - val_auc_1: 0.9999 - val_precision_1: 0.9703 - val_recall_1: 0.9685 - val_true_positives_1: 1631.0000 - val_true_negatives_1: 42050.0000 - val_false_positives_1: 50.0000 - val_false_negatives_1: 53.0000 - val_cohen_kappa: 0.9678 - val_f1_score: 0.9627 - lr: 1.0000e-04\nEpoch 18/30\n832/831 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9784 - categorical_accuracy: 0.9784 - auc_1: 0.9999 - precision_1: 0.9791 - recall_1: 0.9777 - true_positives_1: 13010.0000 - true_negatives_1: 332397.0000 - false_positives_1: 278.0000 - false_negatives_1: 297.0000 - cohen_kappa: 0.9774 - f1_score: 0.9759\nEpoch 18: val_accuracy did not improve from 0.97090\n831/831 [==============================] - 255s 306ms/step - loss: 0.0467 - accuracy: 0.9784 - categorical_accuracy: 0.9784 - auc_1: 0.9999 - precision_1: 0.9791 - recall_1: 0.9777 - true_positives_1: 13010.0000 - true_negatives_1: 332397.0000 - false_positives_1: 278.0000 - false_negatives_1: 297.0000 - cohen_kappa: 0.9774 - f1_score: 0.9759 - val_loss: 0.0524 - val_accuracy: 0.9709 - val_categorical_accuracy: 0.9709 - val_auc_1: 0.9996 - val_precision_1: 0.9720 - val_recall_1: 0.9703 - val_true_positives_1: 1634.0000 - val_true_negatives_1: 42053.0000 - val_false_positives_1: 47.0000 - val_false_negatives_1: 50.0000 - val_cohen_kappa: 0.9696 - val_f1_score: 0.9674 - lr: 1.0000e-04\nEpoch 19/30\n832/831 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9795 - categorical_accuracy: 0.9795 - auc_1: 0.9998 - precision_1: 0.9800 - recall_1: 0.9789 - true_positives_1: 13026.0000 - true_negatives_1: 332409.0000 - false_positives_1: 266.0000 - false_negatives_1: 281.0000 - cohen_kappa: 0.9786 - f1_score: 0.9768\nEpoch 19: val_accuracy improved from 0.97090 to 0.97209, saving model to Best_DenseNet201_v23.h5\n831/831 [==============================] - 259s 311ms/step - loss: 0.0442 - accuracy: 0.9795 - categorical_accuracy: 0.9795 - auc_1: 0.9998 - precision_1: 0.9800 - recall_1: 0.9789 - true_positives_1: 13026.0000 - true_negatives_1: 332409.0000 - false_positives_1: 266.0000 - false_negatives_1: 281.0000 - cohen_kappa: 0.9786 - f1_score: 0.9768 - val_loss: 0.0578 - val_accuracy: 0.9721 - val_categorical_accuracy: 0.9721 - val_auc_1: 0.9993 - val_precision_1: 0.9721 - val_recall_1: 0.9721 - val_true_positives_1: 1637.0000 - val_true_negatives_1: 42053.0000 - val_false_positives_1: 47.0000 - val_false_negatives_1: 47.0000 - val_cohen_kappa: 0.9709 - val_f1_score: 0.9671 - lr: 1.0000e-04\nEpoch 20/30\n832/831 [==============================] - ETA: 0s - loss: 0.0423 - accuracy: 0.9807 - categorical_accuracy: 0.9807 - auc_1: 0.9999 - precision_1: 0.9810 - recall_1: 0.9804 - true_positives_1: 13046.0000 - true_negatives_1: 332422.0000 - false_positives_1: 253.0000 - false_negatives_1: 261.0000 - cohen_kappa: 0.9799 - f1_score: 0.9780\nEpoch 20: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 259s 312ms/step - loss: 0.0423 - accuracy: 0.9807 - categorical_accuracy: 0.9807 - auc_1: 0.9999 - precision_1: 0.9810 - recall_1: 0.9804 - true_positives_1: 13046.0000 - true_negatives_1: 332422.0000 - false_positives_1: 253.0000 - false_negatives_1: 261.0000 - cohen_kappa: 0.9799 - f1_score: 0.9780 - val_loss: 0.0590 - val_accuracy: 0.9697 - val_categorical_accuracy: 0.9697 - val_auc_1: 0.9993 - val_precision_1: 0.9697 - val_recall_1: 0.9691 - val_true_positives_1: 1632.0000 - val_true_negatives_1: 42049.0000 - val_false_positives_1: 51.0000 - val_false_negatives_1: 52.0000 - val_cohen_kappa: 0.9684 - val_f1_score: 0.9644 - lr: 1.0000e-04\nEpoch 21/30\n832/831 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9793 - categorical_accuracy: 0.9793 - auc_1: 0.9999 - precision_1: 0.9799 - recall_1: 0.9790 - true_positives_1: 13027.0000 - true_negatives_1: 332408.0000 - false_positives_1: 267.0000 - false_negatives_1: 280.0000 - cohen_kappa: 0.9784 - f1_score: 0.9765\nEpoch 21: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 255s 307ms/step - loss: 0.0436 - accuracy: 0.9793 - categorical_accuracy: 0.9793 - auc_1: 0.9999 - precision_1: 0.9799 - recall_1: 0.9790 - true_positives_1: 13027.0000 - true_negatives_1: 332408.0000 - false_positives_1: 267.0000 - false_negatives_1: 280.0000 - cohen_kappa: 0.9784 - f1_score: 0.9765 - val_loss: 0.0488 - val_accuracy: 0.9721 - val_categorical_accuracy: 0.9721 - val_auc_1: 0.9996 - val_precision_1: 0.9721 - val_recall_1: 0.9721 - val_true_positives_1: 1637.0000 - val_true_negatives_1: 42053.0000 - val_false_positives_1: 47.0000 - val_false_negatives_1: 47.0000 - val_cohen_kappa: 0.9709 - val_f1_score: 0.9671 - lr: 1.0000e-04\nEpoch 22/30\n832/831 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9805 - categorical_accuracy: 0.9805 - auc_1: 0.9999 - precision_1: 0.9809 - recall_1: 0.9802 - true_positives_1: 13044.0000 - true_negatives_1: 332421.0000 - false_positives_1: 254.0000 - false_negatives_1: 263.0000 - cohen_kappa: 0.9797 - f1_score: 0.9780\nEpoch 22: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n\nEpoch 22: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 250s 300ms/step - loss: 0.0392 - accuracy: 0.9805 - categorical_accuracy: 0.9805 - auc_1: 0.9999 - precision_1: 0.9809 - recall_1: 0.9802 - true_positives_1: 13044.0000 - true_negatives_1: 332421.0000 - false_positives_1: 254.0000 - false_negatives_1: 263.0000 - cohen_kappa: 0.9797 - f1_score: 0.9780 - val_loss: 0.0529 - val_accuracy: 0.9709 - val_categorical_accuracy: 0.9709 - val_auc_1: 0.9993 - val_precision_1: 0.9709 - val_recall_1: 0.9709 - val_true_positives_1: 1635.0000 - val_true_negatives_1: 42051.0000 - val_false_positives_1: 49.0000 - val_false_negatives_1: 49.0000 - val_cohen_kappa: 0.9696 - val_f1_score: 0.9662 - lr: 1.0000e-04\nEpoch 23/30\n832/831 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9822 - categorical_accuracy: 0.9822 - auc_1: 0.9999 - precision_1: 0.9826 - recall_1: 0.9818 - true_positives_1: 13065.0000 - true_negatives_1: 332443.0000 - false_positives_1: 232.0000 - false_negatives_1: 242.0000 - cohen_kappa: 0.9814 - f1_score: 0.9798\nEpoch 23: val_accuracy improved from 0.97209 to 0.97387, saving model to Best_DenseNet201_v23.h5\n831/831 [==============================] - 255s 306ms/step - loss: 0.0384 - accuracy: 0.9822 - categorical_accuracy: 0.9822 - auc_1: 0.9999 - precision_1: 0.9826 - recall_1: 0.9818 - true_positives_1: 13065.0000 - true_negatives_1: 332443.0000 - false_positives_1: 232.0000 - false_negatives_1: 242.0000 - cohen_kappa: 0.9814 - f1_score: 0.9798 - val_loss: 0.0503 - val_accuracy: 0.9739 - val_categorical_accuracy: 0.9739 - val_auc_1: 0.9990 - val_precision_1: 0.9739 - val_recall_1: 0.9739 - val_true_positives_1: 1640.0000 - val_true_negatives_1: 42056.0000 - val_false_positives_1: 44.0000 - val_false_negatives_1: 44.0000 - val_cohen_kappa: 0.9727 - val_f1_score: 0.9695 - lr: 1.0000e-05\nEpoch 24/30\n832/831 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9825 - categorical_accuracy: 0.9825 - auc_1: 1.0000 - precision_1: 0.9830 - recall_1: 0.9821 - true_positives_1: 13069.0000 - true_negatives_1: 332449.0000 - false_positives_1: 226.0000 - false_negatives_1: 238.0000 - cohen_kappa: 0.9817 - f1_score: 0.9797\nEpoch 24: val_accuracy did not improve from 0.97387\n831/831 [==============================] - 250s 300ms/step - loss: 0.0370 - accuracy: 0.9825 - categorical_accuracy: 0.9825 - auc_1: 1.0000 - precision_1: 0.9830 - recall_1: 0.9821 - true_positives_1: 13069.0000 - true_negatives_1: 332449.0000 - false_positives_1: 226.0000 - false_negatives_1: 238.0000 - cohen_kappa: 0.9817 - f1_score: 0.9797 - val_loss: 0.0530 - val_accuracy: 0.9739 - val_categorical_accuracy: 0.9739 - val_auc_1: 0.9993 - val_precision_1: 0.9739 - val_recall_1: 0.9739 - val_true_positives_1: 1640.0000 - val_true_negatives_1: 42056.0000 - val_false_positives_1: 44.0000 - val_false_negatives_1: 44.0000 - val_cohen_kappa: 0.9727 - val_f1_score: 0.9688 - lr: 1.0000e-05\nEpoch 25/30\n832/831 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9814 - categorical_accuracy: 0.9814 - auc_1: 0.9999 - precision_1: 0.9818 - recall_1: 0.9813 - true_positives_1: 13058.0000 - true_negatives_1: 332433.0000 - false_positives_1: 242.0000 - false_negatives_1: 249.0000 - cohen_kappa: 0.9806 - f1_score: 0.9787\nEpoch 25: val_accuracy did not improve from 0.97387\n831/831 [==============================] - 250s 300ms/step - loss: 0.0344 - accuracy: 0.9814 - categorical_accuracy: 0.9814 - auc_1: 0.9999 - precision_1: 0.9818 - recall_1: 0.9813 - true_positives_1: 13058.0000 - true_negatives_1: 332433.0000 - false_positives_1: 242.0000 - false_negatives_1: 249.0000 - cohen_kappa: 0.9806 - f1_score: 0.9787 - val_loss: 0.0492 - val_accuracy: 0.9721 - val_categorical_accuracy: 0.9721 - val_auc_1: 0.9993 - val_precision_1: 0.9721 - val_recall_1: 0.9721 - val_true_positives_1: 1637.0000 - val_true_negatives_1: 42053.0000 - val_false_positives_1: 47.0000 - val_false_negatives_1: 47.0000 - val_cohen_kappa: 0.9709 - val_f1_score: 0.9668 - lr: 1.0000e-05\nEpoch 26/30\n832/831 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9836 - categorical_accuracy: 0.9836 - auc_1: 1.0000 - precision_1: 0.9838 - recall_1: 0.9835 - true_positives_1: 13087.0000 - true_negatives_1: 332460.0000 - false_positives_1: 215.0000 - false_negatives_1: 220.0000 - cohen_kappa: 0.9829 - f1_score: 0.9811\nEpoch 26: val_accuracy did not improve from 0.97387\n831/831 [==============================] - 242s 291ms/step - loss: 0.0324 - accuracy: 0.9836 - categorical_accuracy: 0.9836 - auc_1: 1.0000 - precision_1: 0.9838 - recall_1: 0.9835 - true_positives_1: 13087.0000 - true_negatives_1: 332460.0000 - false_positives_1: 215.0000 - false_negatives_1: 220.0000 - cohen_kappa: 0.9829 - f1_score: 0.9811 - val_loss: 0.0521 - val_accuracy: 0.9727 - val_categorical_accuracy: 0.9727 - val_auc_1: 0.9993 - val_precision_1: 0.9727 - val_recall_1: 0.9727 - val_true_positives_1: 1638.0000 - val_true_negatives_1: 42054.0000 - val_false_positives_1: 46.0000 - val_false_negatives_1: 46.0000 - val_cohen_kappa: 0.9715 - val_f1_score: 0.9676 - lr: 1.0000e-05\nEpoch 27/30\n832/831 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9841 - categorical_accuracy: 0.9841 - auc_1: 1.0000 - precision_1: 0.9842 - recall_1: 0.9837 - true_positives_1: 13090.0000 - true_negatives_1: 332465.0000 - false_positives_1: 210.0000 - false_negatives_1: 217.0000 - cohen_kappa: 0.9834 - f1_score: 0.9814\nEpoch 27: val_accuracy did not improve from 0.97387\nRestoring model weights from the end of the best epoch: 17.\n831/831 [==============================] - 238s 286ms/step - loss: 0.0338 - accuracy: 0.9841 - categorical_accuracy: 0.9841 - auc_1: 1.0000 - precision_1: 0.9842 - recall_1: 0.9837 - true_positives_1: 13090.0000 - true_negatives_1: 332465.0000 - false_positives_1: 210.0000 - false_negatives_1: 217.0000 - cohen_kappa: 0.9834 - f1_score: 0.9814 - val_loss: 0.0515 - val_accuracy: 0.9721 - val_categorical_accuracy: 0.9721 - val_auc_1: 0.9993 - val_precision_1: 0.9721 - val_recall_1: 0.9721 - val_true_positives_1: 1637.0000 - val_true_negatives_1: 42053.0000 - val_false_positives_1: 47.0000 - val_false_negatives_1: 47.0000 - val_cohen_kappa: 0.9709 - val_f1_score: 0.9674 - lr: 1.0000e-05\nEpoch 27: early stopping\n","output_type":"stream"}]},{"cell_type":"code","source":"features = final_model.predict_generator(generator_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T17:59:09.405515Z","iopub.execute_input":"2024-01-03T17:59:09.405754Z","iopub.status.idle":"2024-01-03T17:59:29.144918Z","shell.execute_reply.started":"2024-01-03T17:59:09.405732Z","shell.execute_reply":"2024-01-03T17:59:29.144064Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/1249813240.py:1: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n  features = final_model.predict_generator(generator_test)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-01-03T17:59:09.399276Z","iopub.execute_input":"2024-01-03T17:59:09.399657Z","iopub.status.idle":"2024-01-03T17:59:09.404471Z","shell.execute_reply.started":"2024-01-03T17:59:09.399620Z","shell.execute_reply":"2024-01-03T17:59:09.403517Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Flatten the features\nfeatures_flat = features.reshape(features.shape[0], -1)\n\n# Assuming 'labels' is a list/array of class labels for each image in the test set\nlabels = generator_test.classes\n\n# Apply t-SNE\ntsne = TSNE(n_components=2, random_state=42)\ntsne_result = tsne.fit_transform(features_flat)\n\n# Create a scatter plot\nplt.figure(figsize=(10, 8))\nplt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=labels, cmap='viridis')\nplt.title('t-SNE Visualization of Model Features')\nplt.xlabel('t-SNE Component 1')\nplt.ylabel('t-SNE Component 2')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T17:59:29.147246Z","iopub.execute_input":"2024-01-03T17:59:29.147623Z","iopub.status.idle":"2024-01-03T17:59:35.114151Z","shell.execute_reply.started":"2024-01-03T17:59:29.147588Z","shell.execute_reply":"2024-01-03T17:59:35.113171Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x800 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import beit","metadata":{"execution":{"iopub.status.busy":"2023-12-11T20:06:19.440306Z","iopub.execute_input":"2023-12-11T20:06:19.441526Z","iopub.status.idle":"2023-12-11T20:06:19.511689Z","shell.execute_reply.started":"2023-12-11T20:06:19.441483Z","shell.execute_reply":"2023-12-11T20:06:19.510768Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import swin_transformer_v2","metadata":{"execution":{"iopub.status.busy":"2023-12-11T20:06:25.903555Z","iopub.execute_input":"2023-12-11T20:06:25.904378Z","iopub.status.idle":"2023-12-11T20:06:25.908399Z","shell.execute_reply.started":"2023-12-11T20:06:25.904341Z","shell.execute_reply":"2023-12-11T20:06:25.907410Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import swin_transformer_v2\nmm = swin_transformer_v2.SwinTransformerV2Tiny_window8(input_shape=(112, 112, 3))","metadata":{"execution":{"iopub.status.busy":"2023-12-11T20:06:32.451436Z","iopub.execute_input":"2023-12-11T20:06:32.452405Z","iopub.status.idle":"2023-12-11T20:06:37.010842Z","shell.execute_reply.started":"2023-12-11T20:06:32.452359Z","shell.execute_reply":"2023-12-11T20:06:37.009799Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":">>>> Load pretrained from: /root/.keras/models/swin_transformer_v2_tiny_window8_256_imagenet.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"# from keras_cv_attention_models import nat\n\nmm2 =beit.BeitBasePatch16(input_shape=(224, 224, 3))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T20:06:37.012747Z","iopub.execute_input":"2023-12-11T20:06:37.013074Z","iopub.status.idle":"2023-12-11T20:06:40.858770Z","shell.execute_reply.started":"2023-12-11T20:06:37.013047Z","shell.execute_reply":"2023-12-11T20:06:40.857894Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":">>>> Load pretrained from: /root/.keras/models/beit_base_patch16_224_imagenet21k-ft1k.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"mm_last_layer = custom_model .get_layer('avg_pool').output\n#out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\n#out = Dense(11, activation='softmax', name='prediction1')(out)\nmm_custom = Model(custom_model .input, mm_last_layer)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T08:31:34.207278Z","iopub.execute_input":"2023-12-30T08:31:34.208153Z","iopub.status.idle":"2023-12-30T08:31:34.233332Z","shell.execute_reply.started":"2023-12-30T08:31:34.208110Z","shell.execute_reply":"2023-12-30T08:31:34.232541Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"mm_last_layer = mm2 .get_layer('out_ln').output\n# out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\n# out = Dense(11, activation='softmax', name='prediction1')(out)\nmm2_custom = Model(mm2 .input,mm_last_layer )","metadata":{"execution":{"iopub.status.busy":"2023-12-11T20:06:42.647967Z","iopub.execute_input":"2023-12-11T20:06:42.648461Z","iopub.status.idle":"2023-12-11T20:06:42.680340Z","shell.execute_reply.started":"2023-12-11T20:06:42.648420Z","shell.execute_reply":"2023-12-11T20:06:42.679633Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"mm_last_layer = mm2 .get_layer('stack4_block5_2_output').output\n#out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\n#out = Dense(11, activation='softmax', name='prediction1')(out)\nmm_custom = Model(mm2 .input, mm_last_layer)\n\nfrom tensorflow.keras import layers\ninputs = keras.Input(shape=(112,112,3))\noutputs = layers.average([mm_custom(inputs)])\n","metadata":{"execution":{"iopub.status.busy":"2023-12-30T08:29:49.993592Z","iopub.execute_input":"2023-12-30T08:29:49.994543Z","iopub.status.idle":"2023-12-30T08:29:50.767100Z","shell.execute_reply.started":"2023-12-30T08:29:49.994505Z","shell.execute_reply":"2023-12-30T08:29:50.765635Z"},"trusted":true},"execution_count":15,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mm_last_layer \u001b[38;5;241m=\u001b[39m \u001b[43mmm2\u001b[49m \u001b[38;5;241m.\u001b[39mget_layer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstack4_block5_2_output\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moutput\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#out = Dense(11, activation='softmax', name='prediction1')(out)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m mm_custom \u001b[38;5;241m=\u001b[39m Model(mm2 \u001b[38;5;241m.\u001b[39minput, mm_last_layer)\n","\u001b[0;31mNameError\u001b[0m: name 'mm2' is not defined"],"ename":"NameError","evalue":"name 'mm2' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# from tensorflow.keras import layers\n\n# inputs = keras.Input(shape=(112, 112, 3))\n# mm_last_layer = mm2.get_layer('stack4_block5_2_output').output\n# outputs = layers.average([mm_last_layer])\n\n# avg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\ninputs = keras.Input(shape=(112,112,3))\noutputs = layers.average([mm_custom(inputs)])\n\navg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\navg_ensemble_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-30T08:31:38.844542Z","iopub.execute_input":"2023-12-30T08:31:38.844950Z","iopub.status.idle":"2023-12-30T08:31:39.958058Z","shell.execute_reply.started":"2023-12-30T08:31:38.844907Z","shell.execute_reply":"2023-12-30T08:31:39.956305Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Model: \"model_8\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_6 (InputLayer)        [(None, 112, 112, 3)]     0         \n                                                                 \n model_7 (Functional)        (None, 768)               22487658  \n                                                                 \n average_1 (Average)         (None, 768)               0         \n                                                                 \n=================================================================\nTotal params: 22,487,658\nTrainable params: 22,476,010\nNon-trainable params: 11,648\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras import layers\ninputs = keras.Input(shape=(224,224,3))\noutputs = layers.average([mm_custom(inputs)])\n\navg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\navg_ensemble_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install --upgrade keras_cv_attention_models\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 26\navg_ensemble_model_last_layer = avg_ensemble_model.get_layer('average').output\noutput_layer = Dense(num_classes, activation='softmax', name='output_1')(avg_ensemble_model_last_layer)\nfinal_model = Model(avg_ensemble_model.input, output_layer)\n\nfinal_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-29T13:20:49.721024Z","iopub.execute_input":"2023-12-29T13:20:49.721419Z","iopub.status.idle":"2023-12-29T13:20:49.782999Z","shell.execute_reply.started":"2023-12-29T13:20:49.721390Z","shell.execute_reply":"2023-12-29T13:20:49.782138Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Model: \"model_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 112, 112, 3)]     0         \n                                                                 \n model_1 (Functional)        (None, 512)               16704736  \n                                                                 \n average (Average)           (None, 512)               0         \n                                                                 \n output_1 (Dense)            (None, 26)                13338     \n                                                                 \n=================================================================\nTotal params: 16,718,074\nTrainable params: 16,693,498\nNon-trainable params: 24,576\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = Adam(lr=1e-5)\nloss = 'categorical_crossentropy'\n# metrics = ['categorical_accuracy']\nmetrics = ['accuracy', 'categorical_accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), \n           tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), \n           tf.keras.metrics.FalseNegatives(), tfa.metrics.CohenKappa(num_classes = num_classes), \n           tfa.metrics.F1Score(num_classes = num_classes)]\n\nfinal_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)","metadata":{"execution":{"iopub.status.busy":"2023-12-29T13:20:59.180813Z","iopub.execute_input":"2023-12-29T13:20:59.181220Z","iopub.status.idle":"2023-12-29T13:20:59.244358Z","shell.execute_reply.started":"2023-12-29T13:20:59.181187Z","shell.execute_reply":"2023-12-29T13:20:59.243429Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nlr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1,\n    patience=9, mode=\"max\", min_delta=0.0001, min_lr=0.00001, verbose=1)\ncheckpoint = ModelCheckpoint(filepath='Best_DenseNet2013_v23.h5', save_best_only=True, monitor = 'val_accuracy', verbose=1)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, restore_best_weights=True)\n\ncallbacks = [lr, checkpoint, early_stopping]","metadata":{"execution":{"iopub.status.busy":"2023-12-29T13:21:46.150220Z","iopub.execute_input":"2023-12-29T13:21:46.150596Z","iopub.status.idle":"2023-12-29T13:21:46.157221Z","shell.execute_reply.started":"2023-12-29T13:21:46.150567Z","shell.execute_reply":"2023-12-29T13:21:46.156146Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"epochs = 30\n\nsteps_per_epoch = generator_train.n / batch_size\nsteps_test = generator_test.n / batch_size\n\nhistory = final_model.fit_generator(generator=generator_train,\n                                  epochs=epochs,\n                                  steps_per_epoch=steps_per_epoch,\n                                  validation_data=generator_test,\n                                  validation_steps=steps_test,\n                                   callbacks=callbacks, class_weight =class_weights)","metadata":{"execution":{"iopub.status.busy":"2023-12-29T13:21:50.979832Z","iopub.execute_input":"2023-12-29T13:21:50.980247Z","iopub.status.idle":"2023-12-29T15:13:16.827340Z","shell.execute_reply.started":"2023-12-29T13:21:50.980213Z","shell.execute_reply":"2023-12-29T15:13:16.826338Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/2027776823.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n  history = final_model.fit_generator(generator=generator_train,\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30\n832/831 [==============================] - ETA: 0s - loss: 1.5334 - accuracy: 0.5354 - categorical_accuracy: 0.5354 - auc: 0.9359 - precision: 0.7516 - recall: 0.3906 - true_positives: 5198.0000 - true_negatives: 330957.0000 - false_positives: 1718.0000 - false_negatives: 8109.0000 - cohen_kappa: 0.5163 - f1_score: 0.5287\nEpoch 1: val_accuracy improved from -inf to 0.37173, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 358s 346ms/step - loss: 1.5334 - accuracy: 0.5354 - categorical_accuracy: 0.5354 - auc: 0.9359 - precision: 0.7516 - recall: 0.3906 - true_positives: 5198.0000 - true_negatives: 330957.0000 - false_positives: 1718.0000 - false_negatives: 8109.0000 - cohen_kappa: 0.5163 - f1_score: 0.5287 - val_loss: 2.4273 - val_accuracy: 0.3717 - val_categorical_accuracy: 0.3717 - val_auc: 0.8711 - val_precision: 0.4443 - val_recall: 0.3171 - val_true_positives: 534.0000 - val_true_negatives: 41432.0000 - val_false_positives: 668.0000 - val_false_negatives: 1150.0000 - val_cohen_kappa: 0.3465 - val_f1_score: 0.3623 - lr: 0.0010\nEpoch 2/30\n832/831 [==============================] - ETA: 0s - loss: 0.7923 - accuracy: 0.7449 - categorical_accuracy: 0.7449 - auc: 0.9799 - precision: 0.8302 - recall: 0.6703 - true_positives: 8920.0000 - true_negatives: 330850.0000 - false_positives: 1825.0000 - false_negatives: 4387.0000 - cohen_kappa: 0.7341 - f1_score: 0.7434\nEpoch 2: val_accuracy improved from 0.37173 to 0.60629, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 224s 269ms/step - loss: 0.7923 - accuracy: 0.7449 - categorical_accuracy: 0.7449 - auc: 0.9799 - precision: 0.8302 - recall: 0.6703 - true_positives: 8920.0000 - true_negatives: 330850.0000 - false_positives: 1825.0000 - false_negatives: 4387.0000 - cohen_kappa: 0.7341 - f1_score: 0.7434 - val_loss: 1.4716 - val_accuracy: 0.6063 - val_categorical_accuracy: 0.6063 - val_auc: 0.9406 - val_precision: 0.6627 - val_recall: 0.5635 - val_true_positives: 949.0000 - val_true_negatives: 41617.0000 - val_false_positives: 483.0000 - val_false_negatives: 735.0000 - val_cohen_kappa: 0.5912 - val_f1_score: 0.6386 - lr: 0.0010\nEpoch 3/30\n832/831 [==============================] - ETA: 0s - loss: 0.5858 - accuracy: 0.8096 - categorical_accuracy: 0.8096 - auc: 0.9881 - precision: 0.8631 - recall: 0.7568 - true_positives: 10071.0000 - true_negatives: 331078.0000 - false_positives: 1597.0000 - false_negatives: 3236.0000 - cohen_kappa: 0.8015 - f1_score: 0.8082\nEpoch 3: val_accuracy improved from 0.60629 to 0.61164, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 218s 262ms/step - loss: 0.5858 - accuracy: 0.8096 - categorical_accuracy: 0.8096 - auc: 0.9881 - precision: 0.8631 - recall: 0.7568 - true_positives: 10071.0000 - true_negatives: 331078.0000 - false_positives: 1597.0000 - false_negatives: 3236.0000 - cohen_kappa: 0.8015 - f1_score: 0.8082 - val_loss: 1.5807 - val_accuracy: 0.6116 - val_categorical_accuracy: 0.6116 - val_auc: 0.9330 - val_precision: 0.6503 - val_recall: 0.5730 - val_true_positives: 965.0000 - val_true_negatives: 41581.0000 - val_false_positives: 519.0000 - val_false_negatives: 719.0000 - val_cohen_kappa: 0.5972 - val_f1_score: 0.6356 - lr: 0.0010\nEpoch 4/30\n832/831 [==============================] - ETA: 0s - loss: 0.4635 - accuracy: 0.8461 - categorical_accuracy: 0.8461 - auc: 0.9918 - precision: 0.8892 - recall: 0.8091 - true_positives: 10767.0000 - true_negatives: 331334.0000 - false_positives: 1341.0000 - false_negatives: 2540.0000 - cohen_kappa: 0.8395 - f1_score: 0.8446\nEpoch 4: val_accuracy improved from 0.61164 to 0.82007, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 213s 256ms/step - loss: 0.4635 - accuracy: 0.8461 - categorical_accuracy: 0.8461 - auc: 0.9918 - precision: 0.8892 - recall: 0.8091 - true_positives: 10767.0000 - true_negatives: 331334.0000 - false_positives: 1341.0000 - false_negatives: 2540.0000 - cohen_kappa: 0.8395 - f1_score: 0.8446 - val_loss: 0.5349 - val_accuracy: 0.8201 - val_categorical_accuracy: 0.8201 - val_auc: 0.9894 - val_precision: 0.8590 - val_recall: 0.7779 - val_true_positives: 1310.0000 - val_true_negatives: 41885.0000 - val_false_positives: 215.0000 - val_false_negatives: 374.0000 - val_cohen_kappa: 0.8124 - val_f1_score: 0.8123 - lr: 0.0010\nEpoch 5/30\n832/831 [==============================] - ETA: 0s - loss: 0.4024 - accuracy: 0.8605 - categorical_accuracy: 0.8605 - auc: 0.9932 - precision: 0.8961 - recall: 0.8308 - true_positives: 11056.0000 - true_negatives: 331393.0000 - false_positives: 1282.0000 - false_negatives: 2251.0000 - cohen_kappa: 0.8545 - f1_score: 0.8599\nEpoch 5: val_accuracy did not improve from 0.82007\n831/831 [==============================] - 215s 258ms/step - loss: 0.4024 - accuracy: 0.8605 - categorical_accuracy: 0.8605 - auc: 0.9932 - precision: 0.8961 - recall: 0.8308 - true_positives: 11056.0000 - true_negatives: 331393.0000 - false_positives: 1282.0000 - false_negatives: 2251.0000 - cohen_kappa: 0.8545 - f1_score: 0.8599 - val_loss: 0.6772 - val_accuracy: 0.7815 - val_categorical_accuracy: 0.7815 - val_auc: 0.9828 - val_precision: 0.8326 - val_recall: 0.7381 - val_true_positives: 1243.0000 - val_true_negatives: 41850.0000 - val_false_positives: 250.0000 - val_false_negatives: 441.0000 - val_cohen_kappa: 0.7724 - val_f1_score: 0.7879 - lr: 0.0010\nEpoch 6/30\n832/831 [==============================] - ETA: 0s - loss: 0.3548 - accuracy: 0.8765 - categorical_accuracy: 0.8765 - auc: 0.9943 - precision: 0.9053 - recall: 0.8511 - true_positives: 11325.0000 - true_negatives: 331490.0000 - false_positives: 1185.0000 - false_negatives: 1982.0000 - cohen_kappa: 0.8712 - f1_score: 0.8754\nEpoch 6: val_accuracy did not improve from 0.82007\n831/831 [==============================] - 210s 253ms/step - loss: 0.3548 - accuracy: 0.8765 - categorical_accuracy: 0.8765 - auc: 0.9943 - precision: 0.9053 - recall: 0.8511 - true_positives: 11325.0000 - true_negatives: 331490.0000 - false_positives: 1185.0000 - false_negatives: 1982.0000 - cohen_kappa: 0.8712 - f1_score: 0.8754 - val_loss: 3.4096 - val_accuracy: 0.3741 - val_categorical_accuracy: 0.3741 - val_auc: 0.8094 - val_precision: 0.4187 - val_recall: 0.3456 - val_true_positives: 582.0000 - val_true_negatives: 41292.0000 - val_false_positives: 808.0000 - val_false_negatives: 1102.0000 - val_cohen_kappa: 0.3484 - val_f1_score: 0.3431 - lr: 0.0010\nEpoch 7/30\n832/831 [==============================] - ETA: 0s - loss: 0.3266 - accuracy: 0.8886 - categorical_accuracy: 0.8886 - auc: 0.9953 - precision: 0.9128 - recall: 0.8668 - true_positives: 11534.0000 - true_negatives: 331573.0000 - false_positives: 1102.0000 - false_negatives: 1773.0000 - cohen_kappa: 0.8838 - f1_score: 0.8877\nEpoch 7: val_accuracy improved from 0.82007 to 0.86045, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 221s 266ms/step - loss: 0.3266 - accuracy: 0.8886 - categorical_accuracy: 0.8886 - auc: 0.9953 - precision: 0.9128 - recall: 0.8668 - true_positives: 11534.0000 - true_negatives: 331573.0000 - false_positives: 1102.0000 - false_negatives: 1773.0000 - cohen_kappa: 0.8838 - f1_score: 0.8877 - val_loss: 0.4074 - val_accuracy: 0.8605 - val_categorical_accuracy: 0.8605 - val_auc: 0.9929 - val_precision: 0.8850 - val_recall: 0.8314 - val_true_positives: 1400.0000 - val_true_negatives: 41918.0000 - val_false_positives: 182.0000 - val_false_negatives: 284.0000 - val_cohen_kappa: 0.8546 - val_f1_score: 0.8634 - lr: 0.0010\nEpoch 8/30\n832/831 [==============================] - ETA: 0s - loss: 0.2961 - accuracy: 0.9010 - categorical_accuracy: 0.9010 - auc: 0.9955 - precision: 0.9225 - recall: 0.8825 - true_positives: 11743.0000 - true_negatives: 331689.0000 - false_positives: 986.0000 - false_negatives: 1564.0000 - cohen_kappa: 0.8967 - f1_score: 0.8986\nEpoch 8: val_accuracy did not improve from 0.86045\n831/831 [==============================] - 212s 255ms/step - loss: 0.2961 - accuracy: 0.9010 - categorical_accuracy: 0.9010 - auc: 0.9955 - precision: 0.9225 - recall: 0.8825 - true_positives: 11743.0000 - true_negatives: 331689.0000 - false_positives: 986.0000 - false_negatives: 1564.0000 - cohen_kappa: 0.8967 - f1_score: 0.8986 - val_loss: 1.2833 - val_accuracy: 0.6188 - val_categorical_accuracy: 0.6188 - val_auc: 0.9546 - val_precision: 0.6841 - val_recall: 0.5903 - val_true_positives: 994.0000 - val_true_negatives: 41641.0000 - val_false_positives: 459.0000 - val_false_negatives: 690.0000 - val_cohen_kappa: 0.6037 - val_f1_score: 0.6390 - lr: 0.0010\nEpoch 9/30\n832/831 [==============================] - ETA: 0s - loss: 0.2414 - accuracy: 0.9116 - categorical_accuracy: 0.9116 - auc: 0.9971 - precision: 0.9276 - recall: 0.8979 - true_positives: 11949.0000 - true_negatives: 331743.0000 - false_positives: 932.0000 - false_negatives: 1358.0000 - cohen_kappa: 0.9078 - f1_score: 0.9110\nEpoch 9: val_accuracy improved from 0.86045 to 0.88717, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 213s 256ms/step - loss: 0.2414 - accuracy: 0.9116 - categorical_accuracy: 0.9116 - auc: 0.9971 - precision: 0.9276 - recall: 0.8979 - true_positives: 11949.0000 - true_negatives: 331743.0000 - false_positives: 932.0000 - false_negatives: 1358.0000 - cohen_kappa: 0.9078 - f1_score: 0.9110 - val_loss: 0.3221 - val_accuracy: 0.8872 - val_categorical_accuracy: 0.8872 - val_auc: 0.9933 - val_precision: 0.9020 - val_recall: 0.8747 - val_true_positives: 1473.0000 - val_true_negatives: 41940.0000 - val_false_positives: 160.0000 - val_false_negatives: 211.0000 - val_cohen_kappa: 0.8824 - val_f1_score: 0.8907 - lr: 0.0010\nEpoch 10/30\n832/831 [==============================] - ETA: 0s - loss: 0.2595 - accuracy: 0.9110 - categorical_accuracy: 0.9110 - auc: 0.9965 - precision: 0.9283 - recall: 0.8960 - true_positives: 11923.0000 - true_negatives: 331754.0000 - false_positives: 921.0000 - false_negatives: 1384.0000 - cohen_kappa: 0.9072 - f1_score: 0.9086\nEpoch 10: val_accuracy did not improve from 0.88717\n831/831 [==============================] - 220s 264ms/step - loss: 0.2595 - accuracy: 0.9110 - categorical_accuracy: 0.9110 - auc: 0.9965 - precision: 0.9283 - recall: 0.8960 - true_positives: 11923.0000 - true_negatives: 331754.0000 - false_positives: 921.0000 - false_negatives: 1384.0000 - cohen_kappa: 0.9072 - f1_score: 0.9086 - val_loss: 0.9451 - val_accuracy: 0.7310 - val_categorical_accuracy: 0.7310 - val_auc: 0.9685 - val_precision: 0.7874 - val_recall: 0.6686 - val_true_positives: 1126.0000 - val_true_negatives: 41796.0000 - val_false_positives: 304.0000 - val_false_negatives: 558.0000 - val_cohen_kappa: 0.7195 - val_f1_score: 0.7176 - lr: 0.0010\nEpoch 11/30\n832/831 [==============================] - ETA: 0s - loss: 0.2345 - accuracy: 0.9151 - categorical_accuracy: 0.9151 - auc: 0.9972 - precision: 0.9304 - recall: 0.9018 - true_positives: 12000.0000 - true_negatives: 331778.0000 - false_positives: 897.0000 - false_negatives: 1307.0000 - cohen_kappa: 0.9114 - f1_score: 0.9138\nEpoch 11: val_accuracy improved from 0.88717 to 0.91568, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 216s 259ms/step - loss: 0.2345 - accuracy: 0.9151 - categorical_accuracy: 0.9151 - auc: 0.9972 - precision: 0.9304 - recall: 0.9018 - true_positives: 12000.0000 - true_negatives: 331778.0000 - false_positives: 897.0000 - false_negatives: 1307.0000 - cohen_kappa: 0.9114 - f1_score: 0.9138 - val_loss: 0.2194 - val_accuracy: 0.9157 - val_categorical_accuracy: 0.9157 - val_auc: 0.9970 - val_precision: 0.9294 - val_recall: 0.9068 - val_true_positives: 1527.0000 - val_true_negatives: 41984.0000 - val_false_positives: 116.0000 - val_false_negatives: 157.0000 - val_cohen_kappa: 0.9120 - val_f1_score: 0.9117 - lr: 0.0010\nEpoch 12/30\n832/831 [==============================] - ETA: 0s - loss: 0.2331 - accuracy: 0.9158 - categorical_accuracy: 0.9158 - auc: 0.9975 - precision: 0.9311 - recall: 0.9045 - true_positives: 12036.0000 - true_negatives: 331785.0000 - false_positives: 890.0000 - false_negatives: 1271.0000 - cohen_kappa: 0.9121 - f1_score: 0.9138\nEpoch 12: val_accuracy improved from 0.91568 to 0.92755, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 215s 259ms/step - loss: 0.2331 - accuracy: 0.9158 - categorical_accuracy: 0.9158 - auc: 0.9975 - precision: 0.9311 - recall: 0.9045 - true_positives: 12036.0000 - true_negatives: 331785.0000 - false_positives: 890.0000 - false_negatives: 1271.0000 - cohen_kappa: 0.9121 - f1_score: 0.9138 - val_loss: 0.2142 - val_accuracy: 0.9276 - val_categorical_accuracy: 0.9276 - val_auc: 0.9953 - val_precision: 0.9305 - val_recall: 0.9216 - val_true_positives: 1552.0000 - val_true_negatives: 41984.0000 - val_false_positives: 116.0000 - val_false_negatives: 132.0000 - val_cohen_kappa: 0.9244 - val_f1_score: 0.9245 - lr: 0.0010\nEpoch 13/30\n832/831 [==============================] - ETA: 0s - loss: 0.2118 - accuracy: 0.9258 - categorical_accuracy: 0.9258 - auc: 0.9974 - precision: 0.9392 - recall: 0.9137 - true_positives: 12158.0000 - true_negatives: 331888.0000 - false_positives: 787.0000 - false_negatives: 1149.0000 - cohen_kappa: 0.9226 - f1_score: 0.9240\nEpoch 13: val_accuracy did not improve from 0.92755\n831/831 [==============================] - 221s 265ms/step - loss: 0.2118 - accuracy: 0.9258 - categorical_accuracy: 0.9258 - auc: 0.9974 - precision: 0.9392 - recall: 0.9137 - true_positives: 12158.0000 - true_negatives: 331888.0000 - false_positives: 787.0000 - false_negatives: 1149.0000 - cohen_kappa: 0.9226 - f1_score: 0.9240 - val_loss: 0.2280 - val_accuracy: 0.9204 - val_categorical_accuracy: 0.9204 - val_auc: 0.9970 - val_precision: 0.9325 - val_recall: 0.9103 - val_true_positives: 1533.0000 - val_true_negatives: 41989.0000 - val_false_positives: 111.0000 - val_false_negatives: 151.0000 - val_cohen_kappa: 0.9170 - val_f1_score: 0.9159 - lr: 0.0010\nEpoch 14/30\n832/831 [==============================] - ETA: 0s - loss: 0.2007 - accuracy: 0.9266 - categorical_accuracy: 0.9266 - auc: 0.9976 - precision: 0.9388 - recall: 0.9170 - true_positives: 12203.0000 - true_negatives: 331879.0000 - false_positives: 796.0000 - false_negatives: 1104.0000 - cohen_kappa: 0.9234 - f1_score: 0.9244\nEpoch 14: val_accuracy improved from 0.92755 to 0.93646, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 212s 255ms/step - loss: 0.2007 - accuracy: 0.9266 - categorical_accuracy: 0.9266 - auc: 0.9976 - precision: 0.9388 - recall: 0.9170 - true_positives: 12203.0000 - true_negatives: 331879.0000 - false_positives: 796.0000 - false_negatives: 1104.0000 - cohen_kappa: 0.9234 - f1_score: 0.9244 - val_loss: 0.1699 - val_accuracy: 0.9365 - val_categorical_accuracy: 0.9365 - val_auc: 0.9979 - val_precision: 0.9440 - val_recall: 0.9317 - val_true_positives: 1569.0000 - val_true_negatives: 42007.0000 - val_false_positives: 93.0000 - val_false_negatives: 115.0000 - val_cohen_kappa: 0.9337 - val_f1_score: 0.9294 - lr: 0.0010\nEpoch 15/30\n832/831 [==============================] - ETA: 0s - loss: 0.2057 - accuracy: 0.9248 - categorical_accuracy: 0.9248 - auc: 0.9976 - precision: 0.9371 - recall: 0.9150 - true_positives: 12176.0000 - true_negatives: 331858.0000 - false_positives: 817.0000 - false_negatives: 1131.0000 - cohen_kappa: 0.9215 - f1_score: 0.9228\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 15: val_accuracy did not improve from 0.93646\n831/831 [==============================] - 213s 256ms/step - loss: 0.2057 - accuracy: 0.9248 - categorical_accuracy: 0.9248 - auc: 0.9976 - precision: 0.9371 - recall: 0.9150 - true_positives: 12176.0000 - true_negatives: 331858.0000 - false_positives: 817.0000 - false_negatives: 1131.0000 - cohen_kappa: 0.9215 - f1_score: 0.9228 - val_loss: 0.3294 - val_accuracy: 0.8842 - val_categorical_accuracy: 0.8842 - val_auc: 0.9940 - val_precision: 0.8983 - val_recall: 0.8658 - val_true_positives: 1458.0000 - val_true_negatives: 41935.0000 - val_false_positives: 165.0000 - val_false_negatives: 226.0000 - val_cohen_kappa: 0.8792 - val_f1_score: 0.8826 - lr: 0.0010\nEpoch 16/30\n832/831 [==============================] - ETA: 0s - loss: 0.0864 - accuracy: 0.9654 - categorical_accuracy: 0.9654 - auc: 0.9995 - precision: 0.9690 - recall: 0.9618 - true_positives: 12799.0000 - true_negatives: 332266.0000 - false_positives: 409.0000 - false_negatives: 508.0000 - cohen_kappa: 0.9639 - f1_score: 0.9626\nEpoch 16: val_accuracy improved from 0.93646 to 0.96734, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 220s 265ms/step - loss: 0.0864 - accuracy: 0.9654 - categorical_accuracy: 0.9654 - auc: 0.9995 - precision: 0.9690 - recall: 0.9618 - true_positives: 12799.0000 - true_negatives: 332266.0000 - false_positives: 409.0000 - false_negatives: 508.0000 - cohen_kappa: 0.9639 - f1_score: 0.9626 - val_loss: 0.0618 - val_accuracy: 0.9673 - val_categorical_accuracy: 0.9673 - val_auc: 0.9999 - val_precision: 0.9696 - val_recall: 0.9662 - val_true_positives: 1627.0000 - val_true_negatives: 42049.0000 - val_false_positives: 51.0000 - val_false_negatives: 57.0000 - val_cohen_kappa: 0.9659 - val_f1_score: 0.9624 - lr: 1.0000e-04\nEpoch 17/30\n832/831 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9748 - categorical_accuracy: 0.9748 - auc: 0.9999 - precision: 0.9766 - recall: 0.9735 - true_positives: 12955.0000 - true_negatives: 332365.0000 - false_positives: 310.0000 - false_negatives: 352.0000 - cohen_kappa: 0.9737 - f1_score: 0.9724\nEpoch 17: val_accuracy did not improve from 0.96734\n831/831 [==============================] - 215s 258ms/step - loss: 0.0571 - accuracy: 0.9748 - categorical_accuracy: 0.9748 - auc: 0.9999 - precision: 0.9766 - recall: 0.9735 - true_positives: 12955.0000 - true_negatives: 332365.0000 - false_positives: 310.0000 - false_negatives: 352.0000 - cohen_kappa: 0.9737 - f1_score: 0.9724 - val_loss: 0.0575 - val_accuracy: 0.9673 - val_categorical_accuracy: 0.9673 - val_auc: 0.9999 - val_precision: 0.9685 - val_recall: 0.9662 - val_true_positives: 1627.0000 - val_true_negatives: 42047.0000 - val_false_positives: 53.0000 - val_false_negatives: 57.0000 - val_cohen_kappa: 0.9659 - val_f1_score: 0.9634 - lr: 1.0000e-04\nEpoch 18/30\n832/831 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.9766 - categorical_accuracy: 0.9766 - auc: 0.9999 - precision: 0.9776 - recall: 0.9758 - true_positives: 12985.0000 - true_negatives: 332377.0000 - false_positives: 298.0000 - false_negatives: 322.0000 - cohen_kappa: 0.9755 - f1_score: 0.9740\nEpoch 18: val_accuracy improved from 0.96734 to 0.96912, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 224s 270ms/step - loss: 0.0511 - accuracy: 0.9766 - categorical_accuracy: 0.9766 - auc: 0.9999 - precision: 0.9776 - recall: 0.9758 - true_positives: 12985.0000 - true_negatives: 332377.0000 - false_positives: 298.0000 - false_negatives: 322.0000 - cohen_kappa: 0.9755 - f1_score: 0.9740 - val_loss: 0.0573 - val_accuracy: 0.9691 - val_categorical_accuracy: 0.9691 - val_auc: 0.9996 - val_precision: 0.9697 - val_recall: 0.9691 - val_true_positives: 1632.0000 - val_true_negatives: 42049.0000 - val_false_positives: 51.0000 - val_false_negatives: 52.0000 - val_cohen_kappa: 0.9678 - val_f1_score: 0.9652 - lr: 1.0000e-04\nEpoch 19/30\n832/831 [==============================] - ETA: 0s - loss: 0.0466 - accuracy: 0.9783 - categorical_accuracy: 0.9783 - auc: 0.9999 - precision: 0.9789 - recall: 0.9778 - true_positives: 13012.0000 - true_negatives: 332394.0000 - false_positives: 281.0000 - false_negatives: 295.0000 - cohen_kappa: 0.9773 - f1_score: 0.9756\nEpoch 19: val_accuracy improved from 0.96912 to 0.96971, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 213s 255ms/step - loss: 0.0466 - accuracy: 0.9783 - categorical_accuracy: 0.9783 - auc: 0.9999 - precision: 0.9789 - recall: 0.9778 - true_positives: 13012.0000 - true_negatives: 332394.0000 - false_positives: 281.0000 - false_negatives: 295.0000 - cohen_kappa: 0.9773 - f1_score: 0.9756 - val_loss: 0.0553 - val_accuracy: 0.9697 - val_categorical_accuracy: 0.9697 - val_auc: 0.9996 - val_precision: 0.9709 - val_recall: 0.9691 - val_true_positives: 1632.0000 - val_true_negatives: 42051.0000 - val_false_positives: 49.0000 - val_false_negatives: 52.0000 - val_cohen_kappa: 0.9684 - val_f1_score: 0.9641 - lr: 1.0000e-04\nEpoch 20/30\n832/831 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9784 - categorical_accuracy: 0.9784 - auc: 0.9999 - precision: 0.9790 - recall: 0.9778 - true_positives: 13011.0000 - true_negatives: 332396.0000 - false_positives: 279.0000 - false_negatives: 296.0000 - cohen_kappa: 0.9774 - f1_score: 0.9757\nEpoch 20: val_accuracy did not improve from 0.96971\n831/831 [==============================] - 217s 260ms/step - loss: 0.0471 - accuracy: 0.9784 - categorical_accuracy: 0.9784 - auc: 0.9999 - precision: 0.9790 - recall: 0.9778 - true_positives: 13011.0000 - true_negatives: 332396.0000 - false_positives: 279.0000 - false_negatives: 296.0000 - cohen_kappa: 0.9774 - f1_score: 0.9757 - val_loss: 0.0510 - val_accuracy: 0.9697 - val_categorical_accuracy: 0.9697 - val_auc: 0.9999 - val_precision: 0.9703 - val_recall: 0.9697 - val_true_positives: 1633.0000 - val_true_negatives: 42050.0000 - val_false_positives: 50.0000 - val_false_negatives: 51.0000 - val_cohen_kappa: 0.9684 - val_f1_score: 0.9654 - lr: 1.0000e-04\nEpoch 21/30\n832/831 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9791 - categorical_accuracy: 0.9791 - auc: 0.9999 - precision: 0.9795 - recall: 0.9785 - true_positives: 13021.0000 - true_negatives: 332402.0000 - false_positives: 273.0000 - false_negatives: 286.0000 - cohen_kappa: 0.9782 - f1_score: 0.9765\nEpoch 21: val_accuracy improved from 0.96971 to 0.97209, saving model to Best_DenseNet2013_v23.h5\n831/831 [==============================] - 212s 254ms/step - loss: 0.0433 - accuracy: 0.9791 - categorical_accuracy: 0.9791 - auc: 0.9999 - precision: 0.9795 - recall: 0.9785 - true_positives: 13021.0000 - true_negatives: 332402.0000 - false_positives: 273.0000 - false_negatives: 286.0000 - cohen_kappa: 0.9782 - f1_score: 0.9765 - val_loss: 0.0593 - val_accuracy: 0.9721 - val_categorical_accuracy: 0.9721 - val_auc: 0.9993 - val_precision: 0.9721 - val_recall: 0.9721 - val_true_positives: 1637.0000 - val_true_negatives: 42053.0000 - val_false_positives: 47.0000 - val_false_negatives: 47.0000 - val_cohen_kappa: 0.9709 - val_f1_score: 0.9670 - lr: 1.0000e-04\nEpoch 22/30\n832/831 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9810 - categorical_accuracy: 0.9810 - auc: 1.0000 - precision: 0.9812 - recall: 0.9807 - true_positives: 13050.0000 - true_negatives: 332425.0000 - false_positives: 250.0000 - false_negatives: 257.0000 - cohen_kappa: 0.9802 - f1_score: 0.9786\nEpoch 22: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 213s 256ms/step - loss: 0.0412 - accuracy: 0.9810 - categorical_accuracy: 0.9810 - auc: 1.0000 - precision: 0.9812 - recall: 0.9807 - true_positives: 13050.0000 - true_negatives: 332425.0000 - false_positives: 250.0000 - false_negatives: 257.0000 - cohen_kappa: 0.9802 - f1_score: 0.9786 - val_loss: 0.0583 - val_accuracy: 0.9709 - val_categorical_accuracy: 0.9709 - val_auc: 0.9993 - val_precision: 0.9709 - val_recall: 0.9703 - val_true_positives: 1634.0000 - val_true_negatives: 42051.0000 - val_false_positives: 49.0000 - val_false_negatives: 50.0000 - val_cohen_kappa: 0.9696 - val_f1_score: 0.9665 - lr: 1.0000e-04\nEpoch 23/30\n832/831 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9799 - categorical_accuracy: 0.9799 - auc: 1.0000 - precision: 0.9801 - recall: 0.9795 - true_positives: 13034.0000 - true_negatives: 332410.0000 - false_positives: 265.0000 - false_negatives: 273.0000 - cohen_kappa: 0.9790 - f1_score: 0.9770\nEpoch 23: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 213s 256ms/step - loss: 0.0411 - accuracy: 0.9799 - categorical_accuracy: 0.9799 - auc: 1.0000 - precision: 0.9801 - recall: 0.9795 - true_positives: 13034.0000 - true_negatives: 332410.0000 - false_positives: 265.0000 - false_negatives: 273.0000 - cohen_kappa: 0.9790 - f1_score: 0.9770 - val_loss: 0.0627 - val_accuracy: 0.9691 - val_categorical_accuracy: 0.9691 - val_auc: 0.9993 - val_precision: 0.9697 - val_recall: 0.9685 - val_true_positives: 1631.0000 - val_true_negatives: 42049.0000 - val_false_positives: 51.0000 - val_false_negatives: 53.0000 - val_cohen_kappa: 0.9678 - val_f1_score: 0.9646 - lr: 1.0000e-04\nEpoch 24/30\n832/831 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9804 - categorical_accuracy: 0.9804 - auc: 0.9999 - precision: 0.9807 - recall: 0.9803 - true_positives: 13045.0000 - true_negatives: 332418.0000 - false_positives: 257.0000 - false_negatives: 262.0000 - cohen_kappa: 0.9795 - f1_score: 0.9777\nEpoch 24: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n\nEpoch 24: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 225s 270ms/step - loss: 0.0408 - accuracy: 0.9804 - categorical_accuracy: 0.9804 - auc: 0.9999 - precision: 0.9807 - recall: 0.9803 - true_positives: 13045.0000 - true_negatives: 332418.0000 - false_positives: 257.0000 - false_negatives: 262.0000 - cohen_kappa: 0.9795 - f1_score: 0.9777 - val_loss: 0.0573 - val_accuracy: 0.9662 - val_categorical_accuracy: 0.9662 - val_auc: 0.9996 - val_precision: 0.9673 - val_recall: 0.9662 - val_true_positives: 1627.0000 - val_true_negatives: 42045.0000 - val_false_positives: 55.0000 - val_false_negatives: 57.0000 - val_cohen_kappa: 0.9647 - val_f1_score: 0.9607 - lr: 1.0000e-04\nEpoch 25/30\n832/831 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9817 - categorical_accuracy: 0.9817 - auc: 0.9999 - precision: 0.9820 - recall: 0.9813 - true_positives: 13058.0000 - true_negatives: 332436.0000 - false_positives: 239.0000 - false_negatives: 249.0000 - cohen_kappa: 0.9809 - f1_score: 0.9791\nEpoch 25: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 221s 265ms/step - loss: 0.0362 - accuracy: 0.9817 - categorical_accuracy: 0.9817 - auc: 0.9999 - precision: 0.9820 - recall: 0.9813 - true_positives: 13058.0000 - true_negatives: 332436.0000 - false_positives: 239.0000 - false_negatives: 249.0000 - cohen_kappa: 0.9809 - f1_score: 0.9791 - val_loss: 0.0511 - val_accuracy: 0.9721 - val_categorical_accuracy: 0.9721 - val_auc: 0.9996 - val_precision: 0.9721 - val_recall: 0.9721 - val_true_positives: 1637.0000 - val_true_negatives: 42053.0000 - val_false_positives: 47.0000 - val_false_negatives: 47.0000 - val_cohen_kappa: 0.9709 - val_f1_score: 0.9669 - lr: 1.0000e-05\nEpoch 26/30\n832/831 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9828 - categorical_accuracy: 0.9828 - auc: 0.9999 - precision: 0.9832 - recall: 0.9826 - true_positives: 13075.0000 - true_negatives: 332452.0000 - false_positives: 223.0000 - false_negatives: 232.0000 - cohen_kappa: 0.9820 - f1_score: 0.9803\nEpoch 26: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 219s 263ms/step - loss: 0.0357 - accuracy: 0.9828 - categorical_accuracy: 0.9828 - auc: 0.9999 - precision: 0.9832 - recall: 0.9826 - true_positives: 13075.0000 - true_negatives: 332452.0000 - false_positives: 223.0000 - false_negatives: 232.0000 - cohen_kappa: 0.9820 - f1_score: 0.9803 - val_loss: 0.0501 - val_accuracy: 0.9721 - val_categorical_accuracy: 0.9721 - val_auc: 0.9993 - val_precision: 0.9721 - val_recall: 0.9721 - val_true_positives: 1637.0000 - val_true_negatives: 42053.0000 - val_false_positives: 47.0000 - val_false_negatives: 47.0000 - val_cohen_kappa: 0.9709 - val_f1_score: 0.9669 - lr: 1.0000e-05\nEpoch 27/30\n832/831 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9825 - categorical_accuracy: 0.9825 - auc: 0.9999 - precision: 0.9829 - recall: 0.9823 - true_positives: 13071.0000 - true_negatives: 332448.0000 - false_positives: 227.0000 - false_negatives: 236.0000 - cohen_kappa: 0.9817 - f1_score: 0.9801\nEpoch 27: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 223s 267ms/step - loss: 0.0372 - accuracy: 0.9825 - categorical_accuracy: 0.9825 - auc: 0.9999 - precision: 0.9829 - recall: 0.9823 - true_positives: 13071.0000 - true_negatives: 332448.0000 - false_positives: 227.0000 - false_negatives: 236.0000 - cohen_kappa: 0.9817 - f1_score: 0.9801 - val_loss: 0.0478 - val_accuracy: 0.9697 - val_categorical_accuracy: 0.9697 - val_auc: 0.9996 - val_precision: 0.9697 - val_recall: 0.9697 - val_true_positives: 1633.0000 - val_true_negatives: 42049.0000 - val_false_positives: 51.0000 - val_false_negatives: 51.0000 - val_cohen_kappa: 0.9684 - val_f1_score: 0.9645 - lr: 1.0000e-05\nEpoch 28/30\n832/831 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9829 - categorical_accuracy: 0.9829 - auc: 0.9999 - precision: 0.9829 - recall: 0.9829 - true_positives: 13079.0000 - true_negatives: 332448.0000 - false_positives: 227.0000 - false_negatives: 228.0000 - cohen_kappa: 0.9821 - f1_score: 0.9802\nEpoch 28: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 212s 255ms/step - loss: 0.0335 - accuracy: 0.9829 - categorical_accuracy: 0.9829 - auc: 0.9999 - precision: 0.9829 - recall: 0.9829 - true_positives: 13079.0000 - true_negatives: 332448.0000 - false_positives: 227.0000 - false_negatives: 228.0000 - cohen_kappa: 0.9821 - f1_score: 0.9802 - val_loss: 0.0484 - val_accuracy: 0.9697 - val_categorical_accuracy: 0.9697 - val_auc: 0.9996 - val_precision: 0.9697 - val_recall: 0.9697 - val_true_positives: 1633.0000 - val_true_negatives: 42049.0000 - val_false_positives: 51.0000 - val_false_negatives: 51.0000 - val_cohen_kappa: 0.9684 - val_f1_score: 0.9646 - lr: 1.0000e-05\nEpoch 29/30\n832/831 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9858 - categorical_accuracy: 0.9858 - auc: 0.9999 - precision: 0.9860 - recall: 0.9856 - true_positives: 13115.0000 - true_negatives: 332489.0000 - false_positives: 186.0000 - false_negatives: 192.0000 - cohen_kappa: 0.9852 - f1_score: 0.9836\nEpoch 29: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 214s 257ms/step - loss: 0.0317 - accuracy: 0.9858 - categorical_accuracy: 0.9858 - auc: 0.9999 - precision: 0.9860 - recall: 0.9856 - true_positives: 13115.0000 - true_negatives: 332489.0000 - false_positives: 186.0000 - false_negatives: 192.0000 - cohen_kappa: 0.9852 - f1_score: 0.9836 - val_loss: 0.0456 - val_accuracy: 0.9703 - val_categorical_accuracy: 0.9703 - val_auc: 0.9996 - val_precision: 0.9703 - val_recall: 0.9697 - val_true_positives: 1633.0000 - val_true_negatives: 42050.0000 - val_false_positives: 50.0000 - val_false_negatives: 51.0000 - val_cohen_kappa: 0.9690 - val_f1_score: 0.9655 - lr: 1.0000e-05\nEpoch 30/30\n832/831 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9856 - categorical_accuracy: 0.9856 - auc: 1.0000 - precision: 0.9857 - recall: 0.9854 - true_positives: 13113.0000 - true_negatives: 332485.0000 - false_positives: 190.0000 - false_negatives: 194.0000 - cohen_kappa: 0.9849 - f1_score: 0.9834\nEpoch 30: val_accuracy did not improve from 0.97209\n831/831 [==============================] - 224s 268ms/step - loss: 0.0319 - accuracy: 0.9856 - categorical_accuracy: 0.9856 - auc: 1.0000 - precision: 0.9857 - recall: 0.9854 - true_positives: 13113.0000 - true_negatives: 332485.0000 - false_positives: 190.0000 - false_negatives: 194.0000 - cohen_kappa: 0.9849 - f1_score: 0.9834 - val_loss: 0.0464 - val_accuracy: 0.9703 - val_categorical_accuracy: 0.9703 - val_auc: 0.9999 - val_precision: 0.9703 - val_recall: 0.9703 - val_true_positives: 1634.0000 - val_true_negatives: 42050.0000 - val_false_positives: 50.0000 - val_false_negatives: 50.0000 - val_cohen_kappa: 0.9690 - val_f1_score: 0.9650 - lr: 1.0000e-05\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load model\nfrom tensorflow.keras.models import load_model\nmodel1 = load_model(\"/kaggle/working/Best_DenseNet2013_v23.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-12-29T16:39:15.919513Z","iopub.execute_input":"2023-12-29T16:39:15.919750Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Conv2D, Add, Concatenate\n\ndef mhsa_block(x, num_heads=4, ff_dim=None):\n    if ff_dim is None:\n        ff_dim = x.shape[-1]\n\n    # MHSA (Multi-Head Self Attention)\n    mhsa = MultiHeadSelfAttention(num_heads=num_heads, key_dim=ff_dim // num_heads)(x)\n    mhsa = Dropout(0.3)(mhsa)\n    mhsa = LayerNormalization(epsilon=1e-6)(mhsa)\n    mhsa = Add()([x, mhsa])  # Residual connection\n\n    # Feed Forward\n    ff = Conv2D(ff_dim, kernel_size=1, activation='relu')(mhsa)\n    ff = Dropout(0.3)(ff)\n    ff = Conv2D(x.shape[-1], kernel_size=1)(ff)\n    ff = Dropout(0.3)(ff)\n    ff = Add()([mhsa, ff])  # Residual connection\n\n    return ff\n\ndef lvit_block(x, num_heads=4, ff_dim=None):\n    # Clone the input\n    clone = Conv2D(x.shape[-1], kernel_size=1, strides=2)(x)\n\n    # Apply MHSA to one branch\n    mhsa_branch = mhsa_block(x, num_heads=num_heads, ff_dim=ff_dim)\n\n    # Concatenate the MHSA branch output with the cloned input\n    out = Concatenate()([clone, mhsa_branch])\n\n    # 1x1 convolution to compress dimension and maintain the block's dimension\n    out = Conv2D(x.shape[-1], kernel_size=1, strides=2)(out)\n\n    return out\n\ndef modify_wave_mlp_with_lvit(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Apply LViT block\n    lvit_output = lvit_block(mm_headless.output, num_heads=4, ff_dim=512)\n\n    # Add your custom head\n    head_output = custom_head(lvit_output, num_classes)\n\n    # Create the custom model by combining the base model, LViT block, and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model_with_lvit = modify_wave_mlp_with_lvit(input_shape, num_classes)\ncustom_model_with_lvit.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import GlobalAveragePooling2D\n\ndef modify_wave_mlp_with_lvit(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Apply Global Average Pooling to reduce dimensions\n    gap_output = GlobalAveragePooling2D()(mm_headless.output)\n\n    # Reshape output to match the original shape\n    reshaped_output = Reshape((1, 1, mm_headless.output.shape[-1]))(gap_output)\n\n    # Apply LViT block\n    lvit_output = lvit_block(reshaped_output, num_heads=4, ff_dim=512)\n\n    # Add your custom head\n    head_output = custom_head(lvit_output, num_classes)\n\n    # Create the custom model by combining the base model, LViT block, and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model_with_lvit = modify_wave_mlp_with_lvit(input_shape, num_classes)\ncustom_model_with_lvit.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Conv2D, DepthwiseConv2D, BatchNormalization, Activation, MultiHeadAttention, Dropout, Concatenate, GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\ndef custom_head(input_tensor, num_classes):\n    x = GlobalAveragePooling2D()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef lvit_block(x, mhsa_channels, strides=1, dropout_rate=0.1):\n    # Clone the input\n    input_clone = x\n\n    # Apply DW Conv to reduce computations\n    x = DepthwiseConv2D(3, strides=strides, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    # Apply 1x1 Conv to expand or decrease dimension\n    x = Conv2D(mhsa_channels, 1, padding='same')(x)\n\n    # Pass one branch through MHSA module\n    x = MultiHeadAttention(num_heads=4, key_dim=mhsa_channels // 4)(x, x)\n    x = Dropout(dropout_rate)(x)\n\n    # Concatenate the output of MHSA with the cloned input\n    x = Concatenate()([input_clone, x])\n\n    # Apply 1x1 Conv to compress dimension with a stride of 2\n    x = Conv2D(mhsa_channels, 1, strides=2, padding='same')(x)\n\n    return x\n\ndef custom_lvit(input_shape, num_classes):\n    inputs = Input(shape=input_shape)\n\n    # Initial convolutional layer\n    x = Conv2D(64, 3, strides=2, padding='same', activation='relu')(inputs)\n\n    # LViT block\n    x = lvit_block(x, mhsa_channels=256, strides=1, dropout_rate=0.3)\n\n    # Add your custom head\n    head_output = custom_head(x, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=inputs, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in custom_model.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = custom_lvit(input_shape, num_classes)\ncustom_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom keras_cv_attention_models import wave_mlp, nat\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless_wave_mlp = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head for WaveMLP\n    head_output_wave_mlp = custom_head(mm_headless_wave_mlp.output, num_classes)\n\n    # Create the custom model for WaveMLP by combining the base model and the custom head\n    custom_model_wave_mlp = Model(inputs=mm_headless_wave_mlp.input, outputs=head_output_wave_mlp)\n\n    # Fine-tune the last few layers of the base model for WaveMLP\n    for layer in mm_headless_wave_mlp.layers[:-5]:\n        layer.trainable = True\n\n    return custom_model_wave_mlp\n\n# Example usage for WaveMLP:\ninput_shape_wave_mlp = (112, 112, 3)\nnum_classes_wave_mlp = 26  # Adjust based on your task\n\ncustom_model_wave_mlp = modify_wave_mlp(input_shape_wave_mlp, num_classes_wave_mlp)\ncustom_model_wave_mlp.summary()\n\n# Load DiNAT_Mini model\nmm_nat = nat.DiNAT_Mini(input_shape=(374, 269, 3), pretrained=\"imagenet\")\n\n# Combine the models\ncombined_input = Concatenate()([custom_model_wave_mlp.output, mm_nat.output])\n\n# Add a fusion layer, you can customize this layer based on your needs\nfusion_layer = Dense(256, activation='relu')(combined_input)\nfusion_layer = Dropout(0.3)(fusion_layer)\n\n# Add the final output layer\nfinal_output = Dense(num_classes_wave_mlp, activation='softmax')(fusion_layer)\n\n# Create the combined model\ncombined_model = Model(inputs=[custom_model_wave_mlp.input, mm_nat.input], outputs=final_output)\n\n# Compile the combined model\ncombined_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n# Example usage for the combined model:\ncombined_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom keras_cv_attention_models import wave_mlp, nat\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu', name='custom_dense_1')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu', name='custom_dense_2')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax', name='custom_output')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless_wave_mlp = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head for WaveMLP\n    head_output_wave_mlp = custom_head(mm_headless_wave_mlp.output, num_classes)\n\n    # Create the custom model for WaveMLP by combining the base model and the custom head\n    custom_model_wave_mlp = Model(inputs=mm_headless_wave_mlp.input, outputs=head_output_wave_mlp)\n\n    # Fine-tune the last few layers of the base model for WaveMLP\n    for layer in mm_headless_wave_mlp.layers[:-5]:\n        layer.trainable = True\n\n    return custom_model_wave_mlp\n\n# Example usage for WaveMLP:\ninput_shape_wave_mlp = (112, 112, 3)\nnum_classes_wave_mlp = 26  # Adjust based on your task\n\ncustom_model_wave_mlp = modify_wave_mlp(input_shape_wave_mlp, num_classes_wave_mlp)\ncustom_model_wave_mlp.summary()\n\n# Load DiNAT_Mini model\nmm_nat = nat.DiNAT_Mini(input_shape=(374, 269, 3), pretrained=\"imagenet\")\n\n# Combine the models\ncombined_input = Concatenate(name='combined_input')([custom_model_wave_mlp.output, mm_nat.output])\n\n# Add a fusion layer, you can customize this layer based on your needs\nfusion_layer = Dense(256, activation='relu', name='fusion_dense_1')(combined_input)\nfusion_layer = Dropout(0.3)(fusion_layer)\n\n# Add the final output layer\nfinal_output = Dense(num_classes_wave_mlp, activation='softmax', name='final_output')(fusion_layer)\n\n# Create the combined model\ncombined_model = Model(inputs=[custom_model_wave_mlp.input, mm_nat.input], outputs=final_output)\n\n# Compile the combined model\ncombined_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n# Example usage for the combined model:\ncombined_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import wave_mlp\nmm = wave_mlp.WaveMLP_T(input_shape=(112, 112, 3), pretrained=\"imagenet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import nat\n\nmm2 = nat.NAT_Mini(input_shape=(112, 112, 3))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm_last_layer = custom_model .get_layer('dense_2').output\n#out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\n#out = Dense(11, activation='softmax', name='prediction1')(out)\nmm_custom = Model(custom_model .input, mm_last_layer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\ninputs = keras.Input(shape=(112,112,3))\noutputs = layers.average([mm_custom(inputs)])\n\navg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\navg_ensemble_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 26\navg_ensemble_model_last_layer = avg_ensemble_model.get_layer('average').output\noutput_layer = Dense(num_classes, activation='softmax', name='output_1')(avg_ensemble_model_last_layer)\nfinal_model = Model(avg_ensemble_model.input, output_layer)\n\nfinal_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(lr=1e-5)\nloss = 'categorical_crossentropy'\n# metrics = ['categorical_accuracy']\nmetrics = ['accuracy', 'categorical_accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), \n           tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), \n           tf.keras.metrics.FalseNegatives(), tfa.metrics.CohenKappa(num_classes = num_classes), \n           tfa.metrics.F1Score(num_classes = num_classes)]\n\nfinal_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nlr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1,\n    patience=9, mode=\"max\", min_delta=0.0001, min_lr=0.00001, verbose=1)\ncheckpoint = ModelCheckpoint(filepath='Best_DenseNet201_v23.h5', save_best_only=True, monitor = 'val_accuracy', verbose=1)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, restore_best_weights=True)\n\ncallbacks = [lr, checkpoint, early_stopping]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\n\nsteps_per_epoch = generator_train.n / batch_size\nsteps_test = generator_test.n / batch_size\n\nhistory = final_model.fit_generator(generator=generator_train,\n                                  epochs=epochs,\n                                  steps_per_epoch=steps_per_epoch,\n                                  validation_data=generator_test,\n                                  validation_steps=steps_test,\n                                   callbacks=callbacks, class_weight =class_weights)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import wave_mlp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow-addons\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow_addons.layers import MultiHeadAttention\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU, Dropout, Concatenate\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\ndef evaluate_(model, generator_test):\n    model.evaluate(generator_test)\n    \n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n    \n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()\n\ntrain_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\ndatagen_train = ImageDataGenerator(rescale=1./255,\n                                  width_shift_range=0.1,\n                                  height_shift_range=0.1,\n                                  horizontal_flip=True,\n                                  vertical_flip=False)\n\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\nbatch_size = 16\ngenerator_train = datagen_train.flow_from_directory(directory=train_dir,\n                                                    target_size=(112, 112),\n                                                    batch_size=batch_size,\n                                                    shuffle=True)\n\ngenerator_test = datagen_test.flow_from_directory(directory=test_dir,\n                                                  target_size=(112, 112),\n                                                  batch_size=batch_size,\n                                                  shuffle=False)\n\n# Calculate class weights\nlabels = generator_train.classes\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\nclass_weights = dict(zip(np.unique(labels), class_weights))\nprint(class_weights)\n\nfrom tensorflow_addons.layers import MultiHeadSelfAttention\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\nfrom tensorflow_addons.layers import MultiHeadAttention\n\nfrom tensorflow_addons.layers import MultiHeadAttention\n\ndef add_attention_layers(base_model_output):\n    # Add self-attention for global analysis\n    attention_global = MultiHeadAttention(\n        num_heads=8, key_dim=512, dropout=0.3\n    )(base_model_output, base_model_output, return_attention_scores=False)\n    x = GlobalAveragePooling2D()(attention_global)\n\n    # Add self-attention for local analysis\n    attention_local = MultiHeadAttention(\n        num_heads=8, key_dim=512, dropout=0.3\n    )(base_model_output, base_model_output, return_attention_scores=False)\n    x_local = Flatten()(attention_local)\n\n    # Concatenate global and local attention features\n    x = Concatenate()([x, x_local])\n\n    return x\n\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add attention layers for local and global analysis\n    attention_output = add_attention_layers(mm_headless.output)\n\n    # Add your custom head\n    head_output = custom_head(attention_output, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras-self-attention","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Reshape, Concatenate, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom keras_self_attention import SeqSelfAttention\nfrom keras_cv_attention_models import wave_mlp\n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\ndef evaluate_(model, generator_test):\n    model.evaluate(generator_test)\n    \n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n    \n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()\n\ntrain_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\ndatagen_train = ImageDataGenerator(rescale=1./255, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True, vertical_flip=False)\n\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\nbatch_size = 16\ngenerator_train = datagen_train.flow_from_directory(directory=train_dir, target_size=(112, 112), batch_size=batch_size, shuffle=True)\n\ngenerator_test = datagen_test.flow_from_directory(directory=test_dir, target_size=(112, 112), batch_size=batch_size, shuffle=False)\n\n# Calculate class weights\nlabels = generator_train.classes\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\nclass_weights = dict(zip(np.unique(labels), class_weights))\nprint(class_weights)\n\nfrom keras_self_attention import SeqSelfAttention\n\ndef create_local_global_layer(input_tensor):\n    # Local Attention\n    local_attention = SeqSelfAttention(\n        attention_width=15,\n        attention_activation='sigmoid',\n        name='LocalAttention',\n    )(input_tensor)\n\n    # Multiplicative Attention\n    multiplicative_attention = SeqSelfAttention(\n        attention_width=15,\n        attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n        attention_activation=None,\n        kernel_regularizer=keras.regularizers.l2(1e-6),\n        use_attention_bias=False,\n        name='MultiplicativeAttention',\n    )(input_tensor)\n\n    return local_attention, multiplicative_attention\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Global Average Pooling on the output\n    x = GlobalAveragePooling2D()(mm_headless.output)\n\n    # Reshape to (batch_size, 1, 1, channels)\n    x = Reshape((1, 1, -1))(x)\n\n    # Add the local and multiplicative attention layers\n    local_attention, multiplicative_attention = create_local_global_layer(x)\n\n    # Reshape attention outputs to match the shape of mm_headless.output\n    local_attention = Reshape((-1,))(local_attention)\n    multiplicative_attention = Reshape((-1,))(multiplicative_attention)\n\n    # Concatenate the attention outputs with mm_headless.output\n    concatenated_attention = Concatenate()([mm_headless.output, local_attention, multiplicative_attention])\n\n    # Add your custom head on top of the concatenated attention output\n    head_output = custom_head(concatenated_attention, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras-multi-head","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom keras_multi_head import MultiHead\nfrom keras_cv_attention_models import wave_mlp\nfrom tensorflow.keras.layers import Embedding \nfrom tensorflow.keras.layers import Embedding, concatenate \n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\ndef evaluate_(model, generator_test):\n    model.evaluate(generator_test)\n    \n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n    \n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()\n\ntrain_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\ndatagen_train = ImageDataGenerator(rescale=1./255,\n                                  width_shift_range=0.1,\n                                  height_shift_range=0.1,\n                                  horizontal_flip=True,\n                                  vertical_flip=False)\n\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\nbatch_size = 16\ngenerator_train = datagen_train.flow_from_directory(directory=train_dir,\n                                                    target_size=(112, 112),\n                                                    batch_size=batch_size,\n                                                    shuffle=True)\n\ngenerator_test = datagen_test.flow_from_directory(directory=test_dir,\n                                                  target_size=(112, 112),\n                                                  batch_size=batch_size,\n                                                  shuffle=False)\n\n# Calculate class weights\nlabels = generator_train.classes\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\nclass_weights = dict(zip(np.unique(labels), class_weights))\nprint(class_weights)\n\n!pip install keras_cv_attention_models \n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp_with_multihead(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n    head_output = custom_head(mm_headless.output, num_classes)\n\n    # Create the MultiHead model\n    multihead_model = Sequential()  # Make sure to import Sequential from keras.models\n    multihead_model.add(Embedding(input_dim=5, output_dim=3, name='Embed'))  # Add the Embedding layer\n    multihead_model.add(MultiHead(\n        layer=keras.layers.Bidirectional(keras.layers.LSTM(units=16), name='LSTM'),\n        layer_num=5,\n        reg_index=[1, 4],\n        reg_slice=(slice(None, None), slice(32, 48)),\n        reg_factor=0.1,\n        name='Multi-Head-Attention',\n    ))\n    multihead_model.add(Flatten(name='Flatten'))\n    multihead_model.add(Dense(units=num_classes, activation='softmax', name='Dense'))\n\n    # Combine the base model, custom head, and MultiHead model\n    combined_output = concatenate([head_output, multihead_model.output])  # Connect the outputs\n    combined_model = Model(inputs=mm_headless.input, outputs=combined_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    combined_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return combined_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncombined_model = modify_wave_mlp_with_multihead(input_shape, num_classes)\ncombined_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\n\n\n\ndef evaluate_(model, generator_test):\n    model.evaluate(generator_test)\n    \n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n    \n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()\n\n\ntrain_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\ndatagen_train = ImageDataGenerator(rescale=1./255,\n                                  width_shift_range=0.1,\n                                  height_shift_range=0.1,\n                                  horizontal_flip=True,\n                                  vertical_flip=False)\n\n\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\n\nbatch_size = 16\ngenerator_train = datagen_train.flow_from_directory(directory=train_dir,\n                                                    target_size=(112, 112),\n                                                    batch_size=batch_size,\n                                                    shuffle=True)\n\ngenerator_test = datagen_test.flow_from_directory(directory=test_dir,\n                                                  target_size=(112, 112),\n                                                  batch_size=batch_size,\n                                                  shuffle=False)\n# Calculate class weights\nlabels = generator_train.classes\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\nclass_weights = dict(zip(np.unique(labels), class_weights))\nprint(class_weights)\n\n\n\n!pip install keras_cv_attention_models \n\n\nfrom keras_cv_attention_models import wave_mlp\n\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n    head_output = custom_head(mm_headless.output, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tf_keras_vis.gradcam import Gradcam\nfrom tf_keras_vis.utils import normalize\n\ndef generate_gradcam(model, img_path, layer_name, num_classes):\n    img = cv2.imread(img_path)\n    img = cv2.resize(img, (112, 112))\n    img = img / 255.0\n    img_array = np.expand_dims(img, axis=0)\n\n    # Create Gradcam object\n    gradcam = Gradcam(model, model_modifier=None, clone=False)\n\n    # Generate heatmap\n    cam = gradcam(img_array, penultimate_layer=layer_name, backprop_modifier=None, grad_modifier=None)\n\n    # Render heatmap\n    heatmap = np.uint8(255 * normalize(cam))\n\n    # Resize heatmap to the original image size\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n\n    # Apply colormap (jet) to the heatmap\n    jet = plt.get_cmap(\"jet\")\n    heatmap = jet(heatmap)\n\n    # Superimpose heatmap on the original image\n    superimposed_img = (heatmap[:, :, :3] * 0.4 + img * 0.6).astype(np.uint8)\n\n    # Plot the original image, heatmap, and superimposed image\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 3, 1)\n    plt.imshow(img)\n    plt.title(\"Original Image\")\n    plt.subplot(1, 3, 2)\n    plt.imshow(heatmap)\n    plt.title(\"Grad-CAM Heatmap\")\n    plt.subplot(1, 3, 3)\n    plt.imshow(superimposed_img)\n    plt.title(\"Superimposed Image\")\n    plt.show()\n\n# Example usage:\n# Choose an image from the test set\nsample_img_path = \"/kaggle/input/mango-leaf/mango-prepo/train/Aprupali/Amrupali (163).jpg\"\nimg_class = generator_test.classes[0]  # Replace with the actual class of the image\nimg_class_name = list(generator_test.class_indices.keys())[img_class]\n\n# Generate Grad-CAM visualization for the chosen image\ngenerate_gradcam(custom_model, sample_img_path, 'your_target_layer_name', num_classes)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_grad_cam(model, img_path, class_index, preprocess_input_fn):\n    # Load the image and preprocess it\n    img = cv2.imread(img_path)\n    img = cv2.resize(img, (112, 112))\n    img = preprocess_input_fn(img)\n    img_array = np.expand_dims(img, axis=0)\n\n    # Get the last convolutional layer and the model's output\n    last_conv_layer = model.get_layer('dense_11')\n    classifier_layer_names = [\n        layer.name for layer in model.layers if layer.name.endswith('dense_layer_name')]\n\n    # Create a model that maps the input image to the activations of the last conv layer and output\n    grad_model = Model([model.inputs], [last_conv_layer.output, model.get_layer(classifier_layer_names[0]).output])\n\n    # Get the gradients of the predicted class with respect to the output feature map of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_output, preds = grad_model(img_array)\n        class_channel = preds[:, class_index]\n\n    grads = tape.gradient(class_channel, last_conv_output)\n\n    # Compute the guided gradients\n    guided_grads = tf.cast(last_conv_output > 0, 'float32') * tf.cast(grads > 0, 'float32') * grads\n\n    # Compute the average of the gradients over each feature map\n    weights = tf.reduce_mean(guided_grads, axis=(1, 2))\n\n    # Build a weighted sum of the activations of the last conv layer\n    cam = tf.reduce_sum(tf.multiply(weights, last_conv_output), axis=-1)\n\n    # Normalize the CAM\n    cam = np.maximum(cam, 0)\n    cam = cam / np.max(cam)\n\n    # Resize the CAM to match the input image size\n    cam = cv2.resize(cam[0], (112, 112))\n\n    # Convert the CAM to the range [0, 255]\n    cam = np.uint8(255 * cam)\n\n    # Apply a colormap to the CAM\n    heatmap = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n\n    # Combine the heatmap with the original image\n    img = cv2.cvtColor(img.astype('uint8'), cv2.COLOR_BGR2RGB)\n    superimposed_img = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n\n    # Display the original image, heatmap, and superimposed image\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 3, 1)\n    plt.imshow(img)\n    plt.title('Original Image')\n\n    plt.subplot(1, 3, 2)\n    plt.imshow(heatmap)\n    plt.title('Grad-CAM Heatmap')\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(superimposed_img)\n    plt.title('Superimposed Image')\n    plt.show()\n\n\n# Example usage:\ndef preprocess_input(img):\n    img = img.astype('float32') / 255.0\n    return img\n\n# Assuming you have a test image path\ntest_image_path = '/kaggle/input/mango-leaf/mango-prepo/train/Aprupali/Amrupali (163) flip.png'\nimg_class_index = 0  # Change this based on the class you want to visualize\ngenerate_grad_cam(custom_model, test_image_path, img_class_index, preprocess_input)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom keras_cv_attention_models import wave_mlp\nfrom tensorflow.keras.preprocessing import image\n\n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\n\ndef get_grad_cam(model, img_path, layer_name):\n    img = image.load_img(img_path, target_size=(112, 112))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = img_array / 255.0  # Rescale to [0,1]\n\n    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.output])\n\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        class_index = np.argmax(predictions[0])\n        loss = predictions[:, class_index]\n\n    grads = tape.gradient(loss, conv_outputs)[0]\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    conv_outputs = conv_outputs[0]\n\n    heatmap = tf.reduce_mean(tf.multiply(conv_outputs, pooled_grads), axis=-1)\n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= np.max(heatmap)\n\n    return heatmap\n\ndef evaluate_with_grad_cam(model, generator_test, img_path):\n    model.evaluate(generator_test)\n\n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n\n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()\n\n    # Get Grad-CAM for a sample image\n    img_heatmap = get_grad_cam(model, img_path, layer_name=\"multiply_1230\")\n\n    # Load the original image\n    original_img = cv2.imread(img_path)\n\n    # Resize heatmap to the size of the original image\n    heatmap_resized = cv2.resize(img_heatmap, (original_img.shape[1], original_img.shape[0]))\n\n    # Convert heatmap to RGB\n    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_resized), cv2.COLORMAP_JET)\n\n    # Superimpose the heatmap on the original image\n    superimposed_img = heatmap_colored * 0.4 + original_img * 0.6\n\n    # Display the original image, heatmap, and superimposed image\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 3, 1)\n    plt.imshow(original_img)\n    plt.title('Original Image')\n\n    plt.subplot(1, 3, 2)\n    plt.imshow(heatmap_colored)\n    plt.title('Grad-CAM Heatmap')\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(superimposed_img.astype(int))\n    plt.title('Superimposed Image')\n\n    plt.show()\n\n\ntrain_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\ndatagen_train = ImageDataGenerator(rescale=1./255,\n                                  width_shift_range=0.1,\n                                  height_shift_range=0.1,\n                                  horizontal_flip=True,\n                                  vertical_flip=False)\n\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\nbatch_size = 16\ngenerator_train = datagen_train.flow_from_directory(directory=train_dir,\n                                                    target_size=(112, 112),\n                                                    batch_size=batch_size,\n                                                    shuffle=True)\n\ngenerator_test = datagen_test.flow_from_directory(directory=test_dir,\n                                                  target_size=(112, 112),\n                                                  batch_size=batch_size,\n                                                  shuffle=False)\n\n# Calculate class weights\nlabels = generator_train.classes\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\nclass_weights = dict(zip(np.unique(labels), class_weights))\nprint(class_weights)\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n    head_output = custom_head(mm_headless.output, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tfa.metrics.F1Score(num_classes=num_classes, average='weighted')])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\n# custom_model.summary()\n\n# Example usage of evaluate_with_grad_cam\nevaluate_with_grad_cam(custom_model, generator_test, img_path=\"/kaggle/input/mango-leaf/mango-prepo/train/Aprupali/Amrupali (163) random_gaussian_noise.png\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom sklearn.utils import compute_class_weight\nfrom tensorflow.keras.layers import UpSampling2D\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\n\ndef build_autoencoder(input_shape):\n    # Build a simple convolutional autoencoder\n    # You can customize this architecture based on your requirements\n    autoencoder_input = Input(shape=input_shape)\n    \n    # Encoder\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(autoencoder_input)\n    x = MaxPool2D((2, 2), padding='same')(x)\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n    encoded = MaxPool2D((2, 2), padding='same')(x)\n\n    # Decoder\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = UpSampling2D((2, 2))(x)\n    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n\n    autoencoder = Model(autoencoder_input, decoded)\n    autoencoder.compile(optimizer='adam', loss='mse')  # Use Mean Squared Error as loss for reconstruction\n    \n    return autoencoder\n\n\ndef train_autoencoder(autoencoder, generator_train):\n    # Train the autoencoder on your training data\n    autoencoder.fit(generator_train, epochs=10, steps_per_epoch=len(generator_train), verbose=1)\n\n\ndef extract_encoder_from_autoencoder(autoencoder):\n    # Extract encoder part from the trained autoencoder\n    encoder = Model(autoencoder.input, autoencoder.layers[4].output)  # Adjust layer index based on your autoencoder architecture\n    return encoder\n\n\n# Create and train autoencoder\ninput_shape = (112, 112, 3)  # Adjust based on your image dimensions\nautoencoder = build_autoencoder(input_shape)\ntrain_autoencoder(autoencoder, generator_train)\n\n# Extract encoder from autoencoder\nencoder = extract_encoder_from_autoencoder(autoencoder)\n\n# Build classification model with encoder\nclassification_model = Sequential([\n    encoder,\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(generator_train.num_classes, activation='softmax')\n])\n\n# Compile the model\nclassification_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train classification model\nhistory = classification_model.fit(generator_train, epochs=10, validation_data=generator_test)\n\n# Evaluate the model\nevaluate_(classification_model, generator_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU, UpSampling2D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\n\ndef build_autoencoder(input_shape):\n    # Build a simple convolutional autoencoder\n    # You can customize this architecture based on your requirements\n    autoencoder_input = Input(shape=input_shape)\n    \n    # Encoder\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(autoencoder_input)\n    x = MaxPool2D((2, 2), padding='same')(x)\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n    encoded = MaxPool2D((2, 2), padding='same')(x)\n\n    # Decoder\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = UpSampling2D((2, 2))(x)\n    decoded = Conv2D(3, (3, 3), activation='linear', padding='same')(x)  # Use 'linear' activation instead of 'sigmoid'\n    decoded_reshaped = Reshape(input_shape)(decoded)  # Reshape to match input shape\n\n    autoencoder = Model(autoencoder_input, decoded_reshaped)\n    autoencoder.compile(optimizer='adam', loss='mse')  # Use Mean Squared Error as loss for reconstruction\n    \n    return autoencoder\n\n\ndef train_autoencoder(autoencoder, generator_train):\n    # Train the autoencoder on your training data\n    autoencoder.fit(generator_train, epochs=10, steps_per_epoch=len(generator_train), verbose=1)\n\n\ndef extract_encoder_from_autoencoder(autoencoder):\n    # Extract encoder part from the trained autoencoder\n    encoder = Model(autoencoder.input, autoencoder.layers[4].output)  # Adjust layer index based on your autoencoder architecture\n    return encoder\n\n\ndef evaluate_(model, generator_test):\n    model.evaluate(generator_test)\n    \n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n    \n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()\n\n\n# Paths\ntrain_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\n\n# Data Generators\ndatagen_train = ImageDataGenerator(rescale=1./255,\n                                  width_shift_range=0.1,\n                                  height_shift_range=0.1,\n                                  horizontal_flip=True,\n                                  vertical_flip=False)\n\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\nbatch_size = 16\ngenerator_train = datagen_train.flow_from_directory(directory=train_dir,\n                                                    target_size=(112, 112),\n                                                    batch_size=batch_size,\n                                                    shuffle=True)\n\ngenerator_test = datagen_test.flow_from_directory(directory=test_dir,\n                                                  target_size=(112, 112),\n                                                  batch_size=batch_size,\n                                                  shuffle=False)\n\n# Calculate class weights\nlabels = generator_train.classes\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\nclass_weights = dict(zip(np.unique(labels), class_weights))\nprint(class_weights)\n\n# Create and train autoencoder\ninput_shape = (112, 112, 3)  # Adjust based on your image dimensions\nautoencoder = build_autoencoder(input_shape)\ntrain_autoencoder(autoencoder, generator_train)\n\n# Extract encoder from autoencoder\nencoder = extract_encoder_from_autoencoder(autoencoder)\n\n# Build classification model with encoder\nclassification_model = Sequential([\n    encoder,\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(generator_train.num_classes, activation='softmax')\n])\n\n# Compile the model\nclassification_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train classification model\nhistory = classification_model.fit(generator_train, epochs=10, validation_data=generator_test)\n\n# Evaluate the model\nevaluate_(classification_model, generator_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tensorflow import keras\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D, UpSampling2D\nfrom tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nnp.random.seed(72)\ntf.random.set_seed(72)\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\n\n# Autoencoder Model\ndef build_autoencoder(input_shape):\n    model = Sequential()\n\n    # Encoder\n    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n    model.add(MaxPool2D((2, 2), padding='same'))\n    model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n    model.add(MaxPool2D((2, 2), padding='same'))\n\n    # Decoder\n    model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n    model.add(UpSampling2D((2, 2)))\n    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n    model.add(UpSampling2D((2, 2)))\n    model.add(Conv2D(3, (3, 3), activation='sigmoid', padding='same'))\n\n    return model\n\n\ndef evaluate_(model, generator_test):\n    model.evaluate(generator_test)\n\n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n\n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()\n\n\ntrain_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\ndatagen_train = ImageDataGenerator(rescale=1./255,\n                                  width_shift_range=0.1,\n                                  height_shift_range=0.1,\n                                  horizontal_flip=True,\n                                  vertical_flip=False)\n\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\nbatch_size = 16\ngenerator_train = datagen_train.flow_from_directory(directory=train_dir,\n                                                    target_size=(112, 112),\n                                                    batch_size=batch_size,\n                                                    shuffle=True)\n\ngenerator_test = datagen_test.flow_from_directory(directory=test_dir,\n                                                  target_size=(112, 112),\n                                                  batch_size=batch_size,\n                                                  shuffle=False)\n\n# Build and compile the autoencoder\nautoencoder_input_shape = (112, 112, 3)\nautoencoder = build_autoencoder(autoencoder_input_shape)\nautoencoder.compile(optimizer='adam', loss='mse')  # Use Mean Squared Error for loss\n\n# Train the autoencoder on your training data\nautoencoder.fit(generator_train.next(), generator_train.next(), epochs=10, batch_size=batch_size)\n\n# Use the encoder part of the autoencoder to obtain encoded representations\nencoded_train_data = autoencoder.layers[0].predict(generator_train.next())\n\n# Classification Model using the encoded data\nclassification_model = Sequential()\nclassification_model.add(Flatten(input_shape=(encoded_train_data.shape[1:])))\nclassification_model.add(Dense(256, activation='relu'))\nclassification_model.add(Dropout(0.5))\nclassification_model.add(Dense(generator_train.num_classes, activation='softmax'))\n\n# Compile the classification model\nclassification_model.compile(optimizer='adam',\n                              loss='categorical_crossentropy',\n                              metrics=['accuracy'])\n\n# Train the classification model using the encoded data\nclassification_model.fit(encoded_train_data, keras.utils.to_categorical(generator_train.classes),\n                          epochs=10, batch_size=batch_size, class_weight=class_weights)\n\n# Evaluate the model\nevaluate_(classification_model, generator_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_grad_cam(model, img_path, class_index, preprocess_input_fn):\n    # Load the image and preprocess it\n    img = cv2.imread(img_path)\n    img = cv2.resize(img, (112, 112))\n    img = preprocess_input_fn(img)\n    img_array = np.expand_dims(img, axis=0)\n\n    # Get the last convolutional layer and the model's output\n    last_conv_layer = model.get_layer('dense_11')\n    classifier_layer_name = 'dense_2'  # Replace with the actual name of your dense layer\n\n    # Create a model that maps the input image to the activations of the last conv layer and output\n    grad_model = Model([model.inputs], [last_conv_layer.output, model.get_layer(classifier_layer_name).output])\n\n    # Get the gradients of the predicted class with respect to the output feature map of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_output, preds = grad_model(img_array)\n        class_channel = preds[:, class_index]\n\n    grads = tape.gradient(class_channel, last_conv_output)\n\n    # Compute the guided gradients\n    guided_grads = tf.cast(last_conv_output > 0, 'float32') * tf.cast(grads > 0, 'float32') * grads\n\n    # Compute the average of the gradients over each feature map\n    weights = tf.reduce_mean(guided_grads, axis=(1, 2))\n\n    # Build a weighted sum of the activations of the last conv layer\n    cam = tf.reduce_sum(tf.multiply(weights, last_conv_output), axis=-1)\n\n    # Normalize the CAM\n    cam = np.maximum(cam, 0)\n    cam = cam / np.max(cam)\n\n    # Resize the CAM to match the input image size\n    cam = cv2.resize(cam[0], (112, 112))\n\n    # Convert the CAM to the range [0, 255]\n    cam = np.uint8(255 * cam)\n\n    # Apply a colormap to the CAM\n    heatmap = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n\n    # Combine the heatmap with the original image\n    img = cv2.cvtColor(img.astype('uint8'), cv2.COLOR_BGR2RGB)\n    superimposed_img = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n\n    # Display the original image, heatmap, and superimposed image\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 3, 1)\n    plt.imshow(img)\n    plt.title('Original Image')\n\n    plt.subplot(1, 3, 2)\n    plt.imshow(heatmap)\n    plt.title('Grad-CAM Heatmap')\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(superimposed_img)\n    plt.title('Superimposed Image')\n    plt.show()\n\n\n# Example usage:\ndef preprocess_input(img):\n    img = img.astype('float32') / 255.0\n    return img\n\n# Assuming you have a test image path\ntest_image_path = '/kaggle/input/mango-leaf/mango-prepo/train/Aprupali/Amrupali (163) flip.png'\nimg_class_index = 0  # Change this based on the class you want to visualize\ngenerate_grad_cam(custom_model, test_image_path, img_class_index, preprocess_input)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tf-keras-vis\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import WaveMLP, DPTAttention\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef add_attention_layers(base_model_output):\n    # Add self-attention for global analysis\n    attention_global = DPTAttention(output_dim=512, num_heads=8, name='attention_global')(base_model_output)\n    x = GlobalAveragePooling2D()(attention_global)\n\n    # Add self-attention for local analysis\n    attention_local = DPTAttention(output_dim=512, num_heads=8, name='attention_local')(base_model_output)\n    x_local = Flatten()(attention_local)\n\n    # Concatenate global and local attention features\n    x = Concatenate()([x, x_local])\n\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = WaveMLP.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add attention layers for local and global analysis\n    attention_output = add_attention_layers(mm_headless.output)\n\n    # Add your custom head\n    head_output = custom_head(attention_output, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(256, activation='relu')(x)  # Add your own dense layers\n    x = Dropout(0.5)(x)  # Add dropout for regularization\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\n\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n#     head_output = custom_head(mm_headless.output, num_classes)\n    print(\"mm_headless.output shape:\", mm_headless.output_shape)\n    head_output = custom_head(mm_headless.output, num_classes)\n\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\n# custom_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Add your custom head\n    head_output = custom_head(mm_headless.output, num_classes)\n\n    # Create the custom model by combining the base model and the custom head\n    custom_model = Model(inputs=mm_headless.input, outputs=head_output)\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with a custom learning rate and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'precision', 'recall', 'f1'])\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n\n# def custom_head(input_tensor, num_classes):\n#     x = Flatten()(input_tensor)\n#     x = Dense(512, activation='relu')(x)\n#     x = Dropout(0.3)(x)\n#     x = Dense(256, activation='relu')(x)\n#     x = Dropout(0.3)(x)\n#     x = Dense(num_classes, activation='softmax')(x)\n#     return x\n\n# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Dropout\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.optimizers import Adam\n\n\n# # Define your custom_head function\n# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Dropout\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.optimizers import Adam\n\n# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Dropout\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.optimizers import Adam\n\n# # Define your custom_head function\n\n# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Dropout\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.optimizers import Adam\n\n# # Define your custom_head function\n\n# def autoencoder(base_model_output):\n#     # Extract the shape of the output tensor from the base model\n#     output_shape = base_model_output.shape[1:]\n\n#     # Check if output_shape is valid (at least 3 dimensions)\n#     if len(output_shape) < 3:\n#         raise ValueError(\"Invalid output_shape. It should have at least 3 dimensions.\")\n\n#     # Extract the relevant information from output_shape\n#     height, width, channels = output_shape[0], output_shape[1], output_shape[2]\n\n#     # Calculate the input shape based on the output shape\n#     input_shape = (height // 4, width // 4, channels)\n\n#     # Encoder\n#     encoder_input = Input(shape=input_shape)\n#     x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoder_input)\n#     x = MaxPooling2D((2, 2), padding='same')(x)\n#     x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n#     encoded = MaxPooling2D((2, 2), padding='same')(x)\n\n#     # Decoder\n#     x = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n#     x = UpSampling2D((2, 2))(x)\n#     x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n#     x = UpSampling2D((2, 2))(x)\n#     decoded = Conv2D(channels, (3, 3), activation='sigmoid', padding='same')(x)\n\n#     autoencoder_model = Model(encoder_input, decoded)\n#     autoencoder_model.compile(optimizer='adam', loss='mse')  # Adjust the loss function as needed\n\n#     return autoencoder_model\n\n# def modify_wave_mlp(input_shape, num_classes):\n#     # Load the WaveMLP model without the top layers (head)\n#     mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n#     # Obtain the output tensor from the last layer in the base model\n#     base_model_output = mm_headless.layers[-1].output\n\n#     # Create an autoencoder with input shape matching the last layer shape of the base model\n#     autoencoder_model = autoencoder(base_model_output)\n\n#     # Connect the autoencoder to the base model\n#     autoencoder_input = Input(shape=base_model_output.shape[1:])\n#     autoencoder_output = autoencoder_model(autoencoder_input)\n\n#     # Add your custom head for classification\n#     classification_output = custom_head(autoencoder_output, num_classes)\n\n#     # Create the custom model by combining the base model, autoencoder, and the custom head\n#     custom_model = Model(inputs=[mm_headless.input, autoencoder_input], outputs=[classification_output, autoencoder_output])\n\n#     # Fine-tune the last few layers of the base model\n#     for layer in mm_headless.layers[:-15]:\n#         layer.trainable = True\n\n#     # Compile the model with appropriate loss functions and metrics\n#     custom_model.compile(optimizer=Adam(lr=1e-4), \n#                          loss=['categorical_crossentropy', 'mse'], \n#                          loss_weights=[1.0, 0.5],  # Adjust the weight for the autoencoder loss\n#                          metrics={'dense_3': 'accuracy', 'model_4': 'mse'})\n\n#     return custom_model\n\n# # Example usage:\n# input_shape = (112, 112, 3)\n# num_classes = 26  # Adjust based on your task\n\n# custom_model = modify_wave_mlp(input_shape, num_classes)\n# custom_model.summary()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import imagenet_utils\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras import backend as K\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_gradcam(model, img_path, class_index, layer_name='your_last_conv_layer'):\n    # Load the image with target size\n    img = image.load_img(img_path, target_size=(input_shape[0], input_shape[1]))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = imagenet_utils.preprocess_input(img_array)\n\n    # Get the class predictions and the target layer's output\n    preds, last_conv_output = model.predict(img_array)\n    class_output = preds[0, class_index]\n\n    # Compute the gradient of the class output with respect to the feature map\n    grads = K.gradients(class_output, last_conv_output)[0]\n\n    # Pool the gradients over all the axes leaving out the channel dimension\n    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n\n    # Access the value of the target layer output\n    iterate = K.function([model.input], [pooled_grads, last_conv_output[0]])\n    pooled_grads_value, conv_layer_output_value = iterate([img_array])\n\n    # Weigh the output feature map with the computed gradient values\n    for i in range(pooled_grads.shape[0]):\n        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n\n    # Average the weighted feature map along the channel dimension to obtain the heatmap\n    heatmap = np.mean(conv_layer_output_value, axis=-1)\n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= np.max(heatmap)\n\n    # Resize the heatmap to be the same as the input image\n    img = cv2.imread(img_path)\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n\n    # Apply the heatmap to the original image\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    superimposed_img = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n\n    return superimposed_img\n\n# Specify the path to an image for visualization\nimg_path = '/kaggle/input/mango-leaf/mango-prepo/val/Baper Bari/Bbari (1002) flip.png'\n\n# Choose the index of the class you want to visualize\nclass_index = 0\n\n# Visualize Grad-CAM\ngradcam_img = get_gradcam(custom_model, img_path, class_index)\nplt.imshow(gradcam_img)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import imagenet_utils\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras import backend as K\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n# ... (rest of your code)\n\ndef get_gradcam(model, img_path, class_index, layer_name='your_last_conv_layer'):\n    # Load the image with target size\n    img = image.load_img(img_path, target_size=(input_shape[0], input_shape[1]))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = imagenet_utils.preprocess_input(img_array)\n\n    # Get the class predictions and the target layer's output\n    last_conv_output = model.predict(img_array)\n    class_output = last_conv_output[0, class_index]\n\n    # Compute the gradient of the class output with respect to the feature map\n    grads = K.gradients(class_output, last_conv_output)[0]\n\n    # Pool the gradients over all the axes leaving out the channel dimension\n    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n\n    # Access the value of the target layer output\n    iterate = K.function([model.input], [pooled_grads, last_conv_output[0]])\n    pooled_grads_value, conv_layer_output_value = iterate([img_array])\n\n    # Weigh the output feature map with the computed gradient values\n    for i in range(pooled_grads.shape[0]):\n        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n\n    # Average the weighted feature map along the channel dimension to obtain the heatmap\n    heatmap = np.mean(conv_layer_output_value, axis=-1)\n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= np.max(heatmap)\n\n    # Resize the heatmap to be the same as the input image\n    img = cv2.imread(img_path)\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n\n    # Apply the heatmap to the original image\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    superimposed_img = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n\n    return superimposed_img\n\n# ... (rest of your code)\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()\n\n# Specify the path to an image for visualization\nimg_path = '/kaggle/input/mango-leaf/mango-prepo/val/Baper Bari/Bbari (1002) flip.png'\n\n# Choose the index of the class you want to visualize\nclass_index = 0\n\n# Visualize Grad-CAM\ngradcam_img = get_gradcam(custom_model, img_path, class_index)\nplt.imshow(gradcam_img)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import imagenet_utils\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef custom_head(input_tensor, num_classes):\n    x = Flatten()(input_tensor)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    return x\n\ndef autoencoder(input_shape):\n    # Encoder\n    encoder_input = Input(shape=input_shape)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoder_input)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n    encoded = MaxPooling2D((2, 2), padding='same')(x)\n\n    # Decoder\n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = UpSampling2D((2, 2))(x)\n    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n\n    autoencoder_model = Model(encoder_input, decoded)\n    autoencoder_model.compile(optimizer='adam', loss='mse')  # Adjust the loss function as needed\n\n    return autoencoder_model\n\ndef modify_wave_mlp(input_shape, num_classes):\n    # Load the WaveMLP model without the top layers (head)\n    mm_headless = wave_mlp.WaveMLP_T(input_shape=input_shape, pretrained=\"imagenet\")\n\n    # Get the shape of the last layer in the base model\n    base_model_output_shape = mm_headless.layers[-1].output_shape\n\n    # Check if output_shape is valid (at least 3 dimensions)\n    if len(base_model_output_shape) < 3:\n        raise ValueError(\"Invalid output_shape. It should have at least 3 dimensions.\")\n\n    # Create an autoencoder with input shape matching the last layer shape of the base model\n    autoencoder_model = autoencoder(base_model_output_shape[1:])\n\n    # Connect the autoencoder to the base model\n    autoencoder_input = Input(shape=base_model_output_shape[1:])\n    autoencoder_output = autoencoder_model(autoencoder_input)\n\n    # Add your custom head for classification\n    classification_output = custom_head(autoencoder_output, num_classes)\n\n    # Create the custom model by combining the base model, autoencoder, and the custom head\n    custom_model = Model(inputs=[mm_headless.input, autoencoder_input], outputs=[classification_output, autoencoder_output])\n\n    # Fine-tune the last few layers of the base model\n    for layer in mm_headless.layers[:-15]:\n        layer.trainable = True\n\n    # Compile the model with appropriate loss functions and metrics\n    custom_model.compile(optimizer=Adam(lr=1e-4), \n                         loss=['categorical_crossentropy', 'mse'], \n                         loss_weights=[1.0, 0.5],  # Adjust the weight for the autoencoder loss\n                         metrics={'dense_3': 'accuracy', 'model_1': 'mse'})\n\n    return custom_model\n\n# Example usage:\ninput_shape = (112, 112, 3)\nnum_classes = 26  # Adjust based on your task\n\ncustom_model = modify_wave_mlp(input_shape, num_classes)\ncustom_model.summary()\n\n# ... (rest of your code)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the path to an image for visualization\nimg_path = '/kaggle/input/mango-leaf/mango-prepo/val/Baper Bari/Bbari (1002) flip.png'\n\n# Choose the index of the class you want to visualize\nclass_index = 0\n\n# Visualize Grad-CAM\ngradcam_img = get_gradcam(custom_model, img_path, class_index)\nplt.imshow(gradcam_img)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm_last_layer = custom_model .get_layer('avg_pool').output\n#out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\n#out = Dense(11, activation='softmax', name='prediction1')(out)\nmm_custom = Model(custom_model .input, mm_last_layer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"mm_headless.output shape:\", mm_headless.output_shape)\nhead_output = custom_head(mm_headless.output, num_classes)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import coatnet\nmm = coatnet.CoAtNet0(input_shape=(112, 112, 3), pretrained=\"imagenet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import coatnet\nmm = coatnet.CoAtNet0(input_shape=(112, 112, 3), pretrained=\"imagenet\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import res_mlp\n# mm = res_mlp.ResMLP12()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm = res_mlp.ResMLP12(input_shape=(112, 112, 3), pretrained=\"imagenet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import wave_mlp\nmm = wave_mlp.WaveMLP_T(input_shape=(112, 112, 3), pretrained=\"imagenet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import mobilevit\nmm = mobilevit.MobileViTBasePatch16(input_shape=(112, 112, 3))\nmm2 = mobilevit.MobileViTBasePatch16(input_shape=(112, 112, 3))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mm.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import swin_transformer_v2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm2 = swin_transformer_v2.SwinTransformerV2Tiny_window8(input_shape=(112, 112, 3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mm2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ntransfer_layer = mm.get_layer('avg_pool')\nconv_model = Model(inputs=mm.input, outputs=transfer_layer.output)\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n#for layer in conv_model.layers:\n#    layer.trainable = False\n    \n# Start a new Keras Sequential model.\nnew_model = Sequential()\n\n# Add the convolutional part of the VGG16 model from above.\nnew_model.add(conv_model)\n\n\n# Add the final layer for the actual classification.\nnew_model.add(Dense(2, activation='softmax'))\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_cv_attention_models import beit\nmm2 = beit.BeitBasePatch16(input_shape=(112, 112, 3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm_last_layer = mm.get_layer('avg_pool').output\n#out = Dense(256, activation='relu', name='dense_1')(mm_last_layer)\n#out = Dense(11, activation='softmax', name='prediction1')(out)\nmm_custom = Model(mm.input, mm_last_layer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm2_last_layer = mm2.get_layer('avg_pool').output\n#out2 = Dense(256, activation='relu', name='dense_1')(mm2_last_layer)\n#out2 = Dense(11, activation='softmax', name='prediction1')(out2)\nmm2_custom = Model(mm2.input, mm2_last_layer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a custom linear attention layer\nclass LinearAttentionLayer(keras.layers.Layer):\n    def __init__(self, units, **kwargs):\n        super(LinearAttentionLayer, self).__init__(**kwargs)\n        self.units = units\n\n    def build(self, input_shape):\n        self.W_q = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n        self.W_k = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n        self.W_v = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n\n    def call(self, inputs):\n        Q = tf.matmul(inputs, self.W_q)\n        K = tf.matmul(inputs, self.W_k)\n        V = tf.matmul(inputs, self.W_v)\n\n        attn_scores = tf.matmul(Q, K, transpose_b=True)\n        attn_scores = tf.nn.softmax(attn_scores / tf.math.sqrt(tf.cast(self.units, tf.float32)), axis=-1)\n        output = tf.matmul(attn_scores, V)\n\n        return output\n\n# ... Continue with your code ...\n\n# Add the attention layer where needed in your model\nnum_classes = 2\navg_ensemble_model_last_layer = avg_ensemble_model.get_layer('average').output\n\n# Add Linear Attention Layer here (for example, just before the output layer)\nattention_output = LinearAttentionLayer(64)(avg_ensemble_model_last_layer)\n\noutput_layer = Dense(num_classes, activation='softmax', name='output_1')(attention_output)\nfinal_model = Model(avg_ensemble_model.input, output_layer)\n\nfinal_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\ninputs = keras.Input(shape=(112,112,3))\noutputs = layers.average([ mm2_custom(inputs)])\n\navg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\navg_ensemble_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\ninputs = keras.Input(shape=(112,112,3))\noutputs = layers.average([mm2_custom(inputs)])\n\navg_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\navg_ensemble_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 26\navg_ensemble_model_last_layer = avg_ensemble_model.get_layer('average_3').output\noutput_layer = Dense(num_classes, activation='softmax', name='output_1')(avg_ensemble_model_last_layer)\nfinal_model = Model(avg_ensemble_model.input, output_layer)\n\nfinal_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(lr=1e-5)\nloss = 'categorical_crossentropy'\n# metrics = ['categorical_accuracy']\nmetrics = ['accuracy', 'categorical_accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), \n           tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), \n           tf.keras.metrics.FalseNegatives(), tfa.metrics.CohenKappa(num_classes = num_classes), \n           tfa.metrics.F1Score(num_classes = num_classes)]\n\nfinal_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Delete the existing HDF5 file if it exists\nif os.path.exists('Best_DenseNet201.h5'):\n    os.remove('Best_DenseNet201.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nlr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1,\n    patience=9, mode=\"max\", min_delta=0.0001, min_lr=0.00001, verbose=1)\ncheckpoint = ModelCheckpoint(filepath='Best_DenseNet201_v23.h5', save_best_only=True, monitor = 'val_accuracy', verbose=1)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, restore_best_weights=True)\n\ncallbacks = [lr, checkpoint, early_stopping]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 10\n\nsteps_per_epoch = generator_train.n / batch_size\nsteps_test = generator_test.n / batch_size\n\nhistory = final_model.fit_generator(generator=generator_train,\n                                  epochs=epochs,\n                                  steps_per_epoch=steps_per_epoch,\n                                  validation_data=generator_test,\n                                  validation_steps=steps_test,\n                                   callbacks=callbacks, class_weight =class_weights)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_(final_model, generator_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['categorical_accuracy'])\nplt.plot(history.history['val_categorical_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade scipy scikit-image\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_size = 128\nbatch_size = 8\n\ntrain_dir = r\"/kaggle/input/mango-leaf/mango-prepo/train\"\nval_dir = r\"/kaggle/input/mango-leaf/mango-prepo/val\"\ntest_dir = r\"/kaggle/input/mango-leaf/mango-prepo/test\"\n\n\ndatagen_train = ImageDataGenerator(rescale=1./255, width_shift_range=0.1, height_shift_range=0.1,\n                                  horizontal_flip=True,  vertical_flip=False)\ndatagen_test = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = datagen_train.flow_from_directory(directory=train_dir, target_size=(image_size, image_size),\n                                                    batch_size=batch_size, shuffle=True)\nval_generator = datagen_test.flow_from_directory(directory=val_dir, target_size=(image_size, image_size),\n                                                  batch_size=batch_size, shuffle=False)\ntest_generator = datagen_test.flow_from_directory(directory=test_dir, target_size=(image_size, image_size),\n                                                  batch_size=batch_size, shuffle=False)\n\n#Define the number of classes in your dataset\nnum_classes = train_generator.num_classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer, Attention\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LinearAttention(Layer):\n    def __init__(self, units):\n        super(LinearAttention, self).__init__()\n        self.units = units\n\n    def build(self, input_shape):\n        self.W_q = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n        self.W_k = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n        self.W_v = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', trainable=True)\n\n    def call(self, inputs):\n        Q = tf.matmul(inputs, self.W_q)\n        K = tf.matmul(inputs, self.W_k)\n        V = tf.matmul(inputs, self.W_v)\n\n        attn_scores = tf.matmul(Q, K, transpose_b=True)\n        attn_scores = tf.nn.softmax(attn_scores / tf.math.sqrt(tf.cast(self.units, tf.float32)), axis=-1)\n        output = tf.matmul(attn_scores, V)\n\n        return output\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define your model function with attention\ndef modelfunction_with_attention(base):\n    x = base.output\n\n    # Add Self-Attention Layer\n    att_output = LinearAttention(128)(x)\n\n    # Add more layers if needed\n    x = tf.keras.layers.GlobalAveragePooling2D()(att_output)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    predictions = tf.keras.layers.Dense(units=num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.02, l2=0.02))(x)\n    model = Model(inputs=base.input, outputs=predictions)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def modelfunction(base):\n    x = base.output\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    predictions = tf.keras.layers.Dense(units=num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.02, l2=0.02))(x)\n    model = Model(inputs=base.input, outputs=predictions)\n    return model\n\ndef get_callbacks(weight):\n    checkpoint = ModelCheckpoint(weight, monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n    learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=5, verbose=1, factor=0.2, min_lr=0.0002)\n    callbacks = [checkpoint, learning_rate_reduction]\n    return callbacks\n\ndef evaluate(model, generator_test):\n    model.evaluate(generator_test)\n\n    y_pred = model.predict(generator_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = generator_test.classes\n    class_labels = list(generator_test.class_indices.keys())\n\n    print(classification_report(y_true, y_pred_classes))\n    cm = confusion_matrix(y_true, y_pred_classes)\n\n    # Plotting the confusion matrix\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n    plt.show()\n\ndef model_training(base, weight, epochs):\n    model = modelfunction(base)\n    print(\"\\n\\n\\n-------------------- Model Initialized --------------------\")\n\n    callbacks = get_callbacks(weight)\n    metrics = ['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(),\n               tfa.metrics.CohenKappa(num_classes=num_classes), tfa.metrics.F1Score(num_classes=num_classes)]\n    model.compile(tf.keras.optimizers.Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=metrics)\n\n    history = model.fit(train_generator, steps_per_epoch=366 // batch_size,\n                        validation_data=val_generator,  # Add this line\n                        epochs=epochs, callbacks=callbacks)\n    # Plotting accuracy and loss curves\n    plt.figure(figsize=(12, 4))\n\n    # Plot accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='accuracy')\n    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Accuracy Over Epochs')\n    plt.legend()\n\n    # Plot loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Loss Over Epochs')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n\\n\\n-------------------- Evaluation --------------------\")\n    evaluate(model, val_generator)\n\n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create and train the model with attention\nVGG19 = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_tensor=None, input_shape=None)\nVGG19_model_with_attention = model_training(VGG19, 'VGG19_with_attention.h5', 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lime\nfrom lime import lime_image\nfrom skimage.segmentation import mark_boundaries\nimport matplotlib.pyplot as plt\nimport random\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lime import lime_image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skimage.segmentation import mark_boundaries","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}